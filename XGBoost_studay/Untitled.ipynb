{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "data = pd.read_csv('data/Iris.csv')\n",
    "data_training = data[0:int(len(data)*3/5)]\n",
    "data_test = data[int(len(data)*3/5):len(data)]\n",
    "#分割\n",
    "train_x = np.array(data_training.iloc[:, [i for i in range(data_training.shape[1]-1)]])\n",
    "train_y = np.array(data_training['Species'])\n",
    "test_x = np.array(data_test.iloc[:, [i for i in range(data_test.shape[1]-1)]])\n",
    "test_y = np.array(data_test['Species'])\n",
    "print(\"load data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:31:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anoconda\\envs\\env_lime\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n",
       "              importance_type='gain', interaction_constraints='',\n",
       "              learning_rate=0.05, max_delta_step=0, max_depth=8,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=50, n_jobs=8, num_parallel_tree=1,\n",
       "              objective='multi:softprob', random_state=0, reg_alpha=0,\n",
       "              reg_lambda=1, scale_pos_weight=None, subsample=1,\n",
       "              tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf=XGBClassifier(base_score=0.5, booster='gbtree', learning_rate=0.05, max_depth=8, n_estimators=50)\n",
    "clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\software\\Anoconda\\envs\\env_lime\\lib\\site-packages\\xgboost\\data.py:115: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"memory consumption\")\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(test_x, test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[121.    6.9   3.2   5.7   2.3]\n",
      " [122.    5.6   2.8   4.9   2. ]\n",
      " [123.    7.7   2.8   6.7   2. ]\n",
      " [124.    6.3   2.7   4.9   1.8]\n",
      " [125.    6.7   3.3   5.7   2.1]\n",
      " [126.    7.2   3.2   6.    1.8]\n",
      " [127.    6.2   2.8   4.8   1.8]\n",
      " [128.    6.1   3.    4.9   1.8]\n",
      " [129.    6.4   2.8   5.6   2.1]\n",
      " [130.    7.2   3.    5.8   1.6]\n",
      " [131.    7.4   2.8   6.1   1.9]\n",
      " [132.    7.9   3.8   6.4   2. ]\n",
      " [133.    6.4   2.8   5.6   2.2]\n",
      " [134.    6.3   2.8   5.1   1.5]\n",
      " [135.    6.1   2.6   5.6   1.4]\n",
      " [136.    7.7   3.    6.1   2.3]\n",
      " [137.    6.3   3.4   5.6   2.4]\n",
      " [138.    6.4   3.1   5.5   1.8]\n",
      " [139.    6.    3.    4.8   1.8]\n",
      " [140.    6.9   3.1   5.4   2.1]\n",
      " [141.    6.7   3.1   5.6   2.4]\n",
      " [142.    6.9   3.1   5.1   2.3]\n",
      " [143.    5.8   2.7   5.1   1.9]\n",
      " [144.    6.8   3.2   5.9   2.3]\n",
      " [145.    6.7   3.3   5.7   2.5]\n",
      " [146.    6.7   3.    5.2   2.3]\n",
      " [147.    6.3   2.5   5.    1.9]\n",
      " [148.    6.5   3.    5.2   2. ]\n",
      " [149.    6.2   3.4   5.4   2.3]\n",
      " [150.    5.9   3.    5.1   1.8]]\n"
     ]
    }
   ],
   "source": [
    "print(test_x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_lime",
   "language": "python",
   "name": "env_lime"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

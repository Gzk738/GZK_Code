{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a767a0c3-1350-480e-ad51-db361ffaa6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\Guo Zikun\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 :  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 0, 0]\n",
      "possibility1 :  [0.9907095432281494, 0.9882730841636658, 0.9890959858894348, 0.9855977892875671, 0.9850817918777466, 0.15943843126296997, 0.14193624258041382, 0.10548906773328781, 0.10548906773328781, 0.10548906773328781]\n",
      "possibility2 :  [0.9634395241737366, 0.9650499820709229, 0.9663087129592896, 0.9678927063941956, 0.9807416796684265, 0.11607660353183746, 0.13129590451717377, 0.09952255338430405, 0.09952255338430405, 0.09952255338430405]\n",
      "possibility average :  [0.977074533700943, 0.9766615331172943, 0.9777023494243622, 0.9767452478408813, 0.9829117357730865, 0.13775751739740372, 0.1366160735487938, 0.10250581055879593, 0.10250581055879593, 0.10250581055879593]\n",
      "f1 :  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "possibility1 :  [0.9044394493103027, 0.9021557569503784, 0.8984261751174927, 0.8893319964408875, 0.8947815895080566, 0.9100971221923828, 0.8520525097846985, 0.8703981041908264, 0.9117353558540344, 0.9213162064552307]\n",
      "possibility2 :  [0.5770822763442993, 0.582629919052124, 0.585332453250885, 0.5954742431640625, 0.5889373421669006, 0.5677032470703125, 0.5763299465179443, 0.5705180168151855, 0.5554246306419373, 0.5299413204193115]\n",
      "possibility average :  [0.740760862827301, 0.7423928380012512, 0.7418793141841888, 0.742403119802475, 0.7418594658374786, 0.7389001846313477, 0.7141912281513214, 0.720458060503006, 0.7335799932479858, 0.7256287634372711]\n",
      "f1 :  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "possibility1 :  [0.7985796928405762, 0.8001503944396973, 0.7764983773231506, 0.7819774150848389, 0.9629328846931458, 0.9628254175186157, 0.8923031687736511, 0.8817631602287292, 0.9147061109542847, 0.8672475218772888]\n",
      "possibility2 :  [0.524032711982727, 0.5266357660293579, 0.5211610198020935, 0.5298025608062744, 0.7149590253829956, 0.6854628920555115, 0.6994922161102295, 0.7055693864822388, 0.6928589344024658, 0.6767107248306274]\n",
      "possibility average :  [0.6613062024116516, 0.6633930802345276, 0.6488296985626221, 0.6558899879455566, 0.8389459550380707, 0.8241441547870636, 0.7958976924419403, 0.793666273355484, 0.8037825226783752, 0.7719791233539581]\n",
      "f1 :  [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.1818181818181818, 0.1818181818181818, 0.1818181818181818, 0.1818181818181818]\n",
      "possibility1 :  [0.6172448396682739, 0.608155369758606, 0.5861209034919739, 0.5981776714324951, 0.6113445162773132, 0.6293806433677673, 0.8314406275749207, 0.8700626492500305, 0.8700626492500305, 0.8700626492500305]\n",
      "possibility2 :  [0.763260543346405, 0.7076531052589417, 0.7138245105743408, 0.7169816493988037, 0.7126742601394653, 0.6905356645584106, 0.6850997805595398, 0.6347578167915344, 0.6347578167915344, 0.6347578167915344]\n",
      "possibility average :  [0.6902526915073395, 0.6579042375087738, 0.6499727070331573, 0.6575796604156494, 0.6620093882083893, 0.659958153963089, 0.7582702040672302, 0.7524102330207825, 0.7524102330207825, 0.7524102330207825]\n",
      "f1 :  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "possibility1 :  [0.978022575378418, 0.975751519203186, 0.9757485389709473, 0.9765193462371826, 0.9751855134963989, 0.9717237949371338, 0.9730728268623352, 0.9695155024528503, 0.9581160545349121, 0.9638546109199524]\n",
      "possibility2 :  [0.5428295135498047, 0.5523521900177002, 0.5496739149093628, 0.5587348341941833, 0.549510657787323, 0.5528401136398315, 0.5550386905670166, 0.5317791104316711, 0.5627715587615967, 0.5868301391601562]\n",
      "possibility average :  [0.7604260444641113, 0.7640518546104431, 0.762711226940155, 0.767627090215683, 0.762348085641861, 0.7622819542884827, 0.7640557587146759, 0.7506473064422607, 0.7604438066482544, 0.7753423750400543]\n",
      "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0, 0, 0, 0, 0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.1818181818181818, 0.1818181818181818, 0.1818181818181818, 0.1818181818181818], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]] [[0.9907095432281494, 0.9882730841636658, 0.9890959858894348, 0.9855977892875671, 0.9850817918777466, 0.15943843126296997, 0.14193624258041382, 0.10548906773328781, 0.10548906773328781, 0.10548906773328781], [0.9044394493103027, 0.9021557569503784, 0.8984261751174927, 0.8893319964408875, 0.8947815895080566, 0.9100971221923828, 0.8520525097846985, 0.8703981041908264, 0.9117353558540344, 0.9213162064552307], [0.7985796928405762, 0.8001503944396973, 0.7764983773231506, 0.7819774150848389, 0.9629328846931458, 0.9628254175186157, 0.8923031687736511, 0.8817631602287292, 0.9147061109542847, 0.8672475218772888], [0.6172448396682739, 0.608155369758606, 0.5861209034919739, 0.5981776714324951, 0.6113445162773132, 0.6293806433677673, 0.8314406275749207, 0.8700626492500305, 0.8700626492500305, 0.8700626492500305], [0.978022575378418, 0.975751519203186, 0.9757485389709473, 0.9765193462371826, 0.9751855134963989, 0.9717237949371338, 0.9730728268623352, 0.9695155024528503, 0.9581160545349121, 0.9638546109199524]] [[0.9634395241737366, 0.9650499820709229, 0.9663087129592896, 0.9678927063941956, 0.9807416796684265, 0.11607660353183746, 0.13129590451717377, 0.09952255338430405, 0.09952255338430405, 0.09952255338430405], [0.5770822763442993, 0.582629919052124, 0.585332453250885, 0.5954742431640625, 0.5889373421669006, 0.5677032470703125, 0.5763299465179443, 0.5705180168151855, 0.5554246306419373, 0.5299413204193115], [0.524032711982727, 0.5266357660293579, 0.5211610198020935, 0.5298025608062744, 0.7149590253829956, 0.6854628920555115, 0.6994922161102295, 0.7055693864822388, 0.6928589344024658, 0.6767107248306274], [0.763260543346405, 0.7076531052589417, 0.7138245105743408, 0.7169816493988037, 0.7126742601394653, 0.6905356645584106, 0.6850997805595398, 0.6347578167915344, 0.6347578167915344, 0.6347578167915344], [0.5428295135498047, 0.5523521900177002, 0.5496739149093628, 0.5587348341941833, 0.549510657787323, 0.5528401136398315, 0.5550386905670166, 0.5317791104316711, 0.5627715587615967, 0.5868301391601562]] [[0.977074533700943, 0.9766615331172943, 0.9777023494243622, 0.9767452478408813, 0.9829117357730865, 0.13775751739740372, 0.1366160735487938, 0.10250581055879593, 0.10250581055879593, 0.10250581055879593], [0.740760862827301, 0.7423928380012512, 0.7418793141841888, 0.742403119802475, 0.7418594658374786, 0.7389001846313477, 0.7141912281513214, 0.720458060503006, 0.7335799932479858, 0.7256287634372711], [0.6613062024116516, 0.6633930802345276, 0.6488296985626221, 0.6558899879455566, 0.8389459550380707, 0.8241441547870636, 0.7958976924419403, 0.793666273355484, 0.8037825226783752, 0.7719791233539581], [0.6902526915073395, 0.6579042375087738, 0.6499727070331573, 0.6575796604156494, 0.6620093882083893, 0.659958153963089, 0.7582702040672302, 0.7524102330207825, 0.7524102330207825, 0.7524102330207825], [0.7604260444641113, 0.7640518546104431, 0.762711226940155, 0.767627090215683, 0.762348085641861, 0.7622819542884827, 0.7640557587146759, 0.7506473064422607, 0.7604438066482544, 0.7753423750400543]]\n",
      "初始概率为： 0.8333333333333333\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"float\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1c06b7433bd8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexplain_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m     \u001b[0mbest_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexplain_score\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    552\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'解释概率为'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexplain_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can only concatenate list (not \"float\") to list"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import difflib\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# replace <PATd:/spofrte/modeH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "model_path = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "\n",
    "# load model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "\"\"\"++++++++++++++++++这几个函数是计算f1 score 数值的，代码是抄的，千万不能改！+++++++++++++++++\"\"\"\n",
    "\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "\n",
    "def get_gold_answers(example):\n",
    "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
    "\n",
    "    gold_answers = [answer[\"text\"] for answer in example.answers if answer[\"text\"]]\n",
    "\n",
    "    # if gold_answers doesn't exist it's because this is a negative example -\n",
    "    # the only correct answer is an empty string\n",
    "    if not gold_answers:\n",
    "        gold_answers = [\"\"]\n",
    "\n",
    "    return gold_answers\n",
    "\n",
    "\n",
    "\"\"\"+++++++++++++++++++++++++++++++++++\"\"\"\n",
    "\n",
    "\n",
    "def string_similar(s1, s2):\n",
    "    return difflib.SequenceMatcher(None, s1, s2).quick_ratio()\n",
    "\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.start_logits, output.end_logits\n",
    "\n",
    "\n",
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8, 6)\n",
    "\n",
    "ref_token_id = tokenizer.pad_token_id  # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id  # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id  # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "\n",
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "                    [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)  # * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.bert.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.bert.embeddings(ref_input_ids, token_type_ids=token_type_ids,\n",
    "                                                 position_ids=position_ids)\n",
    "\n",
    "    return input_embeddings, ref_input_embeddings\n",
    "\n",
    "\n",
    "def predict_qt(question, text):\n",
    "    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id,\n",
    "                                                                cls_token_id)\n",
    "    token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "    position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "    attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "    ground_truth = '13'\n",
    "\n",
    "    start_scores, end_scores = predict(input_ids, \\\n",
    "                                       token_type_ids=token_type_ids, \\\n",
    "                                       position_ids=position_ids, \\\n",
    "                                       attention_mask=attention_mask)\n",
    "\n",
    "    #print('Question: ', question)\n",
    "    #print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1]))\n",
    "    return input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens,\n",
    "\n",
    "\n",
    "def explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores,\n",
    "            ground_truth, all_tokens, ):\n",
    "    lig = LayerIntegratedGradients(squad_pos_forward_func, model.bert.embeddings)\n",
    "\n",
    "    attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "                                                    baselines=ref_input_ids,\n",
    "                                                    additional_forward_args=(\n",
    "                                                        token_type_ids, position_ids, attention_mask, 0),\n",
    "                                                    internal_batch_size=4,\n",
    "                                                    return_convergence_delta=True)\n",
    "    attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "                                                additional_forward_args=(\n",
    "                                                    token_type_ids, position_ids, attention_mask, 1),\n",
    "                                                internal_batch_size=4,\n",
    "                                                return_convergence_delta=True)\n",
    "\n",
    "    attributions_start_sum = summarize_attributions(attributions_start)\n",
    "    attributions_end_sum = summarize_attributions(attributions_end)\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    start_position_vis = viz.VisualizationDataRecord(\n",
    "        attributions_start_sum,\n",
    "        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "        torch.argmax(start_scores),\n",
    "        torch.argmax(start_scores),\n",
    "        str(ground_truth),\n",
    "        attributions_start_sum.sum(),\n",
    "        all_tokens,\n",
    "        delta_start)\n",
    "\n",
    "    end_position_vis = viz.VisualizationDataRecord(\n",
    "        attributions_end_sum,\n",
    "        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "        torch.argmax(end_scores),\n",
    "        torch.argmax(end_scores),\n",
    "        str(ground_truth),\n",
    "        attributions_end_sum.sum(),\n",
    "        all_tokens,\n",
    "        delta_end)\n",
    "    # print(all_tokens)\n",
    "    #print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "    #viz.visualize_text([start_position_vis])\n",
    "\n",
    "    #print('\\033[1m', 'Visualizations For End Position', '\\033[0m')\n",
    "\n",
    "    #print(\"attributions_start_sum:   \", len(attributions_start_sum))\n",
    "    # print(\"all tokens:    \", len(all_tokens))\n",
    "\n",
    "    return all_tokens, attributions_start_sum\n",
    "\n",
    "\n",
    "def get_posneg(all_tokens, attributions_start_sum):\n",
    "    positive = []\n",
    "    negative = []\n",
    "    neutral = []\n",
    "    for i, j in enumerate(attributions_start_sum):\n",
    "        if j > 0:\n",
    "            positive.append(i)\n",
    "            # print('positive:',j)\n",
    "        ##print(all_tokens[i])\n",
    "        elif j < 0:\n",
    "            negative.append(i)\n",
    "            # print('negative:',j)\n",
    "            # print(all_tokens[i])\n",
    "        elif j == 0:\n",
    "            neutral.append(i)\n",
    "\n",
    "    s_pos = ''\n",
    "    s_neg = ''\n",
    "\n",
    "    # print(len(attributions_start_sum))\n",
    "    # print(len(positive))\n",
    "    # print(len(negative))\n",
    "\n",
    "    for i in positive:\n",
    "        s_pos += all_tokens[i] + ' '\n",
    "    # print(\"positive :\", s_pos)\n",
    "    for i in negative:\n",
    "        s_neg += all_tokens[i] + ' '\n",
    "    # print(\"negative :\", s_neg)\n",
    "    return positive, negative, neutral\n",
    "\n",
    "\n",
    "def separate_sentence(all_tokens):\n",
    "    sentence = {}\n",
    "    temp = []\n",
    "    num = 0\n",
    "    for i in range(len(all_tokens)):\n",
    "        if all_tokens[i] == \",\" or all_tokens[i] == \".\":\n",
    "            temp.append(all_tokens[i])\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        elif all_tokens[i] == \"[CLS]\":\n",
    "            temp.append(all_tokens[i])\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        elif all_tokens[i] == \"[SEP]\":\n",
    "            sentence[num] = temp\n",
    "            num = num + 1\n",
    "            temp = [all_tokens[i]]\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        else:\n",
    "            temp.append(all_tokens[i])\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_sence_score(sentence, attributions_start_sum):\n",
    "    weight = 0\n",
    "    sum_weight = 0\n",
    "    sentence_value = []\n",
    "    delete_sentence = []\n",
    "    for k, v in sentence.items():\n",
    "        for i in v:\n",
    "            sentence_value.append(i)\n",
    "    scores = {}\n",
    "\n",
    "    for i in range(len(attributions_start_sum)):\n",
    "        try:\n",
    "            scores[sentence_value[i]] = attributions_start_sum[i].item()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i, j in sentence.items():\n",
    "        sum_weight = 0\n",
    "        for word in j:\n",
    "            sum_weight += scores[word]\n",
    "        delete_sentence.append(sum_weight)\n",
    "        # print(sum_weight)\n",
    "    return delete_sentence\n",
    "\n",
    "\n",
    "def get_delete(sentence):\n",
    "    weight = 0\n",
    "    sum_weight = 0\n",
    "    sentence_value = []\n",
    "    delete_sentence = {}\n",
    "    for k, v in sentence.items():\n",
    "        # print(k,':',v)\n",
    "        for i in v:\n",
    "            sentence_value.append(i)\n",
    "    # print(sentence_value)\n",
    "    scores = {}\n",
    "    # print(attributions_start_sum[0].item())\n",
    "\n",
    "    for i in range(len(attributions_start_sum)):\n",
    "        try:\n",
    "            scores[sentence_value[i]] = attributions_start_sum[i].item()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i, j in sentence.items():\n",
    "        sum_weight = 0\n",
    "        for word in j:\n",
    "            weight = 0\n",
    "\n",
    "            sum_weight += scores[word]\n",
    "            delete_sentence[i] = sum_weight\n",
    "    return delete_sentence\n",
    "\n",
    "\n",
    "def delete_sentence(sentence, li_delete_sentence):\n",
    "    for i, j in sentence.items():\n",
    "        if i in li_delete_sentence:\n",
    "            sentence[i] = []\n",
    "        else:\n",
    "            pass\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def rebuild_sentence(ori_sentence):\n",
    "    rebuild_str = \"\"\n",
    "    for i, j in ori_sentence.items():\n",
    "        for word in j:\n",
    "            rebuild_str += word\n",
    "            rebuild_str += \" \"\n",
    "    return rebuild_str\n",
    "\n",
    "\n",
    "def pred_explain(question, text):\n",
    "    input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens, = predict_qt(\n",
    "        text, question)\n",
    "\n",
    "    all_tokens, attributions_start_sum = explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask,\n",
    "                                                 start_scores, end_scores, ground_truth, all_tokens, )\n",
    "\n",
    "    end_score = float(torch.max(torch.softmax(end_scores[0], dim=0)))\n",
    "    start_score = float(torch.max(torch.softmax(start_scores[0], dim=0)))\n",
    "    return all_tokens, attributions_start_sum, end_score, start_score, [torch.argmax(start_scores), torch.argmax(\n",
    "        end_scores) + 1], start_scores, end_scores\n",
    "\n",
    "\n",
    "def max_min(x, y, z):\n",
    "    max = min = x\n",
    "    i = 1\n",
    "    if y > max:\n",
    "        max = y\n",
    "        i = 2\n",
    "    else:\n",
    "        min = y\n",
    "    if z > max:\n",
    "        max = z\n",
    "        i = 3\n",
    "    else:\n",
    "        min = z\n",
    "    return (i)\n",
    "\n",
    "\n",
    "def cycle_prediction(cycle_num, question, text, s_answer):\n",
    "    all_tokens, attributions_start_sum, start_acc, end_acc, an_index, start_scores, end_scores = pred_explain(text,\n",
    "                                                                                                              question)\n",
    "\n",
    "    f1 = []\n",
    "    acc_s = []\n",
    "    acc_e = []\n",
    "    sun = []\n",
    "    ans = []\n",
    "    second_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "    second_answer = re.sub(r' ##', '', second_answer)\n",
    "    f1_score = compute_f1(second_answer, s_answer)\n",
    "    f1.append(f1_score)\n",
    "    for loop in range(cycle_num):\n",
    "        sentence = separate_sentence(all_tokens)\n",
    "        sentence_score = get_sence_score(sentence, attributions_start_sum)\n",
    "        min_sensocer = 999\n",
    "        min_index = 999\n",
    "        for i in range(len(sentence_score)):\n",
    "            if sentence_score[i] < min_sensocer and sentence_score[i] != 0:\n",
    "                min_sensocer = sentence_score[i]\n",
    "                min_index = i\n",
    "        # print(\"should delete\", min_index, min_sensocer)\n",
    "        sentence[min_index] = ''\n",
    "        sentence[1] = ''\n",
    "        retext = \"\"\n",
    "        for i, j in sentence.items():\n",
    "            for words in j:\n",
    "                retext = retext + words + \" \"\n",
    "        li_sep = []\n",
    "        for m in re.finditer(r\"SEP\", retext):\n",
    "            li_sep.append(m.start())\n",
    "            li_sep.append(m.end())\n",
    "        retext = retext[li_sep[1] + 1: li_sep[2] - 1]\n",
    "        retext = re.sub(r' ##', '', retext)\n",
    "\n",
    "        all_tokens, attributions_start_sum, start_acc, end_acc, an_index, start_scores, end_scores = pred_explain(\n",
    "            retext, question)\n",
    "        reanswer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "        # print(start_acc, end_acc)\n",
    "        second_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "        second_answer = re.sub(r' ##', '', second_answer)\n",
    "        # print(\"my answer is \", second_answer)\n",
    "        ans.append(second_answer)\n",
    "        # print(start_acc, end_acc)\n",
    "        acc_s.append(start_acc)\n",
    "        acc_e.append(end_acc)\n",
    "        pos_contri = 0\n",
    "        neg_contri = 0\n",
    "        f1_score = compute_f1(second_answer, s_answer)\n",
    "        f1.append(f1_score)\n",
    "\n",
    "        # print(acc_s, acc_e)\n",
    "        # print(acc_s, acc_e)\n",
    "    \"\"\"输出曲线\"\"\"\n",
    "    \"\"\"plt.plot(range(len(acc_s)), acc_s, label='start score')\n",
    "    plt.plot(range(len(acc_s)), acc_e, label='end score')\n",
    "    sun = []\n",
    "    for i in range(len(acc_s)):\n",
    "        sun.append((acc_s[i] + acc_e[i]) / 2)\n",
    "    print(sun)\n",
    "    plt.plot(range(len(acc_s)), sun, label='average')\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('Possibility')\n",
    "    plt.legend()\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "    \"\"\"\"获取最好的曲线并输出\"\"\"\n",
    "    \"\"\"max_start = 0\n",
    "    max_end = 0\n",
    "    max_ave = 0\n",
    "    for i in acc_s:\n",
    "        if i > max_start:\n",
    "            max_start = i\n",
    "    for j in acc_e:\n",
    "        if j > max_end:\n",
    "            max_end = i\n",
    "\n",
    "    for x in sun:\n",
    "        if x > max_ave:\n",
    "            max_ave = x\n",
    "\n",
    "    print(max_start, max_end, max_ave)\n",
    "\n",
    "    max_list = max_min(max_start, max_end, max_ave)\n",
    "    if max_list == 1:\n",
    "        plt.plot(range(len(acc_s)), acc_s, label='Possibility')\n",
    "        print(acc_s)\n",
    "    if max_list == 2:\n",
    "        plt.plot(range(len(acc_e)), acc_e, label='Possibility')\n",
    "        print(acc_e)\n",
    "    if max_list == 3:\n",
    "        plt.plot(range(len(sun)), sun, label='Possibility')\n",
    "        print(sun)\n",
    "\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('Possibility')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    for i in range(len(ans)):\n",
    "        print(ans[i])\"\"\"\n",
    "    #输出score\n",
    "    \"\"\"plt.plot(range(len(f1)), f1, label='f1 score')\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('f1 score')\n",
    "    plt.legend()\n",
    "    plt.show()\"\"\"\n",
    "    for i in range(len(acc_s)):\n",
    "        sun.append((acc_s[i] + acc_e[i]) / 2)\n",
    "    print(\"f1 : \", f1)\n",
    "    print(\"possibility1 : \", acc_s)\n",
    "    print(\"possibility2 : \", acc_e)\n",
    "    print(\"possibility average : \", sun)\n",
    "    return f1, acc_s, acc_e, sun\n",
    "\n",
    "\n",
    "\n",
    "datasets = load_dataset('squad')\n",
    "g_f1 = []\n",
    "g_accs = []\n",
    "g_acce = []\n",
    "g_sun = []\n",
    "for i in range(5):\n",
    "    text = datasets['train'][i]['context']\n",
    "    question = datasets['train'][i]['question']\n",
    "    answers = datasets['train'][i]['answers']\n",
    "    f1, acc_s, acc_e, sun = cycle_prediction(10, question, text, answers['text'][0])\n",
    "    g_f1.append(f1)\n",
    "    g_accs.append(acc_s)\n",
    "    g_acce.append(acc_e)\n",
    "    g_sun.append(sun)\n",
    "print(g_f1, g_accs ,g_acce ,g_sun )\n",
    "temp = 0\n",
    "explain_score = []\n",
    "for i in g_f1:\n",
    "    temp = temp + i[0]\n",
    "print('初始概率为：', temp/len(g_f1))\n",
    "temp_max = 0\n",
    "for i in g_f1:\n",
    "    for j in i:\n",
    "        if j > temp_max:\n",
    "            temp_max = (j)\n",
    "    explain_score.append(temp_max)\n",
    "    temp_max = 0\n",
    "best_score = 0\n",
    "for i in explain_score:\n",
    "    best_score = best_score + i\n",
    "print('解释概率为', best_score/len(explain_score))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d36166ea-6b54-48f3-a533-d03f428a52c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "解释概率为 0.8363636363636363\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "for i in explain_score:\n",
    "    best_score = best_score + i\n",
    "print('解释概率为', best_score/len(explain_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64a18112-64c1-494c-bdfd-6656d06c27f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 0.1818181818181818, 1.0]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explain_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f4074db-2492-4d15-94dc-5df2d728d721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\Guo Zikun\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd86a244f8124c62804c80a47688a68f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import collections\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "import difflib\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# replace <PATd:/spofrte/modeH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "model_path = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "\n",
    "# load model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "\"\"\"++++++++++++++++++这几个函数是计算f1 score 数值的，代码是抄的，千万不能改！+++++++++++++++++\"\"\"\n",
    "\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "\n",
    "def get_gold_answers(example):\n",
    "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
    "\n",
    "    gold_answers = [answer[\"text\"] for answer in example.answers if answer[\"text\"]]\n",
    "\n",
    "    # if gold_answers doesn't exist it's because this is a negative example -\n",
    "    # the only correct answer is an empty string\n",
    "    if not gold_answers:\n",
    "        gold_answers = [\"\"]\n",
    "\n",
    "    return gold_answers\n",
    "\n",
    "\n",
    "\"\"\"+++++++++++++++++++++++++++++++++++\"\"\"\n",
    "\n",
    "\n",
    "def string_similar(s1, s2):\n",
    "    return difflib.SequenceMatcher(None, s1, s2).quick_ratio()\n",
    "\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.start_logits, output.end_logits\n",
    "\n",
    "\n",
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(8, 6)\n",
    "\n",
    "ref_token_id = tokenizer.pad_token_id  # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id  # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id  # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "\n",
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "                    [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)  # * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.bert.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.bert.embeddings(ref_input_ids, token_type_ids=token_type_ids,\n",
    "                                                 position_ids=position_ids)\n",
    "\n",
    "    return input_embeddings, ref_input_embeddings\n",
    "\n",
    "\n",
    "def predict_qt(question, text):\n",
    "    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id,\n",
    "                                                                cls_token_id)\n",
    "    token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "    position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "    attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "    ground_truth = '13'\n",
    "\n",
    "    start_scores, end_scores = predict(input_ids, \\\n",
    "                                       token_type_ids=token_type_ids, \\\n",
    "                                       position_ids=position_ids, \\\n",
    "                                       attention_mask=attention_mask)\n",
    "\n",
    "    #print('Question: ', question)\n",
    "    #print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1]))\n",
    "    return input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens,\n",
    "\n",
    "\n",
    "def explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores,\n",
    "            ground_truth, all_tokens, ):\n",
    "    lig = LayerIntegratedGradients(squad_pos_forward_func, model.bert.embeddings)\n",
    "\n",
    "    attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "                                                    baselines=ref_input_ids,\n",
    "                                                    additional_forward_args=(\n",
    "                                                        token_type_ids, position_ids, attention_mask, 0),\n",
    "                                                    internal_batch_size=4,\n",
    "                                                    return_convergence_delta=True)\n",
    "    attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "                                                additional_forward_args=(\n",
    "                                                    token_type_ids, position_ids, attention_mask, 1),\n",
    "                                                internal_batch_size=4,\n",
    "                                                return_convergence_delta=True)\n",
    "\n",
    "    attributions_start_sum = summarize_attributions(attributions_start)\n",
    "    attributions_end_sum = summarize_attributions(attributions_end)\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    start_position_vis = viz.VisualizationDataRecord(\n",
    "        attributions_start_sum,\n",
    "        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "        torch.argmax(start_scores),\n",
    "        torch.argmax(start_scores),\n",
    "        str(ground_truth),\n",
    "        attributions_start_sum.sum(),\n",
    "        all_tokens,\n",
    "        delta_start)\n",
    "\n",
    "    end_position_vis = viz.VisualizationDataRecord(\n",
    "        attributions_end_sum,\n",
    "        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "        torch.argmax(end_scores),\n",
    "        torch.argmax(end_scores),\n",
    "        str(ground_truth),\n",
    "        attributions_end_sum.sum(),\n",
    "        all_tokens,\n",
    "        delta_end)\n",
    "    # print(all_tokens)\n",
    "    #print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "    #viz.visualize_text([start_position_vis])\n",
    "\n",
    "    #print('\\033[1m', 'Visualizations For End Position', '\\033[0m')\n",
    "\n",
    "    #print(\"attributions_start_sum:   \", len(attributions_start_sum))\n",
    "    # print(\"all tokens:    \", len(all_tokens))\n",
    "\n",
    "    return all_tokens, attributions_start_sum\n",
    "\n",
    "\n",
    "def get_posneg(all_tokens, attributions_start_sum):\n",
    "    positive = []\n",
    "    negative = []\n",
    "    neutral = []\n",
    "    for i, j in enumerate(attributions_start_sum):\n",
    "        if j > 0:\n",
    "            positive.append(i)\n",
    "            # print('positive:',j)\n",
    "        ##print(all_tokens[i])\n",
    "        elif j < 0:\n",
    "            negative.append(i)\n",
    "            # print('negative:',j)\n",
    "            # print(all_tokens[i])\n",
    "        elif j == 0:\n",
    "            neutral.append(i)\n",
    "\n",
    "    s_pos = ''\n",
    "    s_neg = ''\n",
    "\n",
    "    # print(len(attributions_start_sum))\n",
    "    # print(len(positive))\n",
    "    # print(len(negative))\n",
    "\n",
    "    for i in positive:\n",
    "        s_pos += all_tokens[i] + ' '\n",
    "    # print(\"positive :\", s_pos)\n",
    "    for i in negative:\n",
    "        s_neg += all_tokens[i] + ' '\n",
    "    # print(\"negative :\", s_neg)\n",
    "    return positive, negative, neutral\n",
    "\n",
    "\n",
    "def separate_sentence(all_tokens):\n",
    "    sentence = {}\n",
    "    temp = []\n",
    "    num = 0\n",
    "    for i in range(len(all_tokens)):\n",
    "        if all_tokens[i] == \",\" or all_tokens[i] == \".\":\n",
    "            temp.append(all_tokens[i])\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        elif all_tokens[i] == \"[CLS]\":\n",
    "            temp.append(all_tokens[i])\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        elif all_tokens[i] == \"[SEP]\":\n",
    "            sentence[num] = temp\n",
    "            num = num + 1\n",
    "            temp = [all_tokens[i]]\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        else:\n",
    "            temp.append(all_tokens[i])\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def get_sence_score(sentence, attributions_start_sum):\n",
    "    weight = 0\n",
    "    sum_weight = 0\n",
    "    sentence_value = []\n",
    "    delete_sentence = []\n",
    "    for k, v in sentence.items():\n",
    "        for i in v:\n",
    "            sentence_value.append(i)\n",
    "    scores = {}\n",
    "\n",
    "    for i in range(len(attributions_start_sum)):\n",
    "        try:\n",
    "            scores[sentence_value[i]] = attributions_start_sum[i].item()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i, j in sentence.items():\n",
    "        sum_weight = 0\n",
    "        for word in j:\n",
    "            sum_weight += scores[word]\n",
    "        delete_sentence.append(sum_weight)\n",
    "        # print(sum_weight)\n",
    "    return delete_sentence\n",
    "\n",
    "\n",
    "def get_delete(sentence):\n",
    "    weight = 0\n",
    "    sum_weight = 0\n",
    "    sentence_value = []\n",
    "    delete_sentence = {}\n",
    "    for k, v in sentence.items():\n",
    "        # print(k,':',v)\n",
    "        for i in v:\n",
    "            sentence_value.append(i)\n",
    "    # print(sentence_value)\n",
    "    scores = {}\n",
    "    # print(attributions_start_sum[0].item())\n",
    "\n",
    "    for i in range(len(attributions_start_sum)):\n",
    "        try:\n",
    "            scores[sentence_value[i]] = attributions_start_sum[i].item()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i, j in sentence.items():\n",
    "        sum_weight = 0\n",
    "        for word in j:\n",
    "            weight = 0\n",
    "\n",
    "            sum_weight += scores[word]\n",
    "            delete_sentence[i] = sum_weight\n",
    "    return delete_sentence\n",
    "\n",
    "\n",
    "def delete_sentence(sentence, li_delete_sentence):\n",
    "    for i, j in sentence.items():\n",
    "        if i in li_delete_sentence:\n",
    "            sentence[i] = []\n",
    "        else:\n",
    "            pass\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def rebuild_sentence(ori_sentence):\n",
    "    rebuild_str = \"\"\n",
    "    for i, j in ori_sentence.items():\n",
    "        for word in j:\n",
    "            rebuild_str += word\n",
    "            rebuild_str += \" \"\n",
    "    return rebuild_str\n",
    "\n",
    "\n",
    "def pred_explain(question, text):\n",
    "    input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens, = predict_qt(question, text)\n",
    "\n",
    "    all_tokens, attributions_start_sum = explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask,\n",
    "                                                 start_scores, end_scores, ground_truth, all_tokens, )\n",
    "\n",
    "    end_score = float(torch.max(torch.softmax(end_scores[0], dim=0)))\n",
    "    start_score = float(torch.max(torch.softmax(start_scores[0], dim=0)))\n",
    "    return all_tokens, attributions_start_sum, end_score, start_score, [torch.argmax(start_scores), torch.argmax(\n",
    "        end_scores) + 1], start_scores, end_scores\n",
    "\n",
    "\n",
    "def max_min(x, y, z):\n",
    "    max = min = x\n",
    "    i = 1\n",
    "    if y > max:\n",
    "        max = y\n",
    "        i = 2\n",
    "    else:\n",
    "        min = y\n",
    "    if z > max:\n",
    "        max = z\n",
    "        i = 3\n",
    "    else:\n",
    "        min = z\n",
    "    return (i)\n",
    "\n",
    "def analysis(f1, acc_s, acc_e, sun):\n",
    "    plt.plot(range(len(f1)), f1, \"--bo\", label=\"f1 score\")\n",
    "    plt.show()\n",
    "    plt.plot(range(len(acc_s)), acc_s)\n",
    "    plt.plot(range(len(acc_e)), acc_e)\n",
    "    plt.plot(range(len(sun)), sun)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def cycle_prediction(cycle_num, question, text, s_answer):\n",
    "    all_tokens, attributions_start_sum, start_acc, end_acc, an_index, start_scores, end_scores = pred_explain(text,\n",
    "                                                                                                              question)\n",
    "\n",
    "    f1 = []\n",
    "    acc_s = []\n",
    "    acc_e = []\n",
    "    sun = []\n",
    "    ans = []\n",
    "    second_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "    second_answer = re.sub(r' ##', '', second_answer)\n",
    "    f1_score = compute_f1(second_answer, s_answer)\n",
    "    f1.append(f1_score)\n",
    "    for loop in range(cycle_num):\n",
    "        sentence = separate_sentence(all_tokens)\n",
    "        sentence_score = get_sence_score(sentence, attributions_start_sum)\n",
    "        min_sensocer = 999\n",
    "        \"\"\"min_index = 999\n",
    "        for i in range(len(sentence_score)):\n",
    "            if sentence_score[i] < min_sensocer and sentence_score[i] != 0:\n",
    "                min_sensocer = sentence_score[i]\n",
    "                min_index = i\"\"\"\n",
    "        close20_index = 999\n",
    "        for i in range(len(sentence_score)):\n",
    "            if abs(sentence_score[i]) < abs(min_sensocer) and sentence_score[i] != 0:\n",
    "                min_sensocer = sentence_score[i]\n",
    "                close20_index = i\n",
    "        # print(\"should delete\", min_index, min_sensocer)\n",
    "        sentence[close20_index] = ''\n",
    "        sentence[1] = ''\n",
    "        retext = \"\"\n",
    "        for i, j in sentence.items():\n",
    "            for words in j:\n",
    "                retext = retext + words + \" \"\n",
    "        li_sep = []\n",
    "        for m in re.finditer(r\"SEP\", retext):\n",
    "            li_sep.append(m.start())\n",
    "            li_sep.append(m.end())\n",
    "        retext = retext[li_sep[1] + 1: li_sep[2] - 1]\n",
    "        retext = re.sub(r' ##', '', retext)\n",
    "\n",
    "        all_tokens, attributions_start_sum, start_acc, end_acc, an_index, start_scores, end_scores = pred_explain(\n",
    "            question, retext)\n",
    "        reanswer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "        # print(start_acc, end_acc)\n",
    "        second_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "        second_answer = re.sub(r' ##', '', second_answer)\n",
    "        # print(\"my answer is \", second_answer)\n",
    "        ans.append(second_answer)\n",
    "        # print(start_acc, end_acc)\n",
    "        acc_s.append(start_acc)\n",
    "        acc_e.append(end_acc)\n",
    "        pos_contri = 0\n",
    "        neg_contri = 0\n",
    "        f1_score = compute_f1(second_answer, s_answer)\n",
    "        f1.append(f1_score)\n",
    "\n",
    "        # print(acc_s, acc_e)\n",
    "        # print(acc_s, acc_e)\n",
    "    \"\"\"输出曲线\"\"\"\n",
    "    \"\"\"plt.plot(range(len(acc_s)), acc_s, label='start score')\n",
    "    plt.plot(range(len(acc_s)), acc_e, label='end score')\n",
    "    sun = []\n",
    "    for i in range(len(acc_s)):\n",
    "        sun.append((acc_s[i] + acc_e[i]) / 2)\n",
    "    print(sun)\n",
    "    plt.plot(range(len(acc_s)), sun, label='average')\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('Possibility')\n",
    "    plt.legend()\n",
    "    plt.show()\"\"\"\n",
    "\n",
    "    \"\"\"\"获取最好的曲线并输出\"\"\"\n",
    "    \"\"\"max_start = 0\n",
    "    max_end = 0\n",
    "    max_ave = 0\n",
    "    for i in acc_s:\n",
    "        if i > max_start:\n",
    "            max_start = i\n",
    "    for j in acc_e:\n",
    "        if j > max_end:\n",
    "            max_end = i\n",
    "\n",
    "    for x in sun:\n",
    "        if x > max_ave:\n",
    "            max_ave = x\n",
    "\n",
    "    print(max_start, max_end, max_ave)\n",
    "\n",
    "    max_list = max_min(max_start, max_end, max_ave)\n",
    "    if max_list == 1:\n",
    "        plt.plot(range(len(acc_s)), acc_s, label='Possibility')\n",
    "        print(acc_s)\n",
    "    if max_list == 2:\n",
    "        plt.plot(range(len(acc_e)), acc_e, label='Possibility')\n",
    "        print(acc_e)\n",
    "    if max_list == 3:\n",
    "        plt.plot(range(len(sun)), sun, label='Possibility')\n",
    "        print(sun)\n",
    "\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('Possibility')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    for i in range(len(ans)):\n",
    "        print(ans[i])\"\"\"\n",
    "    #输出score\n",
    "    \"\"\"plt.plot(range(len(f1)), f1, label='f1 score')\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('f1 score')\n",
    "    plt.legend()\n",
    "    plt.show()\"\"\"\n",
    "    for i in range(len(acc_s)):\n",
    "        sun.append((acc_s[i] + acc_e[i]) / 2)\n",
    "    return f1, acc_s, acc_e, sun\n",
    "\n",
    "def muti_pre(cycle_num, question, text, s_answer, pro_keep, pro_next):\n",
    "    input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens, = predict_qt(question, text)\n",
    "    end_acc = float(torch.max(torch.softmax(end_scores[0], dim=0)))\n",
    "    start_acc = float(torch.max(torch.softmax(start_scores[0], dim=0)))\n",
    "    if end_acc > pro_next or start_acc > pro_next:\n",
    "        return [compute_f1(' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1]), s_answer)], [start_acc], [end_acc], [(start_acc+end_acc)/2]\n",
    "\n",
    "    all_tokens, attributions_start_sum, start_acc, end_acc, an_index, start_scores, end_scores = pred_explain(question, text)\n",
    "\n",
    "    f1 = []\n",
    "    acc_s = []\n",
    "    acc_e = []\n",
    "    sun = []\n",
    "    ans = []\n",
    "    second_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "    second_answer = re.sub(r' ##', '', second_answer)\n",
    "    f1_score = compute_f1(second_answer, s_answer)\n",
    "    f1.append(f1_score)\n",
    "    for loop in range(cycle_num):\n",
    "        sentence = separate_sentence(all_tokens)\n",
    "        sentence_score = get_sence_score(sentence, attributions_start_sum)\n",
    "        min_sensocer = 999\n",
    "        close20_index = 999\n",
    "        for i in range(len(sentence_score)):\n",
    "            if abs(sentence_score[i]) < abs(min_sensocer) and sentence_score[i] != 0:\n",
    "                min_sensocer = sentence_score[i]\n",
    "                close20_index = i\n",
    "        # print(\"should delete\", min_index, min_sensocer)\n",
    "        sentence[close20_index] = ''\n",
    "        sentence[1] = ''\n",
    "        retext = \"\"\n",
    "        for i, j in sentence.items():\n",
    "            for words in j:\n",
    "                retext = retext + words + \" \"\n",
    "        li_sep = []\n",
    "        for m in re.finditer(r\"SEP\", retext):\n",
    "            li_sep.append(m.start())\n",
    "            li_sep.append(m.end())\n",
    "        retext = retext[li_sep[1] + 1: li_sep[2] - 1]\n",
    "        retext = re.sub(r' ##', '', retext)\n",
    "\n",
    "\n",
    "        all_tokens, attributions_start_sum, start_acc, end_acc, an_index, start_scores, end_scores = pred_explain(\n",
    "            question, retext)\n",
    "\n",
    "\n",
    "        reanswer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "        # print(start_acc, end_acc)\n",
    "        second_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "        second_answer = re.sub(r' ##', '', second_answer)\n",
    "        # print(\"my answer is \", second_answer)\n",
    "        ans.append(second_answer)\n",
    "        # print(start_acc, end_acc)\n",
    "        acc_s.append(start_acc)\n",
    "        acc_e.append(end_acc)\n",
    "        pos_contri = 0\n",
    "        neg_contri = 0\n",
    "        f1_score = compute_f1(second_answer, s_answer)\n",
    "        f1.append(f1_score)\n",
    "\n",
    "    for i in range(len(acc_s)):\n",
    "        sun.append((acc_s[i] + acc_e[i]) / 2)\n",
    "    return f1, acc_s, acc_e, sun\n",
    "\n",
    "\n",
    "\n",
    "datasets = load_dataset('squad')\n",
    "g_f1 = []\n",
    "g_accs = []\n",
    "g_acce = []\n",
    "g_sun = []\n",
    "handle = {}\n",
    "wrong_ids = []\n",
    "for i in range(1):\n",
    "    text = datasets['train'][i]['context']\n",
    "    question = datasets['train'][i]['question']\n",
    "    answers = datasets['train'][i]['answers']\n",
    "    f1, acc_s, acc_e, sun = muti_pre(10, question, text, answers['text'][0], 0.9, 0.7)\n",
    "    f1_change = f1[sun.index(max(sun))] - f1[0]\n",
    "    pro_change0 = float(max(sun)-sun[0])\n",
    "    pro_change1 = float(max(acc_s)-acc_s[0])\n",
    "    pro_change2 = float(max(acc_e)-acc_e[0])\n",
    "    if len(f1) != 1:\n",
    "        print(\"第\", i ,\"个的f1 score变化是\", f1_change, f1,  \"概率的变化是\", pro_change0, pro_change1, pro_change2, acc_s, acc_e, sun)\n",
    "\n",
    "    g_f1.append(f1)\n",
    "    g_accs.append(acc_s)\n",
    "    g_acce.append(acc_e)\n",
    "    g_sun.append(sun)\n",
    "#print(g_f1, g_accs ,g_acce ,g_sun )\n",
    "handle[\"f1\"] = g_f1\n",
    "handle[\"forword_pro\"] = g_accs\n",
    "handle[\"backword_pro\"] = g_acce\n",
    "handle[\"sun_pro\"] = g_sun\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a170dd29-8b7b-449d-92f7-94600ec0d36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "with open(r\"answers.txt\")as f:\n",
    "    lines = f.readlines()\n",
    "#删除所有的空行\n",
    "for line in lines:\n",
    "    if line == '\\n' or len(line) == 0:\n",
    "        lines.remove(line)\n",
    "#删除最后得回车换行符\n",
    "lines = [line[:len(line) - 1] for line in lines]\n",
    "print(lines[:6])\n",
    "ids = []\n",
    "truths = []\n",
    "answers = []\n",
    "atributions = []\n",
    "foeword_atten = []\n",
    "backword_atten = []\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    try:\n",
    "        if i % 6 == 0 and i != 0:\n",
    "            ids.append(lines[i - 6])\n",
    "            truths.append(lines[i - 5])\n",
    "            answers.append(ast.literal_eval(lines[i - 4]))\n",
    "            atributions.append(lines[i - 3])\n",
    "            foeword_atten.append(lines[i - 2])\n",
    "            backword_atten.append(lines[i - 1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "\n",
    "\n",
    "acc = 0\n",
    "temp = 0\n",
    "for i in range((965)):\n",
    "    f1 = 0\n",
    "    f1 = compute_f1(answers[i][0], truths[i])\n",
    "    acc += f1\n",
    "    temp +=1\n",
    "    print(answers[i][0])\n",
    "    print(temp,\"--\",f1, \"*\"*144)\n",
    "    print(truths[i])\n",
    "\n",
    "print(acc/temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87272f9-3801-4e17-9536-6881b0559f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "with open(r\"answers.txt\")as f:\n",
    "    lines = f.readlines()\n",
    "#删除所有的空行\n",
    "for line in lines:\n",
    "    if line == '\\n' or len(line) == 0:\n",
    "        lines.remove(line)\n",
    "#删除最后得回车换行符\n",
    "lines = [line[:len(line) - 1] for line in lines]\n",
    "print(lines[:6])\n",
    "ids = []\n",
    "truths = []\n",
    "answers = []\n",
    "atributions = []\n",
    "foeword_atten = []\n",
    "backword_atten = []\n",
    "\n",
    "for i in range(len(lines)):\n",
    "    try:\n",
    "        if i % 6 == 0 and i != 0:\n",
    "            ids.append(lines[i - 6])\n",
    "            truths.append(lines[i - 5])\n",
    "            answers.append(ast.literal_eval(lines[i - 4]))\n",
    "            atributions.append(lines[i - 3])\n",
    "            foeword_atten.append(lines[i - 2])\n",
    "            backword_atten.append(lines[i - 1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "\n",
    "best_answersids = []\n",
    "acc = 0\n",
    "temp = 0\n",
    "str_tensor = []\n",
    "for i in atributions[0:965]:\n",
    "    value = i.split(\",\")[::3]\n",
    "    value = [float(i[8:]) for i in value]\n",
    "    str_tensor.append((value))\n",
    "\n",
    "\n",
    "\n",
    "for onetest_answers in str_tensor:\n",
    "    best_answersids.append(onetest_answers.index(max(onetest_answers)))\n",
    "\n",
    "\n",
    "for i in range((965)):\n",
    "    f1 = 0\n",
    "    f1 = compute_f1(answers[i][best_answersids[i]], truths[i])\n",
    "    acc += f1\n",
    "    temp +=1\n",
    "    print(answers[i][best_answersids[i]])\n",
    "    print(temp,\"--\",f1, \"*\"*144)\n",
    "    print(truths[i])\n",
    "\n",
    "print(acc/temp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

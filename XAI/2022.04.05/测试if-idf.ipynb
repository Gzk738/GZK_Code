{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc93eebe-c067-4eef-8ac7-033ce958beee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\GZK\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3fc071aec24cdbaa88bb07581fac08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "824efd6d-60e2-4e63-9c94-0256a0c9ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Architecturally, the school has a Catholic character.\n",
      "-----> Atop the Main Building's gold dome is a golden statue of the Virgin Mary.\n",
      "-----> Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\".\n",
      "-----> Next to the Main Building is the Basilica of the Sacred Heart.\n",
      "-----> Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection.\n",
      "-----> It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858.\n",
      "-----> At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    }
   ],
   "source": [
    "import nltk as tk\n",
    "texts = squad['train']['context']\n",
    "text_n = []\n",
    "tokens = tk.sent_tokenize(texts[0])\n",
    "for token in tokens:\n",
    "    print(\"----->\",token)\n",
    "    text_n.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "80ce12e6-cd26-445f-8a1b-8ac2d5f8d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import math\n",
    "import string \n",
    "import nltk.stem\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "#设置三段文本\n",
    "text_1 = \"In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Tf–idf is one of the most popular term-weighting schemes today; 83% of text-based recommender systems in digital libraries use tf–idf.\"\n",
    "text_2 = \"Variations of the tf–idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query. tf–idf can be successfully used for stop-words filtering in various subject fields, including text summarization and classification.\"\n",
    "text_3 = \"One of the simplest ranking functions is computed by summing the tf–idf for each query term; many more sophisticated ranking functions are variants of this simple model.\"\n",
    "\n",
    "punctuation_map = dict((ord(char), None) for char in string.punctuation)  #引入标点符号，为下步去除标点做准备\n",
    "s = nltk.stem.SnowballStemmer('english')   #在提取词干时,语言使用英语,使用的语言是英语\n",
    "\n",
    "def stem_count(text):\n",
    "    l_text = text.lower()     #全部转化为小写以方便处理 \n",
    "    without_punctuation = l_text.translate(punctuation_map)    #去除文章标点符号\n",
    "    tokens = nltk.word_tokenize(without_punctuation)        #将文章进行分词处理,将一段话转变成一个list\n",
    "    without_stopwords = [w for w in tokens if not w in stopwords.words('english')]    #去除文章的停用词\n",
    "    cleaned_text = [] \n",
    "    for i in range(len(without_stopwords)):\n",
    "        cleaned_text.append(s.stem(without_stopwords[i]))    #提取词干\n",
    "    count = Counter(cleaned_text)                 #实现计数功能\n",
    "    return count\n",
    "\n",
    "#定义TF-IDF的计算过程\n",
    "def D_con(word, count_list): \n",
    "    D_con = 0\n",
    "    for count in count_list:\n",
    "        if word in count:\n",
    "            D_con += 1\n",
    "    return D_con\n",
    "def tf(word, count): \n",
    "    return count[word] / sum(count.values())\n",
    "def idf(word, count_list): \n",
    "    return math.log(len(count_list)) / (1 + D_con(word, count_list))\n",
    "def tfidf(word, count, count_list):\n",
    "    return tf(word, count) * idf(word, count_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40ee3d63-e83c-4684-b1b6-18486c4c75ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q ： What color is the dome of the main building\n"
     ]
    }
   ],
   "source": [
    "a = \"Q ： What color is the dome of the main building\"\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8e44150e-2789-401f-b9d4-ba051cded5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For document 1\n",
      "\tWord: architectur : 0.243239\n",
      "\tWord: school : 0.243239\n",
      "\tWord: cathol : 0.243239\n",
      "\tWord: charact : 0.243239\n",
      "For document 2\n",
      "\tWord: atop : 0.108106\n",
      "\tWord: golden : 0.108106\n",
      "\tWord: gold : 0.072071\n",
      "\tWord: dome : 0.072071\n",
      "\tWord: virgin : 0.072071\n",
      "\tWord: build : 0.054053\n",
      "\tWord: statu : 0.054053\n",
      "For document 3\n",
      "\tWord: front : 0.069497\n",
      "\tWord: face : 0.069497\n",
      "\tWord: copper : 0.069497\n",
      "\tWord: christ : 0.069497\n",
      "\tWord: arm : 0.069497\n",
      "\tWord: uprais : 0.069497\n",
      "\tWord: legend : 0.069497\n",
      "For document 4\n",
      "\tWord: next : 0.162159\n",
      "\tWord: sacr : 0.162159\n",
      "\tWord: heart : 0.162159\n",
      "\tWord: basilica : 0.108106\n",
      "\tWord: build : 0.08108\n",
      "\tWord: main : 0.064864\n",
      "For document 5\n",
      "\tWord: behind : 0.121619\n",
      "\tWord: marian : 0.121619\n",
      "\tWord: place : 0.121619\n",
      "\tWord: prayer : 0.121619\n",
      "\tWord: reflect : 0.121619\n",
      "\tWord: immedi : 0.08108\n",
      "\tWord: basilica : 0.08108\n",
      "For document 6\n",
      "\tWord: replica : 0.08108\n",
      "\tWord: lourd : 0.08108\n",
      "\tWord: franc : 0.08108\n",
      "\tWord: reput : 0.08108\n",
      "\tWord: appear : 0.08108\n",
      "\tWord: saint : 0.08108\n",
      "\tWord: bernadett : 0.08108\n",
      "For document 7\n",
      "\tWord: end : 0.064864\n",
      "\tWord: drive : 0.064864\n",
      "\tWord: direct : 0.064864\n",
      "\tWord: line : 0.064864\n",
      "\tWord: connect : 0.064864\n",
      "\tWord: 3 : 0.064864\n",
      "\tWord: statu : 0.064864\n"
     ]
    }
   ],
   "source": [
    "texts = [text_1, text_2, text_3] \n",
    "count_list = [] \n",
    "for text in text_n: \n",
    "    count_list.append(stem_count(text))      #填入清洗好后的文本\n",
    "for i in range(len(count_list)):\n",
    "    print('For document {}'.format(i+1))\n",
    "    tf_idf = {}\n",
    "    for word in count_list[i]:\n",
    "        tf_idf[word] = tfidf(word, count_list[i], count_list)\n",
    "    sort = sorted(tf_idf.items(), key = lambda x: x[1], reverse=True) #将集合按照TF-IDF值从大到小排列\n",
    "    for word, tf_idf in sort[:7]: \n",
    "        print(\"\\tWord: {} : {}\".format(word, round(tf_idf, 6)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "396701b8-1af1-476e-8c8c-12e48cf2fcc7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sentence_transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msentence_transformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceTransformer\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m SentenceTransformer(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparaphrase-MiniLM-L6-v2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#Our sentences we like to encode\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sentence_transformers'"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "\n",
    "#Our sentences we like to encode\n",
    "sentences = ['This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.',\n",
    "    'The quick brown fox jumps over the lazy dog.']\n",
    "\n",
    "#Sentences are encoded by calling model.encode()\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "#Print the embeddings\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0cac054-c93a-49f3-a214-6aefcf7be8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (2.2.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sentence-transformers) (0.1.91)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sentence-transformers) (4.18.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sentence-transformers) (4.63.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sentence-transformers) (1.21.5)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sentence-transformers) (1.0.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sentence-transformers) (1.7.3)\n",
      "Requirement already satisfied: nltk in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sentence-transformers) (3.7)\n",
      "Requirement already satisfied: torch>=1.6.0 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sentence-transformers) (1.10.2)\n",
      "Requirement already satisfied: torchvision in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sentence-transformers) (0.11.3)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sentence-transformers) (0.5.1)\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (4.2.0)\n",
      "Requirement already satisfied: requests in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.27.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: sacremoses in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.49)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.8.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.4)\n",
      "Requirement already satisfied: click in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from nltk->sentence-transformers) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from nltk->sentence-transformers) (1.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: six in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\gzk\\anaconda3\\envs\\xai\\lib\\site-packages (from torchvision->sentence-transformers) (9.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U sentence-transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

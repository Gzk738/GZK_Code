{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb7b248e",
   "metadata": {},
   "source": [
    "# LIME to Inspect Image & Text Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d83024f",
   "metadata": {},
   "source": [
    "This tutorial focuses on showing how to use Captum's implementation of Local Interpretable Model-agnostic Explanations (LIME) to understand neural models. The following content is divided into an image classification section to present our high-level interface `Lime` class and a text classification section for the more customizable low-level interface `LimeBase`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "701565a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import Lime, LimeBase\n",
    "from captum._utils.models.linear_model import SkLearnLinearRegression, SkLearnLasso\n",
    "\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c3fdb5",
   "metadata": {},
   "source": [
    "## 1. Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dadfc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18\n",
    "from torchvision.datasets import VOCSegmentation\n",
    "import torchvision.transforms as T\n",
    "from captum.attr._core.lime import get_exp_kernel_similarity_function\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a47ad21",
   "metadata": {},
   "source": [
    "### 1.1 Load the model and dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a5b7ca",
   "metadata": {},
   "source": [
    "As we can see, the result is pretty reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7fde11",
   "metadata": {},
   "source": [
    "## 1.3 Inspect the model prediction with Lime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5942feeb",
   "metadata": {},
   "source": [
    "In this section, we will bring in LIME from Captum to analyze how the Resnet made above prediction based on the sample image.\n",
    "\n",
    "Like many other Captum algorithms, Lime also support analyzing a number of input features together as a group. This is very useful when dealing with images, where each color channel in each pixel is an input feature. Such group is also refered as \"super-pixel\". To define our desired groups over input features, all we need is to provide a feature mask.\n",
    "\n",
    "In case of an image input, the feature mask is a 2D image of the same size, where each pixel in the mask indicates the feature group it belongs to via an integer value. Pixels of the same value define a group.\n",
    "\n",
    "This means we can readily use VOC's segmentation masks as feature masks for Captum! However, while segmentaion numbers range from 0 to 255, Captum prefers consecutive group IDs for efficiency. Therefore, we will also include extra steps to convert mask IDs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7c2111",
   "metadata": {},
   "source": [
    "## 2. Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e4bd3",
   "metadata": {},
   "source": [
    "In this section, we will take use of a news subject classification example to demonstrate more customizable functions in Lime. We will train a simple embedding-bag classifier on AG_NEWS dataset and analyze its understanding of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "509854e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method tqdm.__del__ of <tqdm.auto.tqdm object at 0x000002437DF5A160>>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\software\\conda\\envs\\xai\\lib\\site-packages\\tqdm\\std.py\", line 1152, in __del__\n",
      "    self.close()\n",
      "  File \"D:\\software\\conda\\envs\\xai\\lib\\site-packages\\tqdm\\notebook.py\", line 283, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.datasets import AG_NEWS\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from IPython.core.display import HTML, display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826aa5e7",
   "metadata": {},
   "source": [
    "### 2.1 Load the data and define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072c58a",
   "metadata": {},
   "source": [
    "`torchtext` has included the AG_NEWS dataset but since it is only split into train & test, we need to further cut a validation set from the original train split. Then we build the vocabulary of the frequent words based on our train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "986866ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 86716\n",
      "Num of classes: 4\n"
     ]
    }
   ],
   "source": [
    "ag_ds = list(AG_NEWS(split='train'))\n",
    "\n",
    "ag_train, ag_val = ag_ds[:100000], ag_ds[100000:]\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "word_counter = Counter()\n",
    "for (label, line) in ag_train:\n",
    "    word_counter.update(tokenizer(line))\n",
    "voc = Vocab(word_counter)\n",
    "\n",
    "print('Vocabulary size:', len(voc))\n",
    "\n",
    "num_class = len(set(label for label, _ in ag_train))\n",
    "print('Num of classes:', num_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c2895d",
   "metadata": {},
   "source": [
    "The model we use is composed of an embedding-bag, which averages the word embeddings as the latent text representation, and a final linear layer, which maps the latent vector to the logits. Unconventially, `pytorch`'s embedding-bag does not assume the first dimension is batch. Instead, it requires a flattened vector of indices with an additional offset tensor to mark the starting position of each example. You can refer to its [documentation](https://pytorch.org/docs/stable/generated/torch.nn.EmbeddingBag.html#embeddingbag) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a3a83cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingBagModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "        self.linear = nn.Linear(embed_dim, num_class)\n",
    "\n",
    "    def forward(self, inputs, offsets):\n",
    "        embedded = self.embedding(inputs, offsets)\n",
    "        return self.linear(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14853e7",
   "metadata": {},
   "source": [
    "### 2.2 Training and Baseline Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26491c6f",
   "metadata": {},
   "source": [
    "In order to train our classifier, we need to define a collate function to batch the samples into the tensor fomat required by the embedding-bag and create the interable dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5219e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "def collate_batch(batch):\n",
    "    labels = torch.tensor([label - 1 for label, _ in batch]) \n",
    "    text_list = [tokenizer(line) for _, line in batch]\n",
    "    \n",
    "    # flatten tokens across the whole batch\n",
    "    text = torch.tensor([voc[t] for tokens in text_list for t in tokens])\n",
    "    # the offset of each example\n",
    "    offsets = torch.tensor(\n",
    "        [0] + [len(tokens) for tokens in text_list][:-1]\n",
    "    ).cumsum(dim=0)\n",
    "\n",
    "    return labels, text, offsets\n",
    "\n",
    "train_loader = DataLoader(ag_train, batch_size=BATCH_SIZE,\n",
    "                          shuffle=True, collate_fn=collate_batch)\n",
    "val_loader = DataLoader(ag_val, batch_size=BATCH_SIZE,\n",
    "                        shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef056fa",
   "metadata": {},
   "source": [
    "We will then train our embedding-bag model with the common cross-entropy loss and Adam optimizer. Due to the simplicity of this task, 5 epochs should be enough to give us a stable 90% validation accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "456afa78",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './models/embedding_bag_ag_news.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-644ff86f2f11>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m \u001b[0meb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCHECKPOINT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mUSE_PRETRAINED\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\software\\conda\\envs\\xai\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 594\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    595\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\envs\\xai\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    229\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    232\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\software\\conda\\envs\\xai\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    212\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './models/embedding_bag_ag_news.pt'"
     ]
    }
   ],
   "source": [
    "EPOCHS = 7\n",
    "EMB_SIZE = 64\n",
    "CHECKPOINT = './models/embedding_bag_ag_news.pt'\n",
    "USE_PRETRAINED = True  # change to False if you want to retrain your own model\n",
    "\n",
    "def train_model(train_loader, val_loader):\n",
    "    model = EmbeddingBagModel(len(voc), EMB_SIZE, num_class)\n",
    "    \n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    \n",
    "    for epoch in range(1, EPOCHS + 1):      \n",
    "        # training\n",
    "        model.train()\n",
    "        total_acc, total_count = 0, 0\n",
    "        \n",
    "        for idx, (label, text, offsets) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            predited_label = model(text, offsets)\n",
    "            loss(predited_label, label).backward()\n",
    "            optimizer.step()\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "\n",
    "            if (idx + 1) % 500 == 0:\n",
    "                print('epoch {:3d} | {:5d}/{:5d} batches | accuracy {:8.3f}'.format(\n",
    "                    epoch, idx + 1, len(train_loader), total_acc / total_count\n",
    "                ))\n",
    "                total_acc, total_count = 0, 0       \n",
    "        \n",
    "        # evaluation\n",
    "        model.eval()\n",
    "        total_acc, total_count = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for label, text, offsets in val_loader:\n",
    "                predited_label = model(text, offsets)\n",
    "                total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "                total_count += label.size(0)\n",
    "\n",
    "        print('-' * 59)\n",
    "        print('end of epoch {:3d} | valid accuracy {:8.3f} '.format(epoch, total_acc / total_count))\n",
    "        print('-' * 59)\n",
    "    \n",
    "    torch.save(model, CHECKPOINT)\n",
    "    return model\n",
    "        \n",
    "eb_model = torch.load(CHECKPOINT) if USE_PRETRAINED else train_model(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865ca850",
   "metadata": {},
   "source": [
    "Now, let us take the following sports news and test how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3f9475da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eb_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-be8c02740f43>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_offsets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollate_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mprobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meb_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_offsets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Prediction probability:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'eb_model' is not defined"
     ]
    }
   ],
   "source": [
    "test_label = 2  # {1: World, 2: Sports, 3: Business, 4: Sci/Tec}\n",
    "test_line = ('US Men Have Right Touch in Relay Duel Against Australia THENS, Aug. 17 '\n",
    "            '- So Michael Phelps is not going to match the seven gold medals won by Mark Spitz. '\n",
    "            'And it is too early to tell if he will match Aleksandr Dityatin, '\n",
    "            'the Soviet gymnast who won eight total medals in 1980.')\n",
    "\n",
    "test_labels, test_text, test_offsets = collate_batch([(test_label, test_line)])\n",
    "\n",
    "probs = F.softmax(eb_model(test_text, test_offsets), dim=1).squeeze(0)\n",
    "print('Prediction probability:', round(probs[test_labels[0]].item(), 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9531ba98",
   "metadata": {},
   "source": [
    "Our embedding-bag does successfully identify the above news as sports with pretty high confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eac320",
   "metadata": {},
   "source": [
    "### 2.3 Inspect the model prediction with Lime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be15d9b",
   "metadata": {},
   "source": [
    "Finally, it is time to bring back Lime to inspect how the model makes the prediction. However, we will use the more customizable `LimeBase` class this time which is also the low-level implementation powering the `Lime` class we used before. The `Lime` class is opinionated when creating features from perturbed binary interpretable representations. It can only set the \"absense\" features to some baseline values while keeping other \"presense\" features. This is not what we want in this case. For text, the interpretable representation is a binary vector indicating if the word of each position is present or not. The corresponding text input should literally remove the absent words so our embedding-bag can calculate the average embeddings of the left words. Setting them to any baselines will pollute the calculation and moreover, our embedding-bag does not have common baseline tokens like `<padding>` at all. Therefore, we have to use `LimeBase` to customize the conversion logic through the `from_interp_rep_transform` argument.\n",
    "\n",
    "`LimeBase` is not opinionated at all so we have to define every piece manually. Let us talk about them in order:\n",
    "- `forward_func`, the forward function of the model. Notice we cannot pass our model directly since Captum always assumes the first dimension is batch while our embedding-bag requires flattened indices. So we will add the dummy dimension later when calling `attribute` and make a wrapper here to remove the dummy dimension before giving to our model.\n",
    "- `interpretable_model`, the surrogate model. This works the same as we demonstrated in the above image classification example. We also use sklearn linear lasso here.\n",
    "- `similarity_func`, the function calculating the weights for training samples. The most common distance used for texts is the cosine similarity in their latent embedding space. The text inputs are just sequences of token indices, so we have to leverage the trained embedding layer from the model to encode them to their latent vectors. Due to this extra encoding step, we cannot use the util `get_exp_kernel_similarity_function('cosine')` like in the image classification example, which directly calculate the cosine similarity of the given inputs.\n",
    "- `perturb_func`, the function to sample interpretable representations. We present another way to define this argument other than using generator as shown in the above image classification example. Here we directly define a function returning a randomized sample every call. It outputs a binary vector where each token is selected independently and uniformly at random.\n",
    "- `perturb_interpretable_space`, whether perturbed samples are in interpretable space. `LimeBase` also supports sampling in the original input space, but we do not need it in our case.\n",
    "- `from_interp_rep_transform`, the function transforming the perturbed interpretable samples back to the original input space. As explained above, this argument is the main reason for us to use `LimeBase`. We pick the subset of the present tokens from the original text input according to the interpretable representation.\n",
    "- `to_interp_rep_transform`, the opposite of `from_interp_rep_transform`. It is needed only when `perturb_interpretable_space` is set to false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32a5251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the batch dimension for the embedding-bag model\n",
    "def forward_func(text, offsets):\n",
    "    return eb_model(text.squeeze(0), offsets)\n",
    "\n",
    "# encode text indices into latent representations & calculate cosine similarity\n",
    "def exp_embedding_cosine_distance(original_inp, perturbed_inp, _, **kwargs):\n",
    "    original_emb = eb_model.embedding(original_inp, None)\n",
    "    perturbed_emb = eb_model.embedding(perturbed_inp, None)\n",
    "    distance = 1 - F.cosine_similarity(original_emb, perturbed_emb, dim=1)\n",
    "    return torch.exp(-1 * (distance ** 2) / 2)\n",
    "\n",
    "# binary vector where each word is selected independently and uniformly at random\n",
    "def bernoulli_perturb(text, **kwargs):\n",
    "    probs = torch.ones_like(text) * 0.5\n",
    "    return torch.bernoulli(probs).long()\n",
    "\n",
    "# remove absenst token based on the intepretable representation sample\n",
    "def interp_to_input(interp_sample, original_input, **kwargs):\n",
    "    return original_input[interp_sample.bool()].view(original_input.size(0), -1)\n",
    "\n",
    "lasso_lime_base = LimeBase(\n",
    "    forward_func, \n",
    "    interpretable_model=SkLearnLasso(alpha=0.08),\n",
    "    similarity_func=exp_embedding_cosine_distance,\n",
    "    perturb_func=bernoulli_perturb,\n",
    "    perturb_interpretable_space=True,\n",
    "    from_interp_rep_transform=interp_to_input,\n",
    "    to_interp_rep_transform=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8f0fe",
   "metadata": {},
   "source": [
    "The attribution call is the same as the `Lime` class. Just remember to add the dummy batch dimension to the text input and put the offsets in the `additional_forward_args` because it is not a feature for the classification but a metadata for the text input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea35ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs = lasso_lime_base.attribute(\n",
    "    test_text.unsqueeze(0), # add batch dimension for Captum\n",
    "    target=test_labels,\n",
    "    additional_forward_args=(test_offsets,),\n",
    "    n_samples=32000,\n",
    "    show_progress=True\n",
    ").squeeze(0)\n",
    "\n",
    "print('Attribution range:', attrs.min().item(), 'to', attrs.max().item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fac054",
   "metadata": {},
   "source": [
    "At last, let us create a simple visualization to highlight the influential words where green stands for positive correlation and red for negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4bd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_text_attr(attrs):\n",
    "    rgb = lambda x: '255,0,0' if x < 0 else '0,255,0'\n",
    "    alpha = lambda x: abs(x) ** 0.5\n",
    "    token_marks = [\n",
    "        f'<mark style=\"background-color:rgba({rgb(attr)},{alpha(attr)})\">{token}</mark>'\n",
    "        for token, attr in zip(tokenizer(test_line), attrs.tolist())\n",
    "    ]\n",
    "    \n",
    "    display(HTML('<p>' + ' '.join(token_marks) + '</p>'))\n",
    "    \n",
    "show_text_attr(attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca315e07",
   "metadata": {},
   "source": [
    "The above visulization should render something like the image below where the model links the \"Sports\" subject to many reasonable words, like \"match\" and \"medals\".\n",
    "\n",
    "![Lime Text](img/lime_text_viz.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e07a59d-a6f0-43d7-a949-9d5cd6e2a16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n",
      "The `xla_device` argument has been deprecated in v4.4.0 of Transformers. It is ignored and you can safely remove it from your `config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> hall</s>\n",
      "<pad> diversified</s>\n",
      "finished\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/t5-base-finetuned-squadv2\")\n",
    "t5 = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/t5-base-finetuned-squadv2\")\n",
    "\n",
    "\n",
    "def get_answer(question, context):\n",
    "    input_text = \"question: %s  context: %s\" % (question, context)\n",
    "    features = tokenizer([input_text], return_tensors='pt')\n",
    "\n",
    "    output = t5.generate(input_ids=features['input_ids'],\n",
    "                            attention_mask=features['attention_mask'])\n",
    "\n",
    "    return tokenizer.decode(output[0])\n",
    "\n",
    "\n",
    "context = \"In Norse mythology, Valhalla is a majestic, enormous hall located in Asgard, ruled over by the god Odin.\"\n",
    "question = \"What is Valhalla ?\"\n",
    "\n",
    "answer = get_answer(question, context)\n",
    "print(answer)\n",
    "\n",
    "\n",
    "context = \"The economy of Victoria is highly diversified: service sectors including financial and property services, health, education, wholesale, retail, hospitality and manufacturing constitute the majority of employment. Victoria's total gross state product (GSP) is ranked second in Australia, although Victoria is ranked fourth\"\n",
    "question = \"What kind of economy does Victoria have?\"\n",
    "\n",
    "answer = get_answer(question, context)\n",
    "print(answer)\n",
    "print(\"finished\")\n",
    "# output: 'HF-Transformers and Google'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b94634-8920-4c00-9539-129a8fb553ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "        module name  input shape output shape      params memory(MB)             MAdd          Flops   MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0        features.0    3 224 224   64  55  55     23296.0       0.74    140,553,600.0   70,470,400.0     695296.0     774400.0      52.58%    1469696.0\n",
      "1        features.1   64  55  55   64  55  55         0.0       0.74        193,600.0      193,600.0     774400.0     774400.0       0.15%    1548800.0\n",
      "2        features.2   64  55  55   64  27  27         0.0       0.18        373,248.0      193,600.0     774400.0     186624.0       1.19%     961024.0\n",
      "3        features.3   64  27  27  192  27  27    307392.0       0.53    447,897,600.0  224,088,768.0    1416192.0     559872.0       5.05%    1976064.0\n",
      "4        features.4  192  27  27  192  27  27         0.0       0.53        139,968.0      139,968.0     559872.0     559872.0       0.09%    1119744.0\n",
      "5        features.5  192  27  27  192  13  13         0.0       0.12        259,584.0      139,968.0     559872.0     129792.0       0.86%     689664.0\n",
      "6        features.6  192  13  13  384  13  13    663936.0       0.25    224,280,576.0  112,205,184.0    2785536.0     259584.0       4.42%    3045120.0\n",
      "7        features.7  384  13  13  384  13  13         0.0       0.25         64,896.0       64,896.0     259584.0     259584.0       0.11%     519168.0\n",
      "8        features.8  384  13  13  256  13  13    884992.0       0.17    299,040,768.0  149,563,648.0    3799552.0     173056.0       4.61%    3972608.0\n",
      "9        features.9  256  13  13  256  13  13         0.0       0.17         43,264.0       43,264.0     173056.0     173056.0       0.08%     346112.0\n",
      "10      features.10  256  13  13  256  13  13    590080.0       0.17    199,360,512.0   99,723,520.0    2533376.0     173056.0       2.90%    2706432.0\n",
      "11      features.11  256  13  13  256  13  13         0.0       0.17         43,264.0       43,264.0     173056.0     173056.0       0.08%     346112.0\n",
      "12      features.12  256  13  13  256   6   6         0.0       0.04         73,728.0       43,264.0     173056.0      36864.0       0.29%     209920.0\n",
      "13          avgpool  256   6   6  256   6   6         0.0       0.04              0.0            0.0          0.0          0.0       0.23%          0.0\n",
      "14     classifier.0         9216         9216         0.0       0.04              0.0            0.0          0.0          0.0       0.05%          0.0\n",
      "15     classifier.1         9216         4096  37752832.0       0.02     75,493,376.0   37,748,736.0  151048192.0      16384.0      17.60%  151064576.0\n",
      "16     classifier.2         4096         4096         0.0       0.02          4,096.0        4,096.0      16384.0      16384.0       0.07%      32768.0\n",
      "17     classifier.3         4096         4096         0.0       0.02              0.0            0.0          0.0          0.0       0.04%          0.0\n",
      "18     classifier.4         4096         4096  16781312.0       0.02     33,550,336.0   16,777,216.0   67141632.0      16384.0       7.64%   67158016.0\n",
      "19     classifier.5         4096         4096         0.0       0.02          4,096.0        4,096.0      16384.0      16384.0       0.05%      32768.0\n",
      "20     classifier.6         4096         1000   4097000.0       0.00      8,191,000.0    4,096,000.0   16404384.0       4000.0       1.91%   16408384.0\n",
      "total                                          61100840.0       4.19  1,429,567,512.0  715,543,488.0   16404384.0       4000.0     100.00%  253606976.0\n",
      "=======================================================================================================================================================\n",
      "Total params: 61,100,840\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 4.19MB\n",
      "Total MAdd: 1.43GMAdd\n",
      "Total Flops: 715.54MFlops\n",
      "Total MemR+W: 241.86MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchstat import stat\n",
    "import torchvision.models as models\n",
    "t5 = models.alexnet()\n",
    "stat(t5, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "913693cd-70be-4bb7-a358-0c85d7002f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 61.10M\n"
     ]
    }
   ],
   "source": [
    "total = sum([param.nelement() for param in t5.parameters()])\n",
    "\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31be9e05-36cd-4ca6-be5a-968bb069e6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "bert = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fe72cc7-4664-47ef-a6ec-770921ca9816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 334.09M\n"
     ]
    }
   ],
   "source": [
    "total = sum([param.nelement() for param in bert.parameters()])\n",
    "\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f1d1d6c-fe13-4b57-914d-84590685a27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: AdaptiveAvgPool2d is not supported!\n",
      "[Flops]: AdaptiveAvgPool2d is not supported!\n",
      "[Memory]: AdaptiveAvgPool2d is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "        module name  input shape output shape      params memory(MB)             MAdd          Flops   MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0        features.0    3 224 224   64  55  55     23296.0       0.74    140,553,600.0   70,470,400.0     695296.0     774400.0       6.15%    1469696.0\n",
      "1        features.1   64  55  55   64  55  55         0.0       0.74        193,600.0      193,600.0     774400.0     774400.0       0.67%    1548800.0\n",
      "2        features.2   64  55  55   64  27  27         0.0       0.18        373,248.0      193,600.0     774400.0     186624.0       2.64%     961024.0\n",
      "3        features.3   64  27  27  192  27  27    307392.0       0.53    447,897,600.0  224,088,768.0    1416192.0     559872.0       7.71%    1976064.0\n",
      "4        features.4  192  27  27  192  27  27         0.0       0.53        139,968.0      139,968.0     559872.0     559872.0       0.48%    1119744.0\n",
      "5        features.5  192  27  27  192  13  13         0.0       0.12        259,584.0      139,968.0     559872.0     129792.0       2.02%     689664.0\n",
      "6        features.6  192  13  13  384  13  13    663936.0       0.25    224,280,576.0  112,205,184.0    2785536.0     259584.0       8.40%    3045120.0\n",
      "7        features.7  384  13  13  384  13  13         0.0       0.25         64,896.0       64,896.0     259584.0     259584.0       0.81%     519168.0\n",
      "8        features.8  384  13  13  256  13  13    884992.0       0.17    299,040,768.0  149,563,648.0    3799552.0     173056.0       7.88%    3972608.0\n",
      "9        features.9  256  13  13  256  13  13         0.0       0.17         43,264.0       43,264.0     173056.0     173056.0       0.51%     346112.0\n",
      "10      features.10  256  13  13  256  13  13    590080.0       0.17    199,360,512.0   99,723,520.0    2533376.0     173056.0       4.86%    2706432.0\n",
      "11      features.11  256  13  13  256  13  13         0.0       0.17         43,264.0       43,264.0     173056.0     173056.0       0.47%     346112.0\n",
      "12      features.12  256  13  13  256   6   6         0.0       0.04         73,728.0       43,264.0     173056.0      36864.0       1.09%     209920.0\n",
      "13          avgpool  256   6   6  256   6   6         0.0       0.04              0.0            0.0          0.0          0.0       1.05%          0.0\n",
      "14     classifier.0         9216         9216         0.0       0.04              0.0            0.0          0.0          0.0       0.87%          0.0\n",
      "15     classifier.1         9216         4096  37752832.0       0.02     75,493,376.0   37,748,736.0  151048192.0      16384.0      33.24%  151064576.0\n",
      "16     classifier.2         4096         4096         0.0       0.02          4,096.0        4,096.0      16384.0      16384.0       0.44%      32768.0\n",
      "17     classifier.3         4096         4096         0.0       0.02              0.0            0.0          0.0          0.0       0.63%          0.0\n",
      "18     classifier.4         4096         4096  16781312.0       0.02     33,550,336.0   16,777,216.0   67141632.0      16384.0      15.50%   67158016.0\n",
      "19     classifier.5         4096         4096         0.0       0.02          4,096.0        4,096.0      16384.0      16384.0       0.43%      32768.0\n",
      "20     classifier.6         4096         1000   4097000.0       0.00      8,191,000.0    4,096,000.0   16404384.0       4000.0       4.15%   16408384.0\n",
      "total                                          61100840.0       4.19  1,429,567,512.0  715,543,488.0   16404384.0       4000.0     100.00%  253606976.0\n",
      "=======================================================================================================================================================\n",
      "Total params: 61,100,840\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 4.19MB\n",
      "Total MAdd: 1.43GMAdd\n",
      "Total Flops: 715.54MFlops\n",
      "Total MemR+W: 241.86MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torchstat import stat\n",
    "import torchvision.models as models\n",
    "bert = models.alexnet()\n",
    "stat(bert, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f20fdaae-5928-4311-b5ee-f60d2f4e7eff",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save() missing 1 required positional argument: 'f'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28164/1686811201.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: save() missing 1 required positional argument: 'f'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.save(t5, )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

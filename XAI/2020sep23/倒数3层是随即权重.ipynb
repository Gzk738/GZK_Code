{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07bb943-8d3a-46de-bd61-c8c939441cf2",
   "metadata": {},
   "source": [
    "# 这是bert每层的名字\n",
    "bert.embeddings.position_ids\n",
    "bert.embeddings.word_embeddings.weight\n",
    "bert.embeddings.position_embeddings.weight\n",
    "bert.embeddings.token_type_embeddings.weight\n",
    "bert.embeddings.LayerNorm.weight\n",
    "bert.embeddings.LayerNorm.bias\n",
    "bert.encoder.layer.0.attention.self.query.weight\n",
    "bert.encoder.layer.0.attention.self.query.bias\n",
    "bert.encoder.layer.0.attention.self.key.weight\n",
    "bert.encoder.layer.0.attention.self.key.bias\n",
    "bert.encoder.layer.0.attention.self.value.weight\n",
    "bert.encoder.layer.0.attention.self.value.bias\n",
    "bert.encoder.layer.0.attention.output.dense.weight\n",
    "bert.encoder.layer.0.attention.output.dense.bias\n",
    "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.0.intermediate.dense.weight\n",
    "bert.encoder.layer.0.intermediate.dense.bias\n",
    "bert.encoder.layer.0.output.dense.weight\n",
    "bert.encoder.layer.0.output.dense.bias\n",
    "bert.encoder.layer.0.output.LayerNorm.weight\n",
    "bert.encoder.layer.0.output.LayerNorm.bias\n",
    "bert.encoder.layer.1.attention.self.query.weight\n",
    "bert.encoder.layer.1.attention.self.query.bias\n",
    "bert.encoder.layer.1.attention.self.key.weight\n",
    "bert.encoder.layer.1.attention.self.key.bias\n",
    "bert.encoder.layer.1.attention.self.value.weight\n",
    "bert.encoder.layer.1.attention.self.value.bias\n",
    "bert.encoder.layer.1.attention.output.dense.weight\n",
    "bert.encoder.layer.1.attention.output.dense.bias\n",
    "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.1.intermediate.dense.weight\n",
    "bert.encoder.layer.1.intermediate.dense.bias\n",
    "bert.encoder.layer.1.output.dense.weight\n",
    "bert.encoder.layer.1.output.dense.bias\n",
    "bert.encoder.layer.1.output.LayerNorm.weight\n",
    "bert.encoder.layer.1.output.LayerNorm.bias\n",
    "bert.encoder.layer.2.attention.self.query.weight\n",
    "bert.encoder.layer.2.attention.self.query.bias\n",
    "bert.encoder.layer.2.attention.self.key.weight\n",
    "bert.encoder.layer.2.attention.self.key.bias\n",
    "bert.encoder.layer.2.attention.self.value.weight\n",
    "bert.encoder.layer.2.attention.self.value.bias\n",
    "bert.encoder.layer.2.attention.output.dense.weight\n",
    "bert.encoder.layer.2.attention.output.dense.bias\n",
    "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.2.intermediate.dense.weight\n",
    "bert.encoder.layer.2.intermediate.dense.bias\n",
    "bert.encoder.layer.2.output.dense.weight\n",
    "bert.encoder.layer.2.output.dense.bias\n",
    "bert.encoder.layer.2.output.LayerNorm.weight\n",
    "bert.encoder.layer.2.output.LayerNorm.bias\n",
    "bert.encoder.layer.3.attention.self.query.weight\n",
    "bert.encoder.layer.3.attention.self.query.bias\n",
    "bert.encoder.layer.3.attention.self.key.weight\n",
    "bert.encoder.layer.3.attention.self.key.bias\n",
    "bert.encoder.layer.3.attention.self.value.weight\n",
    "bert.encoder.layer.3.attention.self.value.bias\n",
    "bert.encoder.layer.3.attention.output.dense.weight\n",
    "bert.encoder.layer.3.attention.output.dense.bias\n",
    "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.3.intermediate.dense.weight\n",
    "bert.encoder.layer.3.intermediate.dense.bias\n",
    "bert.encoder.layer.3.output.dense.weight\n",
    "bert.encoder.layer.3.output.dense.bias\n",
    "bert.encoder.layer.3.output.LayerNorm.weight\n",
    "bert.encoder.layer.3.output.LayerNorm.bias\n",
    "bert.encoder.layer.4.attention.self.query.weight\n",
    "bert.encoder.layer.4.attention.self.query.bias\n",
    "bert.encoder.layer.4.attention.self.key.weight\n",
    "bert.encoder.layer.4.attention.self.key.bias\n",
    "bert.encoder.layer.4.attention.self.value.weight\n",
    "bert.encoder.layer.4.attention.self.value.bias\n",
    "bert.encoder.layer.4.attention.output.dense.weight\n",
    "bert.encoder.layer.4.attention.output.dense.bias\n",
    "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.4.intermediate.dense.weight\n",
    "bert.encoder.layer.4.intermediate.dense.bias\n",
    "bert.encoder.layer.4.output.dense.weight\n",
    "bert.encoder.layer.4.output.dense.bias\n",
    "bert.encoder.layer.4.output.LayerNorm.weight\n",
    "bert.encoder.layer.4.output.LayerNorm.bias\n",
    "bert.encoder.layer.5.attention.self.query.weight\n",
    "bert.encoder.layer.5.attention.self.query.bias\n",
    "bert.encoder.layer.5.attention.self.key.weight\n",
    "bert.encoder.layer.5.attention.self.key.bias\n",
    "bert.encoder.layer.5.attention.self.value.weight\n",
    "bert.encoder.layer.5.attention.self.value.bias\n",
    "bert.encoder.layer.5.attention.output.dense.weight\n",
    "bert.encoder.layer.5.attention.output.dense.bias\n",
    "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.5.intermediate.dense.weight\n",
    "bert.encoder.layer.5.intermediate.dense.bias\n",
    "bert.encoder.layer.5.output.dense.weight\n",
    "bert.encoder.layer.5.output.dense.bias\n",
    "bert.encoder.layer.5.output.LayerNorm.weight\n",
    "bert.encoder.layer.5.output.LayerNorm.bias\n",
    "bert.encoder.layer.6.attention.self.query.weight\n",
    "bert.encoder.layer.6.attention.self.query.bias\n",
    "bert.encoder.layer.6.attention.self.key.weight\n",
    "bert.encoder.layer.6.attention.self.key.bias\n",
    "bert.encoder.layer.6.attention.self.value.weight\n",
    "bert.encoder.layer.6.attention.self.value.bias\n",
    "bert.encoder.layer.6.attention.output.dense.weight\n",
    "bert.encoder.layer.6.attention.output.dense.bias\n",
    "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.6.intermediate.dense.weight\n",
    "bert.encoder.layer.6.intermediate.dense.bias\n",
    "bert.encoder.layer.6.output.dense.weight\n",
    "bert.encoder.layer.6.output.dense.bias\n",
    "bert.encoder.layer.6.output.LayerNorm.weight\n",
    "bert.encoder.layer.6.output.LayerNorm.bias\n",
    "bert.encoder.layer.7.attention.self.query.weight\n",
    "bert.encoder.layer.7.attention.self.query.bias\n",
    "bert.encoder.layer.7.attention.self.key.weight\n",
    "bert.encoder.layer.7.attention.self.key.bias\n",
    "bert.encoder.layer.7.attention.self.value.weight\n",
    "bert.encoder.layer.7.attention.self.value.bias\n",
    "bert.encoder.layer.7.attention.output.dense.weight\n",
    "bert.encoder.layer.7.attention.output.dense.bias\n",
    "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.7.intermediate.dense.weight\n",
    "bert.encoder.layer.7.intermediate.dense.bias\n",
    "bert.encoder.layer.7.output.dense.weight\n",
    "bert.encoder.layer.7.output.dense.bias\n",
    "bert.encoder.layer.7.output.LayerNorm.weight\n",
    "bert.encoder.layer.7.output.LayerNorm.bias\n",
    "bert.encoder.layer.8.attention.self.query.weight\n",
    "bert.encoder.layer.8.attention.self.query.bias\n",
    "bert.encoder.layer.8.attention.self.key.weight\n",
    "bert.encoder.layer.8.attention.self.key.bias\n",
    "bert.encoder.layer.8.attention.self.value.weight\n",
    "bert.encoder.layer.8.attention.self.value.bias\n",
    "bert.encoder.layer.8.attention.output.dense.weight\n",
    "bert.encoder.layer.8.attention.output.dense.bias\n",
    "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.8.intermediate.dense.weight\n",
    "bert.encoder.layer.8.intermediate.dense.bias\n",
    "bert.encoder.layer.8.output.dense.weight\n",
    "bert.encoder.layer.8.output.dense.bias\n",
    "bert.encoder.layer.8.output.LayerNorm.weight\n",
    "bert.encoder.layer.8.output.LayerNorm.bias\n",
    "bert.encoder.layer.9.attention.self.query.weight\n",
    "bert.encoder.layer.9.attention.self.query.bias\n",
    "bert.encoder.layer.9.attention.self.key.weight\n",
    "bert.encoder.layer.9.attention.self.key.bias\n",
    "bert.encoder.layer.9.attention.self.value.weight\n",
    "bert.encoder.layer.9.attention.self.value.bias\n",
    "bert.encoder.layer.9.attention.output.dense.weight\n",
    "bert.encoder.layer.9.attention.output.dense.bias\n",
    "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.9.intermediate.dense.weight\n",
    "bert.encoder.layer.9.intermediate.dense.bias\n",
    "bert.encoder.layer.9.output.dense.weight\n",
    "bert.encoder.layer.9.output.dense.bias\n",
    "bert.encoder.layer.9.output.LayerNorm.weight\n",
    "bert.encoder.layer.9.output.LayerNorm.bias\n",
    "bert.encoder.layer.10.attention.self.query.weight\n",
    "bert.encoder.layer.10.attention.self.query.bias\n",
    "bert.encoder.layer.10.attention.self.key.weight\n",
    "bert.encoder.layer.10.attention.self.key.bias\n",
    "bert.encoder.layer.10.attention.self.value.weight\n",
    "bert.encoder.layer.10.attention.self.value.bias\n",
    "bert.encoder.layer.10.attention.output.dense.weight\n",
    "bert.encoder.layer.10.attention.output.dense.bias\n",
    "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.10.intermediate.dense.weight\n",
    "bert.encoder.layer.10.intermediate.dense.bias\n",
    "bert.encoder.layer.10.output.dense.weight\n",
    "bert.encoder.layer.10.output.dense.bias\n",
    "bert.encoder.layer.10.output.LayerNorm.weight\n",
    "bert.encoder.layer.10.output.LayerNorm.bias\n",
    "bert.encoder.layer.11.attention.self.query.weight\n",
    "bert.encoder.layer.11.attention.self.query.bias\n",
    "bert.encoder.layer.11.attention.self.key.weight\n",
    "bert.encoder.layer.11.attention.self.key.bias\n",
    "bert.encoder.layer.11.attention.self.value.weight\n",
    "bert.encoder.layer.11.attention.self.value.bias\n",
    "bert.encoder.layer.11.attention.output.dense.weight\n",
    "bert.encoder.layer.11.attention.output.dense.bias\n",
    "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.11.intermediate.dense.weight\n",
    "bert.encoder.layer.11.intermediate.dense.bias\n",
    "bert.encoder.layer.11.output.dense.weight\n",
    "bert.encoder.layer.11.output.dense.bias\n",
    "bert.encoder.layer.11.output.LayerNorm.weight\n",
    "bert.encoder.layer.11.output.LayerNorm.bias\n",
    "bert.encoder.layer.12.attention.self.query.weight\n",
    "bert.encoder.layer.12.attention.self.query.bias\n",
    "bert.encoder.layer.12.attention.self.key.weight\n",
    "bert.encoder.layer.12.attention.self.key.bias\n",
    "bert.encoder.layer.12.attention.self.value.weight\n",
    "bert.encoder.layer.12.attention.self.value.bias\n",
    "bert.encoder.layer.12.attention.output.dense.weight\n",
    "bert.encoder.layer.12.attention.output.dense.bias\n",
    "bert.encoder.layer.12.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.12.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.12.intermediate.dense.weight\n",
    "bert.encoder.layer.12.intermediate.dense.bias\n",
    "bert.encoder.layer.12.output.dense.weight\n",
    "bert.encoder.layer.12.output.dense.bias\n",
    "bert.encoder.layer.12.output.LayerNorm.weight\n",
    "bert.encoder.layer.12.output.LayerNorm.bias\n",
    "bert.encoder.layer.13.attention.self.query.weight\n",
    "bert.encoder.layer.13.attention.self.query.bias\n",
    "bert.encoder.layer.13.attention.self.key.weight\n",
    "bert.encoder.layer.13.attention.self.key.bias\n",
    "bert.encoder.layer.13.attention.self.value.weight\n",
    "bert.encoder.layer.13.attention.self.value.bias\n",
    "bert.encoder.layer.13.attention.output.dense.weight\n",
    "bert.encoder.layer.13.attention.output.dense.bias\n",
    "bert.encoder.layer.13.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.13.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.13.intermediate.dense.weight\n",
    "bert.encoder.layer.13.intermediate.dense.bias\n",
    "bert.encoder.layer.13.output.dense.weight\n",
    "bert.encoder.layer.13.output.dense.bias\n",
    "bert.encoder.layer.13.output.LayerNorm.weight\n",
    "bert.encoder.layer.13.output.LayerNorm.bias\n",
    "bert.encoder.layer.14.attention.self.query.weight\n",
    "bert.encoder.layer.14.attention.self.query.bias\n",
    "bert.encoder.layer.14.attention.self.key.weight\n",
    "bert.encoder.layer.14.attention.self.key.bias\n",
    "bert.encoder.layer.14.attention.self.value.weight\n",
    "bert.encoder.layer.14.attention.self.value.bias\n",
    "bert.encoder.layer.14.attention.output.dense.weight\n",
    "bert.encoder.layer.14.attention.output.dense.bias\n",
    "bert.encoder.layer.14.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.14.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.14.intermediate.dense.weight\n",
    "bert.encoder.layer.14.intermediate.dense.bias\n",
    "bert.encoder.layer.14.output.dense.weight\n",
    "bert.encoder.layer.14.output.dense.bias\n",
    "bert.encoder.layer.14.output.LayerNorm.weight\n",
    "bert.encoder.layer.14.output.LayerNorm.bias\n",
    "bert.encoder.layer.15.attention.self.query.weight\n",
    "bert.encoder.layer.15.attention.self.query.bias\n",
    "bert.encoder.layer.15.attention.self.key.weight\n",
    "bert.encoder.layer.15.attention.self.key.bias\n",
    "bert.encoder.layer.15.attention.self.value.weight\n",
    "bert.encoder.layer.15.attention.self.value.bias\n",
    "bert.encoder.layer.15.attention.output.dense.weight\n",
    "bert.encoder.layer.15.attention.output.dense.bias\n",
    "bert.encoder.layer.15.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.15.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.15.intermediate.dense.weight\n",
    "bert.encoder.layer.15.intermediate.dense.bias\n",
    "bert.encoder.layer.15.output.dense.weight\n",
    "bert.encoder.layer.15.output.dense.bias\n",
    "bert.encoder.layer.15.output.LayerNorm.weight\n",
    "bert.encoder.layer.15.output.LayerNorm.bias\n",
    "bert.encoder.layer.16.attention.self.query.weight\n",
    "bert.encoder.layer.16.attention.self.query.bias\n",
    "bert.encoder.layer.16.attention.self.key.weight\n",
    "bert.encoder.layer.16.attention.self.key.bias\n",
    "bert.encoder.layer.16.attention.self.value.weight\n",
    "bert.encoder.layer.16.attention.self.value.bias\n",
    "bert.encoder.layer.16.attention.output.dense.weight\n",
    "bert.encoder.layer.16.attention.output.dense.bias\n",
    "bert.encoder.layer.16.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.16.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.16.intermediate.dense.weight\n",
    "bert.encoder.layer.16.intermediate.dense.bias\n",
    "bert.encoder.layer.16.output.dense.weight\n",
    "bert.encoder.layer.16.output.dense.bias\n",
    "bert.encoder.layer.16.output.LayerNorm.weight\n",
    "bert.encoder.layer.16.output.LayerNorm.bias\n",
    "bert.encoder.layer.17.attention.self.query.weight\n",
    "bert.encoder.layer.17.attention.self.query.bias\n",
    "bert.encoder.layer.17.attention.self.key.weight\n",
    "bert.encoder.layer.17.attention.self.key.bias\n",
    "bert.encoder.layer.17.attention.self.value.weight\n",
    "bert.encoder.layer.17.attention.self.value.bias\n",
    "bert.encoder.layer.17.attention.output.dense.weight\n",
    "bert.encoder.layer.17.attention.output.dense.bias\n",
    "bert.encoder.layer.17.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.17.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.17.intermediate.dense.weight\n",
    "bert.encoder.layer.17.intermediate.dense.bias\n",
    "bert.encoder.layer.17.output.dense.weight\n",
    "bert.encoder.layer.17.output.dense.bias\n",
    "bert.encoder.layer.17.output.LayerNorm.weight\n",
    "bert.encoder.layer.17.output.LayerNorm.bias\n",
    "bert.encoder.layer.18.attention.self.query.weight\n",
    "bert.encoder.layer.18.attention.self.query.bias\n",
    "bert.encoder.layer.18.attention.self.key.weight\n",
    "bert.encoder.layer.18.attention.self.key.bias\n",
    "bert.encoder.layer.18.attention.self.value.weight\n",
    "bert.encoder.layer.18.attention.self.value.bias\n",
    "bert.encoder.layer.18.attention.output.dense.weight\n",
    "bert.encoder.layer.18.attention.output.dense.bias\n",
    "bert.encoder.layer.18.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.18.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.18.intermediate.dense.weight\n",
    "bert.encoder.layer.18.intermediate.dense.bias\n",
    "bert.encoder.layer.18.output.dense.weight\n",
    "bert.encoder.layer.18.output.dense.bias\n",
    "bert.encoder.layer.18.output.LayerNorm.weight\n",
    "bert.encoder.layer.18.output.LayerNorm.bias\n",
    "bert.encoder.layer.19.attention.self.query.weight\n",
    "bert.encoder.layer.19.attention.self.query.bias\n",
    "bert.encoder.layer.19.attention.self.key.weight\n",
    "bert.encoder.layer.19.attention.self.key.bias\n",
    "bert.encoder.layer.19.attention.self.value.weight\n",
    "bert.encoder.layer.19.attention.self.value.bias\n",
    "bert.encoder.layer.19.attention.output.dense.weight\n",
    "bert.encoder.layer.19.attention.output.dense.bias\n",
    "bert.encoder.layer.19.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.19.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.19.intermediate.dense.weight\n",
    "bert.encoder.layer.19.intermediate.dense.bias\n",
    "bert.encoder.layer.19.output.dense.weight\n",
    "bert.encoder.layer.19.output.dense.bias\n",
    "bert.encoder.layer.19.output.LayerNorm.weight\n",
    "bert.encoder.layer.19.output.LayerNorm.bias\n",
    "bert.encoder.layer.20.attention.self.query.weight\n",
    "bert.encoder.layer.20.attention.self.query.bias\n",
    "bert.encoder.layer.20.attention.self.key.weight\n",
    "bert.encoder.layer.20.attention.self.key.bias\n",
    "bert.encoder.layer.20.attention.self.value.weight\n",
    "bert.encoder.layer.20.attention.self.value.bias\n",
    "bert.encoder.layer.20.attention.output.dense.weight\n",
    "bert.encoder.layer.20.attention.output.dense.bias\n",
    "bert.encoder.layer.20.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.20.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.20.intermediate.dense.weight\n",
    "bert.encoder.layer.20.intermediate.dense.bias\n",
    "bert.encoder.layer.20.output.dense.weight\n",
    "bert.encoder.layer.20.output.dense.bias\n",
    "bert.encoder.layer.20.output.LayerNorm.weight\n",
    "bert.encoder.layer.20.output.LayerNorm.bias\n",
    "bert.encoder.layer.21.attention.self.query.weight\n",
    "bert.encoder.layer.21.attention.self.query.bias\n",
    "bert.encoder.layer.21.attention.self.key.weight\n",
    "bert.encoder.layer.21.attention.self.key.bias\n",
    "bert.encoder.layer.21.attention.self.value.weight\n",
    "bert.encoder.layer.21.attention.self.value.bias\n",
    "bert.encoder.layer.21.attention.output.dense.weight\n",
    "bert.encoder.layer.21.attention.output.dense.bias\n",
    "bert.encoder.layer.21.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.21.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.21.intermediate.dense.weight\n",
    "bert.encoder.layer.21.intermediate.dense.bias\n",
    "bert.encoder.layer.21.output.dense.weight\n",
    "bert.encoder.layer.21.output.dense.bias\n",
    "bert.encoder.layer.21.output.LayerNorm.weight\n",
    "bert.encoder.layer.21.output.LayerNorm.bias\n",
    "bert.encoder.layer.22.attention.self.query.weight\n",
    "bert.encoder.layer.22.attention.self.query.bias\n",
    "bert.encoder.layer.22.attention.self.key.weight\n",
    "bert.encoder.layer.22.attention.self.key.bias\n",
    "bert.encoder.layer.22.attention.self.value.weight\n",
    "bert.encoder.layer.22.attention.self.value.bias\n",
    "bert.encoder.layer.22.attention.output.dense.weight\n",
    "bert.encoder.layer.22.attention.output.dense.bias\n",
    "bert.encoder.layer.22.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.22.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.22.intermediate.dense.weight\n",
    "bert.encoder.layer.22.intermediate.dense.bias\n",
    "bert.encoder.layer.22.output.dense.weight\n",
    "bert.encoder.layer.22.output.dense.bias\n",
    "bert.encoder.layer.22.output.LayerNorm.weight\n",
    "bert.encoder.layer.22.output.LayerNorm.bias\n",
    "bert.encoder.layer.23.attention.self.query.weight\n",
    "bert.encoder.layer.23.attention.self.query.bias\n",
    "bert.encoder.layer.23.attention.self.key.weight\n",
    "bert.encoder.layer.23.attention.self.key.bias\n",
    "bert.encoder.layer.23.attention.self.value.weight\n",
    "bert.encoder.layer.23.attention.self.value.bias\n",
    "bert.encoder.layer.23.attention.output.dense.weight\n",
    "bert.encoder.layer.23.attention.output.dense.bias\n",
    "bert.encoder.layer.23.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.23.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.23.intermediate.dense.weight\n",
    "bert.encoder.layer.23.intermediate.dense.bias\n",
    "bert.encoder.layer.23.output.dense.weight\n",
    "bert.encoder.layer.23.output.dense.bias\n",
    "bert.encoder.layer.23.output.LayerNorm.weight\n",
    "bert.encoder.layer.23.output.LayerNorm.bias\n",
    "qa_outputs.weight\n",
    "qa_outputs.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a767a0c3-1350-480e-ad51-db361ffaa6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 133\n",
      "tensor([0.7400, 0.7400, 0.7400,  ..., 0.7400, 0.7400, 0.7400], device='cuda:0')\n",
      "tensor(0.6894, device='cuda:0')\n",
      "<class 'str'> <class 'torch.nn.parameter.Parameter'>\n",
      "qa_outputs.weight Parameter containing:\n",
      "tensor([[ 0.0234, -0.0060,  0.0238,  ..., -0.0315, -0.0137,  0.0296],\n",
      "        [ 0.0481,  0.0265,  0.0083,  ...,  0.0021, -0.0065,  0.0082]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "tensor([ 0.0234, -0.0060,  0.0238,  ..., -0.0315, -0.0137,  0.0296],\n",
      "       device='cuda:0')\n",
      "tensor([0.6032, 0.0527, 0.7246,  ..., 0.4294, 0.2053, 0.5664], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# replace <PATd:/spofrte/modeH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "model_path = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "\n",
    "# load model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "for name,param in model.named_parameters():\n",
    "    if name == 'bert.encoder.layer.23.output.dense.weight':\n",
    "        temp = torch.rand(1)\n",
    "\n",
    "        param.data[0] = temp\n",
    "        print(param.data[0])\n",
    "\n",
    "\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.start_logits, output.end_logits\n",
    "\n",
    "\n",
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values\n",
    "\n",
    "fig = plt.figure()  \n",
    "fig.set_size_inches(8, 6)\n",
    "\n",
    "ref_token_id = tokenizer.pad_token_id  # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id  # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id  # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "\n",
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "                    [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)  # * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.bert.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.bert.embeddings(ref_input_ids, token_type_ids=token_type_ids,\n",
    "                                                 position_ids=position_ids)\n",
    "\n",
    "    return input_embeddings, ref_input_embeddings\n",
    "\n",
    "\n",
    "def predict_qt(question, text):\n",
    "    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id,\n",
    "                                                                cls_token_id)\n",
    "    token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "    position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "    attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "    ground_truth = '13'\n",
    "\n",
    "    start_scores, end_scores = predict(input_ids, \\\n",
    "                                       token_type_ids=token_type_ids, \\\n",
    "                                       position_ids=position_ids, \\\n",
    "                                       attention_mask=attention_mask)\n",
    "\n",
    "    print('Question: ', question)\n",
    "    print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1]))\n",
    "    return input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens,\n",
    "\n",
    "\n",
    "def explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores,\n",
    "            ground_truth, all_tokens, ):\n",
    "    lig = LayerIntegratedGradients(squad_pos_forward_func, model.bert.embeddings)\n",
    "\n",
    "    attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "                                                    baselines=ref_input_ids,\n",
    "                                                    additional_forward_args=(\n",
    "                                                        token_type_ids, position_ids, attention_mask, 0),\n",
    "                                                    internal_batch_size=4,\n",
    "                                                    return_convergence_delta=True)\n",
    "    attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "                                                additional_forward_args=(\n",
    "                                                    token_type_ids, position_ids, attention_mask, 1),\n",
    "                                                internal_batch_size=4,\n",
    "                                                return_convergence_delta=True)\n",
    "\n",
    "    attributions_start_sum = summarize_attributions(attributions_start)\n",
    "    attributions_end_sum = summarize_attributions(attributions_end)\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    start_position_vis = viz.VisualizationDataRecord(\n",
    "        attributions_start_sum,\n",
    "        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "        torch.argmax(start_scores),\n",
    "        torch.argmax(start_scores),\n",
    "        str(ground_truth),\n",
    "        attributions_start_sum.sum(),\n",
    "        all_tokens,\n",
    "        delta_start)\n",
    "\n",
    "    end_position_vis = viz.VisualizationDataRecord(\n",
    "        attributions_end_sum,\n",
    "        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "        torch.argmax(end_scores),\n",
    "        torch.argmax(end_scores),\n",
    "        str(ground_truth),\n",
    "        attributions_end_sum.sum(),\n",
    "        all_tokens,\n",
    "        delta_end)\n",
    "    #print(all_tokens)\n",
    "    print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "    viz.visualize_text([start_position_vis])\n",
    "\n",
    "    print('\\033[1m', 'Visualizations For End Position', '\\033[0m')\n",
    "\n",
    "    print(\"attributions_start_sum:   \", len(attributions_start_sum))\n",
    "    #print(\"all tokens:    \", len(all_tokens))\n",
    "\n",
    "    return all_tokens, attributions_start_sum\n",
    "\n",
    "\n",
    "def get_posneg(all_tokens, attributions_start_sum):\n",
    "    positive = []\n",
    "    negative = []\n",
    "    neutral = []\n",
    "    for i, j in enumerate(attributions_start_sum):\n",
    "        if j > 0:\n",
    "            positive.append(i)\n",
    "            # print('positive:',j)\n",
    "        ##print(all_tokens[i])\n",
    "        elif j < 0:\n",
    "            negative.append(i)\n",
    "            # print('negative:',j)\n",
    "            # print(all_tokens[i])\n",
    "        elif j == 0:\n",
    "            neutral.append(i)\n",
    "\n",
    "    s_pos = ''\n",
    "    s_neg = ''\n",
    "\n",
    "    # print(len(attributions_start_sum))\n",
    "    # print(len(positive))\n",
    "    # print(len(negative))\n",
    "\n",
    "    for i in positive:\n",
    "        s_pos += all_tokens[i] + ' '\n",
    "    #print(\"positive :\", s_pos)\n",
    "    for i in negative:\n",
    "        s_neg += all_tokens[i] + ' '\n",
    "    #print(\"negative :\", s_neg)\n",
    "    return positive, negative, neutral\n",
    "\n",
    "\n",
    "def separate_sentence(all_tokens):\n",
    "    sentence = {}\n",
    "    temp = []\n",
    "    num = 0\n",
    "    for i in range(len(all_tokens)):\n",
    "        if all_tokens[i] == \",\" or all_tokens[i] == \".\":\n",
    "            temp.append(all_tokens[i])\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        elif all_tokens[i] == \"[CLS]\":\n",
    "            temp.append(all_tokens[i])\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        elif all_tokens[i] == \"[SEP]\":\n",
    "            sentence[num] = temp\n",
    "            num = num + 1\n",
    "            temp = [all_tokens[i]]\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        else:\n",
    "            temp.append(all_tokens[i])\n",
    "    return sentence\n",
    "def get_sence_score(sentence, attributions_start_sum):\n",
    "    weight = 0\n",
    "    sum_weight = 0\n",
    "    sentence_value=[]\n",
    "    delete_sentence = []\n",
    "    for k,v in sentence.items():\n",
    "        for i in v:\n",
    "            sentence_value.append(i)\n",
    "    scores={}\n",
    "\n",
    "\n",
    "    for i in range(len(attributions_start_sum)):\n",
    "        try:\n",
    "            scores[sentence_value[i]]=attributions_start_sum[i].item()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    for i, j in sentence.items():\n",
    "        sum_weight = 0\n",
    "        for word in j:\n",
    "            sum_weight +=  scores[word]\n",
    "        delete_sentence.append(sum_weight)\n",
    "        #print(sum_weight)\n",
    "    return delete_sentence\n",
    "\n",
    "def get_delete(sentence):\n",
    "    weight = 0\n",
    "    sum_weight = 0\n",
    "    sentence_value = []\n",
    "    delete_sentence = {}\n",
    "    for k, v in sentence.items():\n",
    "        # print(k,':',v)\n",
    "        for i in v:\n",
    "            sentence_value.append(i)\n",
    "    #print(sentence_value)\n",
    "    scores = {}\n",
    "    # print(attributions_start_sum[0].item())\n",
    "\n",
    "    for i in range(len(attributions_start_sum)):\n",
    "        try:\n",
    "            scores[sentence_value[i]] = attributions_start_sum[i].item()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i, j in sentence.items():\n",
    "        sum_weight = 0\n",
    "        for word in j:\n",
    "            weight = 0\n",
    "\n",
    "            sum_weight += scores[word]\n",
    "            delete_sentence[i] = sum_weight\n",
    "    return delete_sentence\n",
    "\n",
    "\n",
    "def delete_sentence(sentence, li_delete_sentence):\n",
    "    for i, j in sentence.items():\n",
    "        if i in li_delete_sentence:\n",
    "            sentence[i] = []\n",
    "        else:\n",
    "            pass\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def rebuild_sentence(ori_sentence):\n",
    "    rebuild_str = \"\"\n",
    "    for i, j in ori_sentence.items():\n",
    "        for word in j:\n",
    "            rebuild_str += word\n",
    "            rebuild_str += \" \"\n",
    "    return rebuild_str\n",
    "\n",
    "\n",
    "def pred_explain(question, text):\n",
    "    input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens, = predict_qt(\n",
    "        text, question)\n",
    "\n",
    "    all_tokens, attributions_start_sum = explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask,\n",
    "                                                 start_scores, end_scores, ground_truth, all_tokens, )\n",
    "\n",
    "    end_score = float(torch.max(torch.softmax(end_scores[0], dim=0)))\n",
    "    start_score = float(torch.max(torch.softmax(start_scores[0], dim=0)))\n",
    "    return all_tokens, attributions_start_sum, end_score, start_score, [torch.argmax(start_scores), torch.argmax(end_scores)+1], start_scores, end_scores\n",
    "def max_min(x, y, z):\n",
    "    max = min = x\n",
    "    i = 1\n",
    "    if y > max:\n",
    "        max = y\n",
    "        i = 2\n",
    "    else:\n",
    "        min = y\n",
    "    if z > max:\n",
    "        max = z\n",
    "        i =3\n",
    "    else:\n",
    "        min = z\n",
    "    return (i)\n",
    "def cycle_prediction(cycle_num, question, text):\n",
    "    all_tokens, attributions_start_sum, start_acc, end_acc,  an_index, start_scores, end_scores = pred_explain(text, question)\n",
    "    first_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "    first_answer = re.sub(r' ##', '', first_answer)\n",
    "    print(\"my answer is \", first_answer)\n",
    "    print(start_acc, end_acc)\n",
    "    second_answer = ''\n",
    "    sentence = separate_sentence(all_tokens)\n",
    "    a = 0\n",
    "    pos_contri = 0\n",
    "    neg_contri = 0\n",
    "    average_neg = []\n",
    "    average_pos = []\n",
    "    for i, j in enumerate(attributions_start_sum):\n",
    "        if j <0 : \n",
    "            neg_contri+= j\n",
    "        elif j >0:\n",
    "            pos_contri += j\n",
    "    print(\"positive contribution:\", pos_contri)\n",
    "    average_pos.append(pos_contri)\n",
    "    print(\"negative contribution:\", neg_contri)\n",
    "    average_neg.append(neg_contri)\n",
    "\n",
    "    acc_s = []\n",
    "    acc_e = []\n",
    "    sun = []\n",
    "    ans = []\n",
    "    ans.append(first_answer)\n",
    "    #print(start_acc, end_acc)\n",
    "    acc_s.append(start_acc)\n",
    "    acc_e.append(end_acc)\n",
    "\n",
    "    for loop in range(cycle_num):\n",
    "        sentence = separate_sentence(all_tokens)\n",
    "        sentence_score  = get_sence_score(sentence, attributions_start_sum)\n",
    "        min_sensocer = 999\n",
    "        min_index = 999\n",
    "        for i in range(len(sentence_score)):\n",
    "            if sentence_score[i] < min_sensocer and sentence_score[i] != 0:\n",
    "                min_sensocer = sentence_score[i]\n",
    "                min_index = i\n",
    "        #print(\"should delete\", min_index, min_sensocer)\n",
    "        sentence[min_index] = ''\n",
    "        sentence[1] = ''\n",
    "        retext = \"\"\n",
    "        for i, j in sentence.items():\n",
    "            for words in j:\n",
    "                retext = retext + words + \" \"\n",
    "        li_sep = []\n",
    "        for m in re.finditer(r\"SEP\", retext):\n",
    "            li_sep.append(m.start())\n",
    "            li_sep.append(m.end())\n",
    "        retext = retext[li_sep[1]+1: li_sep[2] -1]\n",
    "        retext = re.sub(r' ##', '', retext)\n",
    "\n",
    "\n",
    "\n",
    "        all_tokens, attributions_start_sum, start_acc, end_acc,  an_index, start_scores, end_scores= pred_explain(retext, question)\n",
    "        reanswer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "        #print(start_acc, end_acc)\n",
    "        second_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "        second_answer = re.sub(r' ##', '', second_answer)\n",
    "        #print(\"my answer is \", second_answer)\n",
    "        ans.append(second_answer)\n",
    "        #print(start_acc, end_acc)\n",
    "        acc_s.append(start_acc)\n",
    "        acc_e.append(end_acc)\n",
    "        pos_contri = 0\n",
    "        neg_contri = 0\n",
    "        for i, j in enumerate(attributions_start_sum):\n",
    "            if j <0:\n",
    "                neg_contri+= j\n",
    "            elif j >0:\n",
    "                pos_contri += j\n",
    "        print(\"positive contribution:\", pos_contri)\n",
    "        average_pos.append(pos_contri)\n",
    "        print(\"negative contribution:\", neg_contri)\n",
    "        average_neg.append(neg_contri)\n",
    "\n",
    "\n",
    "        #print(acc_s, acc_e)\n",
    "        #print(acc_s, acc_e)\n",
    "    plt.plot(range(len(acc_s)), acc_s, label = 'start score')\n",
    "    plt.plot(range(len(acc_s)), acc_e, label = 'end score')\n",
    "    sun = []\n",
    "    for i in range(len(acc_s)):\n",
    "        sun.append((acc_s[i] + acc_e[i])/2)\n",
    "    print(sun)\n",
    "    plt.plot(range(len(acc_s)), sun, label = 'average')\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('Possibility')    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    \"\"\"\"获取最好的曲线并输出\"\"\"\n",
    "    max_start = 0\n",
    "    max_end = 0\n",
    "    max_ave = 0\n",
    "    for i in acc_s:\n",
    "        if i > max_start:\n",
    "            max_start = i\n",
    "    for j in acc_e:\n",
    "        if j > max_end:\n",
    "            max_end = i\n",
    "\n",
    "    for x in sun:\n",
    "        if x > max_ave:\n",
    "            max_ave = x\n",
    "\n",
    "    print(max_start ,max_end ,max_ave )\n",
    "\n",
    "\n",
    "    max_list = max_min(max_start, max_end, max_ave)\n",
    "    if max_list == 1:\n",
    "        plt.plot(range(len(acc_s)), acc_s, label = 'Possibility')\n",
    "        print(acc_s)\n",
    "    if max_list == 2:\n",
    "        plt.plot(range(len(acc_e)), acc_e, label = 'Possibility')\n",
    "        print(acc_e)\n",
    "    if max_list == 3:\n",
    "        plt.plot(range(len(sun)), sun, label = 'Possibility')\n",
    "        print(sun)\n",
    "\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('Possibility')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \"\"\"贡献的数值分别是多少\"\"\"\n",
    "    plt.plot(range(len(average_pos)), average_pos, label = 'pos score')\n",
    "    plt.plot(range(len(average_neg)), average_neg, label = 'neg score')\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('average contribution of pos/neg')    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    for i in range(len(ans)):\n",
    "        print(ans[i], \"pos/neg : \", -(average_pos[i]/average_neg[i]))\n",
    "    average_contribution = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04f36f0-c8fe-4de3-944f-7740476c31f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What magazine named Beyoncé as the most powerful female musician for 2015?\n",
      "Predicted Answer:  powerful female musician for 2015 ? [SEP] a self - described ' modern - day feminist ' , beyonce creates songs that are often characterized by themes of love , relationships , and mono ##gam ##y , as well as female sexuality and empowerment . on stage , her dynamic , highly choreographed performances have led to critics hail ##ing her as one of the best entertainer ##s in contemporary popular music . throughout a career spanning 19 years , she has sold over 118 million records as a solo artist , and a further 60 million with destiny ' s child , making her one of the best - selling music artists of all time . she has won 20 grammy awards and is the most nominated woman in the award ' s history . the recording industry association of america recognized her as the top certified artist in america during the 2000s decade . in 2009 , billboard named her the top radio songs artist of the decade , the top female artist of the 2000s and their artist of the millennium in 2011 . time listed her among the 100 most influential people in the world in 2013 and 2014 . forbes\n",
      "\u001b[1m Visualizations For Start Position \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>8</b></text></td><td><text style=\"padding-right:2em\"><b>8 (0.23)</b></text></td><td><text style=\"padding-right:2em\"><b>13</b></text></td><td><text style=\"padding-right:2em\"><b>4.64</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> magazine                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> named                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> beyonce                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> powerful                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> female                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> musician                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2015                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> self                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> described                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> modern                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> day                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> feminist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> beyonce                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> creates                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> songs                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> often                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> characterized                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> themes                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> love                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> relationships                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mono                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##gam                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##y                    </font></mark><mark style=\"background-color: hsl(120, 75%, 87%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> well                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> female                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sexuality                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> empowerment                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stage                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dynamic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> highly                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> choreographed                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> performances                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> led                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> critics                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hail                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ing                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> entertainer                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> contemporary                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> popular                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> music                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> throughout                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> career                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spanning                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 19                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> years                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sold                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> over                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 118                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> million                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> records                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> solo                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> further                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 60                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> million                    </font></mark><mark style=\"background-color: hsl(0, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> destiny                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> child                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> making                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> selling                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> music                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artists                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> won                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 20                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> grammy                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> awards                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nominated                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> woman                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> award                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> history                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> recording                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> industry                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> association                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> america                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> recognized                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> top                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> certified                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> america                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> during                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2000s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decade                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2009                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> billboard                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> named                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> top                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> radio                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> songs                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decade                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> top                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> female                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2000s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> their                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> millennium                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2011                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> listed                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> among                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 100                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> influential                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> people                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> world                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2013                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2014                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> forbes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> magazine                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> also                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> listed                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> powerful                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> female                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> musician                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2015                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Visualizations For End Position \u001b[0m\n",
      "attributions_start_sum:    227\n"
     ]
    }
   ],
   "source": [
    "text = \"A self-described 'modern-day feminist', Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.\"\n",
    "question = \"What magazine named Beyoncé as the most powerful female musician for 2015?\"\n",
    "\n",
    "all_tokens, attributions_start_sum, start_acc, end_acc,  an_index, start_scores, end_scores = pred_explain(text, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79055c5a-323b-4e5c-af06-c53421790eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attr_nomal = []\n",
    "for i, j in enumerate(attributions_start_sum):\n",
    "    attr_nomal.append(j)\n",
    "plt.plot(range(len(attr_nomal)), attr_nomal)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

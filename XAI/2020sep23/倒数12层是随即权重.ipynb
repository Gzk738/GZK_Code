{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07bb943-8d3a-46de-bd61-c8c939441cf2",
   "metadata": {},
   "source": [
    "# 这是bert每层的名字\n",
    "bert.embeddings.position_ids\n",
    "bert.embeddings.word_embeddings.weight\n",
    "bert.embeddings.position_embeddings.weight\n",
    "bert.embeddings.token_type_embeddings.weight\n",
    "bert.embeddings.LayerNorm.weight\n",
    "bert.embeddings.LayerNorm.bias\n",
    "bert.encoder.layer.0.attention.self.query.weight\n",
    "bert.encoder.layer.0.attention.self.query.bias\n",
    "bert.encoder.layer.0.attention.self.key.weight\n",
    "bert.encoder.layer.0.attention.self.key.bias\n",
    "bert.encoder.layer.0.attention.self.value.weight\n",
    "bert.encoder.layer.0.attention.self.value.bias\n",
    "bert.encoder.layer.0.attention.output.dense.weight\n",
    "bert.encoder.layer.0.attention.output.dense.bias\n",
    "bert.encoder.layer.0.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.0.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.0.intermediate.dense.weight\n",
    "bert.encoder.layer.0.intermediate.dense.bias\n",
    "bert.encoder.layer.0.output.dense.weight\n",
    "bert.encoder.layer.0.output.dense.bias\n",
    "bert.encoder.layer.0.output.LayerNorm.weight\n",
    "bert.encoder.layer.0.output.LayerNorm.bias\n",
    "bert.encoder.layer.1.attention.self.query.weight\n",
    "bert.encoder.layer.1.attention.self.query.bias\n",
    "bert.encoder.layer.1.attention.self.key.weight\n",
    "bert.encoder.layer.1.attention.self.key.bias\n",
    "bert.encoder.layer.1.attention.self.value.weight\n",
    "bert.encoder.layer.1.attention.self.value.bias\n",
    "bert.encoder.layer.1.attention.output.dense.weight\n",
    "bert.encoder.layer.1.attention.output.dense.bias\n",
    "bert.encoder.layer.1.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.1.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.1.intermediate.dense.weight\n",
    "bert.encoder.layer.1.intermediate.dense.bias\n",
    "bert.encoder.layer.1.output.dense.weight\n",
    "bert.encoder.layer.1.output.dense.bias\n",
    "bert.encoder.layer.1.output.LayerNorm.weight\n",
    "bert.encoder.layer.1.output.LayerNorm.bias\n",
    "bert.encoder.layer.2.attention.self.query.weight\n",
    "bert.encoder.layer.2.attention.self.query.bias\n",
    "bert.encoder.layer.2.attention.self.key.weight\n",
    "bert.encoder.layer.2.attention.self.key.bias\n",
    "bert.encoder.layer.2.attention.self.value.weight\n",
    "bert.encoder.layer.2.attention.self.value.bias\n",
    "bert.encoder.layer.2.attention.output.dense.weight\n",
    "bert.encoder.layer.2.attention.output.dense.bias\n",
    "bert.encoder.layer.2.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.2.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.2.intermediate.dense.weight\n",
    "bert.encoder.layer.2.intermediate.dense.bias\n",
    "bert.encoder.layer.2.output.dense.weight\n",
    "bert.encoder.layer.2.output.dense.bias\n",
    "bert.encoder.layer.2.output.LayerNorm.weight\n",
    "bert.encoder.layer.2.output.LayerNorm.bias\n",
    "bert.encoder.layer.3.attention.self.query.weight\n",
    "bert.encoder.layer.3.attention.self.query.bias\n",
    "bert.encoder.layer.3.attention.self.key.weight\n",
    "bert.encoder.layer.3.attention.self.key.bias\n",
    "bert.encoder.layer.3.attention.self.value.weight\n",
    "bert.encoder.layer.3.attention.self.value.bias\n",
    "bert.encoder.layer.3.attention.output.dense.weight\n",
    "bert.encoder.layer.3.attention.output.dense.bias\n",
    "bert.encoder.layer.3.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.3.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.3.intermediate.dense.weight\n",
    "bert.encoder.layer.3.intermediate.dense.bias\n",
    "bert.encoder.layer.3.output.dense.weight\n",
    "bert.encoder.layer.3.output.dense.bias\n",
    "bert.encoder.layer.3.output.LayerNorm.weight\n",
    "bert.encoder.layer.3.output.LayerNorm.bias\n",
    "bert.encoder.layer.4.attention.self.query.weight\n",
    "bert.encoder.layer.4.attention.self.query.bias\n",
    "bert.encoder.layer.4.attention.self.key.weight\n",
    "bert.encoder.layer.4.attention.self.key.bias\n",
    "bert.encoder.layer.4.attention.self.value.weight\n",
    "bert.encoder.layer.4.attention.self.value.bias\n",
    "bert.encoder.layer.4.attention.output.dense.weight\n",
    "bert.encoder.layer.4.attention.output.dense.bias\n",
    "bert.encoder.layer.4.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.4.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.4.intermediate.dense.weight\n",
    "bert.encoder.layer.4.intermediate.dense.bias\n",
    "bert.encoder.layer.4.output.dense.weight\n",
    "bert.encoder.layer.4.output.dense.bias\n",
    "bert.encoder.layer.4.output.LayerNorm.weight\n",
    "bert.encoder.layer.4.output.LayerNorm.bias\n",
    "bert.encoder.layer.5.attention.self.query.weight\n",
    "bert.encoder.layer.5.attention.self.query.bias\n",
    "bert.encoder.layer.5.attention.self.key.weight\n",
    "bert.encoder.layer.5.attention.self.key.bias\n",
    "bert.encoder.layer.5.attention.self.value.weight\n",
    "bert.encoder.layer.5.attention.self.value.bias\n",
    "bert.encoder.layer.5.attention.output.dense.weight\n",
    "bert.encoder.layer.5.attention.output.dense.bias\n",
    "bert.encoder.layer.5.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.5.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.5.intermediate.dense.weight\n",
    "bert.encoder.layer.5.intermediate.dense.bias\n",
    "bert.encoder.layer.5.output.dense.weight\n",
    "bert.encoder.layer.5.output.dense.bias\n",
    "bert.encoder.layer.5.output.LayerNorm.weight\n",
    "bert.encoder.layer.5.output.LayerNorm.bias\n",
    "bert.encoder.layer.6.attention.self.query.weight\n",
    "bert.encoder.layer.6.attention.self.query.bias\n",
    "bert.encoder.layer.6.attention.self.key.weight\n",
    "bert.encoder.layer.6.attention.self.key.bias\n",
    "bert.encoder.layer.6.attention.self.value.weight\n",
    "bert.encoder.layer.6.attention.self.value.bias\n",
    "bert.encoder.layer.6.attention.output.dense.weight\n",
    "bert.encoder.layer.6.attention.output.dense.bias\n",
    "bert.encoder.layer.6.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.6.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.6.intermediate.dense.weight\n",
    "bert.encoder.layer.6.intermediate.dense.bias\n",
    "bert.encoder.layer.6.output.dense.weight\n",
    "bert.encoder.layer.6.output.dense.bias\n",
    "bert.encoder.layer.6.output.LayerNorm.weight\n",
    "bert.encoder.layer.6.output.LayerNorm.bias\n",
    "bert.encoder.layer.7.attention.self.query.weight\n",
    "bert.encoder.layer.7.attention.self.query.bias\n",
    "bert.encoder.layer.7.attention.self.key.weight\n",
    "bert.encoder.layer.7.attention.self.key.bias\n",
    "bert.encoder.layer.7.attention.self.value.weight\n",
    "bert.encoder.layer.7.attention.self.value.bias\n",
    "bert.encoder.layer.7.attention.output.dense.weight\n",
    "bert.encoder.layer.7.attention.output.dense.bias\n",
    "bert.encoder.layer.7.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.7.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.7.intermediate.dense.weight\n",
    "bert.encoder.layer.7.intermediate.dense.bias\n",
    "bert.encoder.layer.7.output.dense.weight\n",
    "bert.encoder.layer.7.output.dense.bias\n",
    "bert.encoder.layer.7.output.LayerNorm.weight\n",
    "bert.encoder.layer.7.output.LayerNorm.bias\n",
    "bert.encoder.layer.8.attention.self.query.weight\n",
    "bert.encoder.layer.8.attention.self.query.bias\n",
    "bert.encoder.layer.8.attention.self.key.weight\n",
    "bert.encoder.layer.8.attention.self.key.bias\n",
    "bert.encoder.layer.8.attention.self.value.weight\n",
    "bert.encoder.layer.8.attention.self.value.bias\n",
    "bert.encoder.layer.8.attention.output.dense.weight\n",
    "bert.encoder.layer.8.attention.output.dense.bias\n",
    "bert.encoder.layer.8.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.8.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.8.intermediate.dense.weight\n",
    "bert.encoder.layer.8.intermediate.dense.bias\n",
    "bert.encoder.layer.8.output.dense.weight\n",
    "bert.encoder.layer.8.output.dense.bias\n",
    "bert.encoder.layer.8.output.LayerNorm.weight\n",
    "bert.encoder.layer.8.output.LayerNorm.bias\n",
    "bert.encoder.layer.9.attention.self.query.weight\n",
    "bert.encoder.layer.9.attention.self.query.bias\n",
    "bert.encoder.layer.9.attention.self.key.weight\n",
    "bert.encoder.layer.9.attention.self.key.bias\n",
    "bert.encoder.layer.9.attention.self.value.weight\n",
    "bert.encoder.layer.9.attention.self.value.bias\n",
    "bert.encoder.layer.9.attention.output.dense.weight\n",
    "bert.encoder.layer.9.attention.output.dense.bias\n",
    "bert.encoder.layer.9.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.9.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.9.intermediate.dense.weight\n",
    "bert.encoder.layer.9.intermediate.dense.bias\n",
    "bert.encoder.layer.9.output.dense.weight\n",
    "bert.encoder.layer.9.output.dense.bias\n",
    "bert.encoder.layer.9.output.LayerNorm.weight\n",
    "bert.encoder.layer.9.output.LayerNorm.bias\n",
    "bert.encoder.layer.10.attention.self.query.weight\n",
    "bert.encoder.layer.10.attention.self.query.bias\n",
    "bert.encoder.layer.10.attention.self.key.weight\n",
    "bert.encoder.layer.10.attention.self.key.bias\n",
    "bert.encoder.layer.10.attention.self.value.weight\n",
    "bert.encoder.layer.10.attention.self.value.bias\n",
    "bert.encoder.layer.10.attention.output.dense.weight\n",
    "bert.encoder.layer.10.attention.output.dense.bias\n",
    "bert.encoder.layer.10.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.10.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.10.intermediate.dense.weight\n",
    "bert.encoder.layer.10.intermediate.dense.bias\n",
    "bert.encoder.layer.10.output.dense.weight\n",
    "bert.encoder.layer.10.output.dense.bias\n",
    "bert.encoder.layer.10.output.LayerNorm.weight\n",
    "bert.encoder.layer.10.output.LayerNorm.bias\n",
    "bert.encoder.layer.11.attention.self.query.weight\n",
    "bert.encoder.layer.11.attention.self.query.bias\n",
    "bert.encoder.layer.11.attention.self.key.weight\n",
    "bert.encoder.layer.11.attention.self.key.bias\n",
    "bert.encoder.layer.11.attention.self.value.weight\n",
    "bert.encoder.layer.11.attention.self.value.bias\n",
    "bert.encoder.layer.11.attention.output.dense.weight\n",
    "bert.encoder.layer.11.attention.output.dense.bias\n",
    "bert.encoder.layer.11.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.11.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.11.intermediate.dense.weight\n",
    "bert.encoder.layer.11.intermediate.dense.bias\n",
    "bert.encoder.layer.11.output.dense.weight\n",
    "bert.encoder.layer.11.output.dense.bias\n",
    "bert.encoder.layer.11.output.LayerNorm.weight\n",
    "bert.encoder.layer.11.output.LayerNorm.bias\n",
    "bert.encoder.layer.12.attention.self.query.weight\n",
    "bert.encoder.layer.12.attention.self.query.bias\n",
    "bert.encoder.layer.12.attention.self.key.weight\n",
    "bert.encoder.layer.12.attention.self.key.bias\n",
    "bert.encoder.layer.12.attention.self.value.weight\n",
    "bert.encoder.layer.12.attention.self.value.bias\n",
    "bert.encoder.layer.12.attention.output.dense.weight\n",
    "bert.encoder.layer.12.attention.output.dense.bias\n",
    "bert.encoder.layer.12.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.12.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.12.intermediate.dense.weight\n",
    "bert.encoder.layer.12.intermediate.dense.bias\n",
    "bert.encoder.layer.12.output.dense.weight\n",
    "bert.encoder.layer.12.output.dense.bias\n",
    "bert.encoder.layer.12.output.LayerNorm.weight\n",
    "bert.encoder.layer.12.output.LayerNorm.bias\n",
    "bert.encoder.layer.13.attention.self.query.weight\n",
    "bert.encoder.layer.13.attention.self.query.bias\n",
    "bert.encoder.layer.13.attention.self.key.weight\n",
    "bert.encoder.layer.13.attention.self.key.bias\n",
    "bert.encoder.layer.13.attention.self.value.weight\n",
    "bert.encoder.layer.13.attention.self.value.bias\n",
    "bert.encoder.layer.13.attention.output.dense.weight\n",
    "bert.encoder.layer.13.attention.output.dense.bias\n",
    "bert.encoder.layer.13.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.13.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.13.intermediate.dense.weight\n",
    "bert.encoder.layer.13.intermediate.dense.bias\n",
    "bert.encoder.layer.13.output.dense.weight\n",
    "bert.encoder.layer.13.output.dense.bias\n",
    "bert.encoder.layer.13.output.LayerNorm.weight\n",
    "bert.encoder.layer.13.output.LayerNorm.bias\n",
    "bert.encoder.layer.14.attention.self.query.weight\n",
    "bert.encoder.layer.14.attention.self.query.bias\n",
    "bert.encoder.layer.14.attention.self.key.weight\n",
    "bert.encoder.layer.14.attention.self.key.bias\n",
    "bert.encoder.layer.14.attention.self.value.weight\n",
    "bert.encoder.layer.14.attention.self.value.bias\n",
    "bert.encoder.layer.14.attention.output.dense.weight\n",
    "bert.encoder.layer.14.attention.output.dense.bias\n",
    "bert.encoder.layer.14.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.14.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.14.intermediate.dense.weight\n",
    "bert.encoder.layer.14.intermediate.dense.bias\n",
    "bert.encoder.layer.14.output.dense.weight\n",
    "bert.encoder.layer.14.output.dense.bias\n",
    "bert.encoder.layer.14.output.LayerNorm.weight\n",
    "bert.encoder.layer.14.output.LayerNorm.bias\n",
    "bert.encoder.layer.15.attention.self.query.weight\n",
    "bert.encoder.layer.15.attention.self.query.bias\n",
    "bert.encoder.layer.15.attention.self.key.weight\n",
    "bert.encoder.layer.15.attention.self.key.bias\n",
    "bert.encoder.layer.15.attention.self.value.weight\n",
    "bert.encoder.layer.15.attention.self.value.bias\n",
    "bert.encoder.layer.15.attention.output.dense.weight\n",
    "bert.encoder.layer.15.attention.output.dense.bias\n",
    "bert.encoder.layer.15.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.15.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.15.intermediate.dense.weight\n",
    "bert.encoder.layer.15.intermediate.dense.bias\n",
    "bert.encoder.layer.15.output.dense.weight\n",
    "bert.encoder.layer.15.output.dense.bias\n",
    "bert.encoder.layer.15.output.LayerNorm.weight\n",
    "bert.encoder.layer.15.output.LayerNorm.bias\n",
    "bert.encoder.layer.16.attention.self.query.weight\n",
    "bert.encoder.layer.16.attention.self.query.bias\n",
    "bert.encoder.layer.16.attention.self.key.weight\n",
    "bert.encoder.layer.16.attention.self.key.bias\n",
    "bert.encoder.layer.16.attention.self.value.weight\n",
    "bert.encoder.layer.16.attention.self.value.bias\n",
    "bert.encoder.layer.16.attention.output.dense.weight\n",
    "bert.encoder.layer.16.attention.output.dense.bias\n",
    "bert.encoder.layer.16.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.16.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.16.intermediate.dense.weight\n",
    "bert.encoder.layer.16.intermediate.dense.bias\n",
    "bert.encoder.layer.16.output.dense.weight\n",
    "bert.encoder.layer.16.output.dense.bias\n",
    "bert.encoder.layer.16.output.LayerNorm.weight\n",
    "bert.encoder.layer.16.output.LayerNorm.bias\n",
    "bert.encoder.layer.17.attention.self.query.weight\n",
    "bert.encoder.layer.17.attention.self.query.bias\n",
    "bert.encoder.layer.17.attention.self.key.weight\n",
    "bert.encoder.layer.17.attention.self.key.bias\n",
    "bert.encoder.layer.17.attention.self.value.weight\n",
    "bert.encoder.layer.17.attention.self.value.bias\n",
    "bert.encoder.layer.17.attention.output.dense.weight\n",
    "bert.encoder.layer.17.attention.output.dense.bias\n",
    "bert.encoder.layer.17.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.17.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.17.intermediate.dense.weight\n",
    "bert.encoder.layer.17.intermediate.dense.bias\n",
    "bert.encoder.layer.17.output.dense.weight\n",
    "bert.encoder.layer.17.output.dense.bias\n",
    "bert.encoder.layer.17.output.LayerNorm.weight\n",
    "bert.encoder.layer.17.output.LayerNorm.bias\n",
    "bert.encoder.layer.18.attention.self.query.weight\n",
    "bert.encoder.layer.18.attention.self.query.bias\n",
    "bert.encoder.layer.18.attention.self.key.weight\n",
    "bert.encoder.layer.18.attention.self.key.bias\n",
    "bert.encoder.layer.18.attention.self.value.weight\n",
    "bert.encoder.layer.18.attention.self.value.bias\n",
    "bert.encoder.layer.18.attention.output.dense.weight\n",
    "bert.encoder.layer.18.attention.output.dense.bias\n",
    "bert.encoder.layer.18.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.18.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.18.intermediate.dense.weight\n",
    "bert.encoder.layer.18.intermediate.dense.bias\n",
    "bert.encoder.layer.18.output.dense.weight\n",
    "bert.encoder.layer.18.output.dense.bias\n",
    "bert.encoder.layer.18.output.LayerNorm.weight\n",
    "bert.encoder.layer.18.output.LayerNorm.bias\n",
    "bert.encoder.layer.19.attention.self.query.weight\n",
    "bert.encoder.layer.19.attention.self.query.bias\n",
    "bert.encoder.layer.19.attention.self.key.weight\n",
    "bert.encoder.layer.19.attention.self.key.bias\n",
    "bert.encoder.layer.19.attention.self.value.weight\n",
    "bert.encoder.layer.19.attention.self.value.bias\n",
    "bert.encoder.layer.19.attention.output.dense.weight\n",
    "bert.encoder.layer.19.attention.output.dense.bias\n",
    "bert.encoder.layer.19.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.19.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.19.intermediate.dense.weight\n",
    "bert.encoder.layer.19.intermediate.dense.bias\n",
    "bert.encoder.layer.19.output.dense.weight\n",
    "bert.encoder.layer.19.output.dense.bias\n",
    "bert.encoder.layer.19.output.LayerNorm.weight\n",
    "bert.encoder.layer.19.output.LayerNorm.bias\n",
    "bert.encoder.layer.20.attention.self.query.weight\n",
    "bert.encoder.layer.20.attention.self.query.bias\n",
    "bert.encoder.layer.20.attention.self.key.weight\n",
    "bert.encoder.layer.20.attention.self.key.bias\n",
    "bert.encoder.layer.20.attention.self.value.weight\n",
    "bert.encoder.layer.20.attention.self.value.bias\n",
    "bert.encoder.layer.20.attention.output.dense.weight\n",
    "bert.encoder.layer.20.attention.output.dense.bias\n",
    "bert.encoder.layer.20.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.20.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.20.intermediate.dense.weight\n",
    "bert.encoder.layer.20.intermediate.dense.bias\n",
    "bert.encoder.layer.20.output.dense.weight\n",
    "bert.encoder.layer.20.output.dense.bias\n",
    "bert.encoder.layer.20.output.LayerNorm.weight\n",
    "bert.encoder.layer.20.output.LayerNorm.bias\n",
    "bert.encoder.layer.21.attention.self.query.weight\n",
    "bert.encoder.layer.21.attention.self.query.bias\n",
    "bert.encoder.layer.21.attention.self.key.weight\n",
    "bert.encoder.layer.21.attention.self.key.bias\n",
    "bert.encoder.layer.21.attention.self.value.weight\n",
    "bert.encoder.layer.21.attention.self.value.bias\n",
    "bert.encoder.layer.21.attention.output.dense.weight\n",
    "bert.encoder.layer.21.attention.output.dense.bias\n",
    "bert.encoder.layer.21.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.21.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.21.intermediate.dense.weight\n",
    "bert.encoder.layer.21.intermediate.dense.bias\n",
    "bert.encoder.layer.21.output.dense.weight\n",
    "bert.encoder.layer.21.output.dense.bias\n",
    "bert.encoder.layer.21.output.LayerNorm.weight\n",
    "bert.encoder.layer.21.output.LayerNorm.bias\n",
    "bert.encoder.layer.22.attention.self.query.weight\n",
    "bert.encoder.layer.22.attention.self.query.bias\n",
    "bert.encoder.layer.22.attention.self.key.weight\n",
    "bert.encoder.layer.22.attention.self.key.bias\n",
    "bert.encoder.layer.22.attention.self.value.weight\n",
    "bert.encoder.layer.22.attention.self.value.bias\n",
    "bert.encoder.layer.22.attention.output.dense.weight\n",
    "bert.encoder.layer.22.attention.output.dense.bias\n",
    "bert.encoder.layer.22.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.22.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.22.intermediate.dense.weight\n",
    "bert.encoder.layer.22.intermediate.dense.bias\n",
    "bert.encoder.layer.22.output.dense.weight\n",
    "bert.encoder.layer.22.output.dense.bias\n",
    "bert.encoder.layer.22.output.LayerNorm.weight\n",
    "bert.encoder.layer.22.output.LayerNorm.bias\n",
    "bert.encoder.layer.23.attention.self.query.weight\n",
    "bert.encoder.layer.23.attention.self.query.bias\n",
    "bert.encoder.layer.23.attention.self.key.weight\n",
    "bert.encoder.layer.23.attention.self.key.bias\n",
    "bert.encoder.layer.23.attention.self.value.weight\n",
    "bert.encoder.layer.23.attention.self.value.bias\n",
    "bert.encoder.layer.23.attention.output.dense.weight\n",
    "bert.encoder.layer.23.attention.output.dense.bias\n",
    "bert.encoder.layer.23.attention.output.LayerNorm.weight\n",
    "bert.encoder.layer.23.attention.output.LayerNorm.bias\n",
    "bert.encoder.layer.23.intermediate.dense.weight\n",
    "bert.encoder.layer.23.intermediate.dense.bias\n",
    "bert.encoder.layer.23.output.dense.weight\n",
    "bert.encoder.layer.23.output.dense.bias\n",
    "bert.encoder.layer.23.output.LayerNorm.weight\n",
    "bert.encoder.layer.23.output.LayerNorm.bias\n",
    "qa_outputs.weight\n",
    "qa_outputs.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a767a0c3-1350-480e-ad51-db361ffaa6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR! Session/line number was not unique in database. History logging moved to new session 127\n",
      "tensor(0.8616, device='cuda:0')\n",
      "<class 'str'> <class 'torch.nn.parameter.Parameter'>\n",
      "qa_outputs.weight Parameter containing:\n",
      "tensor([[ 0.0234, -0.0060,  0.0238,  ..., -0.0315, -0.0137,  0.0296],\n",
      "        [ 0.0481,  0.0265,  0.0083,  ...,  0.0021, -0.0065,  0.0082]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "tensor([ 0.0234, -0.0060,  0.0238,  ..., -0.0315, -0.0137,  0.0296],\n",
      "       device='cuda:0')\n",
      "tensor([0.9938, 0.6113, 0.4779,  ..., 0.2881, 0.1735, 0.9467], device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 576x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import load_dataset\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# replace <PATd:/spofrte/modeH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "model_path = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "\n",
    "# load model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "for name,param in model.named_parameters():\n",
    "    if name == 'qa_outputs.weight':\n",
    "        print(type(name), type(param))\n",
    "        print(name, param)\n",
    "        print(param.data[0])\n",
    "        param.data[0] = torch.rand(1024)\n",
    "        print(param.data[0])\n",
    "    if name == 'bert.encoder.layer.23.output.LayerNorm.weight':\n",
    "        temp = torch.rand(1)\n",
    "\n",
    "        param.data[0] = temp\n",
    "        print(param.data[0])\n",
    "\n",
    "\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs, token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids, attention_mask=attention_mask, )\n",
    "    return output.start_logits, output.end_logits\n",
    "\n",
    "\n",
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values\n",
    "\n",
    "fig = plt.figure()  \n",
    "fig.set_size_inches(8, 6)\n",
    "\n",
    "ref_token_id = tokenizer.pad_token_id  # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id  # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id  # A token used for prepending to the concatenated question-text word sequence\n",
    "\n",
    "\n",
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions\n",
    "\n",
    "\n",
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "                    [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    seq_len = input_ids.size(1)\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)  # * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "\n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.bert.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.bert.embeddings(ref_input_ids, token_type_ids=token_type_ids,\n",
    "                                                 position_ids=position_ids)\n",
    "\n",
    "    return input_embeddings, ref_input_embeddings\n",
    "\n",
    "\n",
    "def predict_qt(question, text):\n",
    "    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id,\n",
    "                                                                cls_token_id)\n",
    "    token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "    position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "    attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "    ground_truth = '13'\n",
    "\n",
    "    start_scores, end_scores = predict(input_ids, \\\n",
    "                                       token_type_ids=token_type_ids, \\\n",
    "                                       position_ids=position_ids, \\\n",
    "                                       attention_mask=attention_mask)\n",
    "\n",
    "    print('Question: ', question)\n",
    "    print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1]))\n",
    "    return input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens,\n",
    "\n",
    "\n",
    "def explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores,\n",
    "            ground_truth, all_tokens, ):\n",
    "    lig = LayerIntegratedGradients(squad_pos_forward_func, model.bert.embeddings)\n",
    "\n",
    "    attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "                                                    baselines=ref_input_ids,\n",
    "                                                    additional_forward_args=(\n",
    "                                                        token_type_ids, position_ids, attention_mask, 0),\n",
    "                                                    internal_batch_size=4,\n",
    "                                                    return_convergence_delta=True)\n",
    "    attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "                                                additional_forward_args=(\n",
    "                                                    token_type_ids, position_ids, attention_mask, 1),\n",
    "                                                internal_batch_size=4,\n",
    "                                                return_convergence_delta=True)\n",
    "\n",
    "    attributions_start_sum = summarize_attributions(attributions_start)\n",
    "    attributions_end_sum = summarize_attributions(attributions_end)\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    start_position_vis = viz.VisualizationDataRecord(\n",
    "        attributions_start_sum,\n",
    "        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "        torch.argmax(start_scores),\n",
    "        torch.argmax(start_scores),\n",
    "        str(ground_truth),\n",
    "        attributions_start_sum.sum(),\n",
    "        all_tokens,\n",
    "        delta_start)\n",
    "\n",
    "    end_position_vis = viz.VisualizationDataRecord(\n",
    "        attributions_end_sum,\n",
    "        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "        torch.argmax(end_scores),\n",
    "        torch.argmax(end_scores),\n",
    "        str(ground_truth),\n",
    "        attributions_end_sum.sum(),\n",
    "        all_tokens,\n",
    "        delta_end)\n",
    "    #print(all_tokens)\n",
    "    print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "    viz.visualize_text([start_position_vis])\n",
    "\n",
    "    print('\\033[1m', 'Visualizations For End Position', '\\033[0m')\n",
    "\n",
    "    print(\"attributions_start_sum:   \", len(attributions_start_sum))\n",
    "    #print(\"all tokens:    \", len(all_tokens))\n",
    "\n",
    "    return all_tokens, attributions_start_sum\n",
    "\n",
    "\n",
    "def get_posneg(all_tokens, attributions_start_sum):\n",
    "    positive = []\n",
    "    negative = []\n",
    "    neutral = []\n",
    "    for i, j in enumerate(attributions_start_sum):\n",
    "        if j > 0:\n",
    "            positive.append(i)\n",
    "            # print('positive:',j)\n",
    "        ##print(all_tokens[i])\n",
    "        elif j < 0:\n",
    "            negative.append(i)\n",
    "            # print('negative:',j)\n",
    "            # print(all_tokens[i])\n",
    "        elif j == 0:\n",
    "            neutral.append(i)\n",
    "\n",
    "    s_pos = ''\n",
    "    s_neg = ''\n",
    "\n",
    "    # print(len(attributions_start_sum))\n",
    "    # print(len(positive))\n",
    "    # print(len(negative))\n",
    "\n",
    "    for i in positive:\n",
    "        s_pos += all_tokens[i] + ' '\n",
    "    #print(\"positive :\", s_pos)\n",
    "    for i in negative:\n",
    "        s_neg += all_tokens[i] + ' '\n",
    "    #print(\"negative :\", s_neg)\n",
    "    return positive, negative, neutral\n",
    "\n",
    "\n",
    "def separate_sentence(all_tokens):\n",
    "    sentence = {}\n",
    "    temp = []\n",
    "    num = 0\n",
    "    for i in range(len(all_tokens)):\n",
    "        if all_tokens[i] == \",\" or all_tokens[i] == \".\":\n",
    "            temp.append(all_tokens[i])\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        elif all_tokens[i] == \"[CLS]\":\n",
    "            temp.append(all_tokens[i])\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        elif all_tokens[i] == \"[SEP]\":\n",
    "            sentence[num] = temp\n",
    "            num = num + 1\n",
    "            temp = [all_tokens[i]]\n",
    "            sentence[num] = temp\n",
    "            temp = []\n",
    "            num = num + 1\n",
    "        else:\n",
    "            temp.append(all_tokens[i])\n",
    "    return sentence\n",
    "def get_sence_score(sentence, attributions_start_sum):\n",
    "    weight = 0\n",
    "    sum_weight = 0\n",
    "    sentence_value=[]\n",
    "    delete_sentence = []\n",
    "    for k,v in sentence.items():\n",
    "        for i in v:\n",
    "            sentence_value.append(i)\n",
    "    scores={}\n",
    "\n",
    "\n",
    "    for i in range(len(attributions_start_sum)):\n",
    "        try:\n",
    "            scores[sentence_value[i]]=attributions_start_sum[i].item()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "    for i, j in sentence.items():\n",
    "        sum_weight = 0\n",
    "        for word in j:\n",
    "            sum_weight +=  scores[word]\n",
    "        delete_sentence.append(sum_weight)\n",
    "        #print(sum_weight)\n",
    "    return delete_sentence\n",
    "\n",
    "def get_delete(sentence):\n",
    "    weight = 0\n",
    "    sum_weight = 0\n",
    "    sentence_value = []\n",
    "    delete_sentence = {}\n",
    "    for k, v in sentence.items():\n",
    "        # print(k,':',v)\n",
    "        for i in v:\n",
    "            sentence_value.append(i)\n",
    "    #print(sentence_value)\n",
    "    scores = {}\n",
    "    # print(attributions_start_sum[0].item())\n",
    "\n",
    "    for i in range(len(attributions_start_sum)):\n",
    "        try:\n",
    "            scores[sentence_value[i]] = attributions_start_sum[i].item()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    for i, j in sentence.items():\n",
    "        sum_weight = 0\n",
    "        for word in j:\n",
    "            weight = 0\n",
    "\n",
    "            sum_weight += scores[word]\n",
    "            delete_sentence[i] = sum_weight\n",
    "    return delete_sentence\n",
    "\n",
    "\n",
    "def delete_sentence(sentence, li_delete_sentence):\n",
    "    for i, j in sentence.items():\n",
    "        if i in li_delete_sentence:\n",
    "            sentence[i] = []\n",
    "        else:\n",
    "            pass\n",
    "    return sentence\n",
    "\n",
    "\n",
    "def rebuild_sentence(ori_sentence):\n",
    "    rebuild_str = \"\"\n",
    "    for i, j in ori_sentence.items():\n",
    "        for word in j:\n",
    "            rebuild_str += word\n",
    "            rebuild_str += \" \"\n",
    "    return rebuild_str\n",
    "\n",
    "\n",
    "def pred_explain(question, text):\n",
    "    input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens, = predict_qt(\n",
    "        text, question)\n",
    "\n",
    "    all_tokens, attributions_start_sum = explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask,\n",
    "                                                 start_scores, end_scores, ground_truth, all_tokens, )\n",
    "\n",
    "    end_score = float(torch.max(torch.softmax(end_scores[0], dim=0)))\n",
    "    start_score = float(torch.max(torch.softmax(start_scores[0], dim=0)))\n",
    "    return all_tokens, attributions_start_sum, end_score, start_score, [torch.argmax(start_scores), torch.argmax(end_scores)+1], start_scores, end_scores\n",
    "def max_min(x, y, z):\n",
    "    max = min = x\n",
    "    i = 1\n",
    "    if y > max:\n",
    "        max = y\n",
    "        i = 2\n",
    "    else:\n",
    "        min = y\n",
    "    if z > max:\n",
    "        max = z\n",
    "        i =3\n",
    "    else:\n",
    "        min = z\n",
    "    return (i)\n",
    "def cycle_prediction(cycle_num, question, text):\n",
    "    all_tokens, attributions_start_sum, start_acc, end_acc,  an_index, start_scores, end_scores = pred_explain(text, question)\n",
    "    first_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "    first_answer = re.sub(r' ##', '', first_answer)\n",
    "    print(\"my answer is \", first_answer)\n",
    "    print(start_acc, end_acc)\n",
    "    second_answer = ''\n",
    "    sentence = separate_sentence(all_tokens)\n",
    "    a = 0\n",
    "    pos_contri = 0\n",
    "    neg_contri = 0\n",
    "    average_neg = []\n",
    "    average_pos = []\n",
    "    for i, j in enumerate(attributions_start_sum):\n",
    "        if j <0 : \n",
    "            neg_contri+= j\n",
    "        elif j >0:\n",
    "            pos_contri += j\n",
    "    print(\"positive contribution:\", pos_contri)\n",
    "    average_pos.append(pos_contri)\n",
    "    print(\"negative contribution:\", neg_contri)\n",
    "    average_neg.append(neg_contri)\n",
    "\n",
    "    acc_s = []\n",
    "    acc_e = []\n",
    "    sun = []\n",
    "    ans = []\n",
    "    ans.append(first_answer)\n",
    "    #print(start_acc, end_acc)\n",
    "    acc_s.append(start_acc)\n",
    "    acc_e.append(end_acc)\n",
    "\n",
    "    for loop in range(cycle_num):\n",
    "        sentence = separate_sentence(all_tokens)\n",
    "        sentence_score  = get_sence_score(sentence, attributions_start_sum)\n",
    "        min_sensocer = 999\n",
    "        min_index = 999\n",
    "        for i in range(len(sentence_score)):\n",
    "            if sentence_score[i] < min_sensocer and sentence_score[i] != 0:\n",
    "                min_sensocer = sentence_score[i]\n",
    "                min_index = i\n",
    "        #print(\"should delete\", min_index, min_sensocer)\n",
    "        sentence[min_index] = ''\n",
    "        sentence[1] = ''\n",
    "        retext = \"\"\n",
    "        for i, j in sentence.items():\n",
    "            for words in j:\n",
    "                retext = retext + words + \" \"\n",
    "        li_sep = []\n",
    "        for m in re.finditer(r\"SEP\", retext):\n",
    "            li_sep.append(m.start())\n",
    "            li_sep.append(m.end())\n",
    "        retext = retext[li_sep[1]+1: li_sep[2] -1]\n",
    "        retext = re.sub(r' ##', '', retext)\n",
    "\n",
    "\n",
    "\n",
    "        all_tokens, attributions_start_sum, start_acc, end_acc,  an_index, start_scores, end_scores= pred_explain(retext, question)\n",
    "        reanswer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "        #print(start_acc, end_acc)\n",
    "        second_answer = ' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1])\n",
    "        second_answer = re.sub(r' ##', '', second_answer)\n",
    "        #print(\"my answer is \", second_answer)\n",
    "        ans.append(second_answer)\n",
    "        #print(start_acc, end_acc)\n",
    "        acc_s.append(start_acc)\n",
    "        acc_e.append(end_acc)\n",
    "        pos_contri = 0\n",
    "        neg_contri = 0\n",
    "        for i, j in enumerate(attributions_start_sum):\n",
    "            if j <0:\n",
    "                neg_contri+= j\n",
    "            elif j >0:\n",
    "                pos_contri += j\n",
    "        print(\"positive contribution:\", pos_contri)\n",
    "        average_pos.append(pos_contri)\n",
    "        print(\"negative contribution:\", neg_contri)\n",
    "        average_neg.append(neg_contri)\n",
    "\n",
    "\n",
    "        #print(acc_s, acc_e)\n",
    "        #print(acc_s, acc_e)\n",
    "    plt.plot(range(len(acc_s)), acc_s, label = 'start score')\n",
    "    plt.plot(range(len(acc_s)), acc_e, label = 'end score')\n",
    "    sun = []\n",
    "    for i in range(len(acc_s)):\n",
    "        sun.append((acc_s[i] + acc_e[i])/2)\n",
    "    print(sun)\n",
    "    plt.plot(range(len(acc_s)), sun, label = 'average')\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('Possibility')    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "        \n",
    "    \"\"\"\"获取最好的曲线并输出\"\"\"\n",
    "    max_start = 0\n",
    "    max_end = 0\n",
    "    max_ave = 0\n",
    "    for i in acc_s:\n",
    "        if i > max_start:\n",
    "            max_start = i\n",
    "    for j in acc_e:\n",
    "        if j > max_end:\n",
    "            max_end = i\n",
    "\n",
    "    for x in sun:\n",
    "        if x > max_ave:\n",
    "            max_ave = x\n",
    "\n",
    "    print(max_start ,max_end ,max_ave )\n",
    "\n",
    "\n",
    "    max_list = max_min(max_start, max_end, max_ave)\n",
    "    if max_list == 1:\n",
    "        plt.plot(range(len(acc_s)), acc_s, label = 'Possibility')\n",
    "        print(acc_s)\n",
    "    if max_list == 2:\n",
    "        plt.plot(range(len(acc_e)), acc_e, label = 'Possibility')\n",
    "        print(acc_e)\n",
    "    if max_list == 3:\n",
    "        plt.plot(range(len(sun)), sun, label = 'Possibility')\n",
    "        print(sun)\n",
    "\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('Possibility')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \"\"\"贡献的数值分别是多少\"\"\"\n",
    "    plt.plot(range(len(average_pos)), average_pos, label = 'pos score')\n",
    "    plt.plot(range(len(average_neg)), average_neg, label = 'neg score')\n",
    "    plt.xlabel('Number of predictions')\n",
    "    plt.ylabel('average contribution of pos/neg')    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    for i in range(len(ans)):\n",
    "        print(ans[i], \"pos/neg : \", -(average_pos[i]/average_neg[i]))\n",
    "    average_contribution = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a04f36f0-c8fe-4de3-944f-7740476c31f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What magazine named Beyoncé as the most powerful female musician for 2015?\n",
      "Predicted Answer:  \n",
      "\u001b[1m Visualizations For Start Position \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>226</b></text></td><td><text style=\"padding-right:2em\"><b>226 (0.11)</b></text></td><td><text style=\"padding-right:2em\"><b>13</b></text></td><td><text style=\"padding-right:2em\"><b>0.09</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> magazine                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> named                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> beyonce                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> powerful                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> female                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> musician                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> for                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2015                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> self                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> described                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> modern                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> day                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> feminist                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> beyonce                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> creates                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> songs                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> that                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> are                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> often                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> characterized                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> by                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> themes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> love                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> relationships                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> mono                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##gam                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##y                    </font></mark><mark style=\"background-color: hsl(120, 75%, 81%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> well                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> female                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sexuality                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> empowerment                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> on                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> stage                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> dynamic                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> highly                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> choreographed                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> performances                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> have                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> led                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> critics                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> hail                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##ing                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> entertainer                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> contemporary                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> popular                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> music                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> throughout                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> career                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> spanning                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 19                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> years                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> sold                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> over                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 118                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> million                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> records                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> solo                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> a                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> further                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 60                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> million                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> with                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> destiny                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> child                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> making                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> one                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> best                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> -                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> selling                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> music                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artists                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> she                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> has                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> won                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 20                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> grammy                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> awards                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> nominated                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> woman                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> award                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> '                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> history                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> recording                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> industry                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> association                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> america                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> recognized                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> top                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> certified                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> america                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> during                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2000s                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decade                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2009                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> billboard                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> named                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> top                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> radio                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> songs                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> decade                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> top                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> female                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artist                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2000s                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> their                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> artist                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> millennium                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2011                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> time                    </font></mark><mark style=\"background-color: hsl(120, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> listed                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> among                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 100                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> influential                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> people                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> world                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> in                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2013                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2014                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> forbes                    </font></mark><mark style=\"background-color: hsl(120, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> magazine                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> also                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> listed                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> her                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> as                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> the                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> most                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> powerful                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> female                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> musician                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> 2015                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Visualizations For End Position \u001b[0m\n",
      "attributions_start_sum:    227\n"
     ]
    }
   ],
   "source": [
    "text = \"A self-described 'modern-day feminist', Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.\"\n",
    "question = \"What magazine named Beyoncé as the most powerful female musician for 2015?\"\n",
    "\n",
    "all_tokens, attributions_start_sum, start_acc, end_acc,  an_index, start_scores, end_scores = pred_explain(text, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79055c5a-323b-4e5c-af06-c53421790eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "attr_nomal = []\n",
    "for i, j in enumerate(attributions_start_sum):\n",
    "    attr_nomal.append(j)\n",
    "plt.plot(range(len(attr_nomal)), attr_nomal)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100cebdd-1df5-41cb-8d49-f6925f1fbb1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

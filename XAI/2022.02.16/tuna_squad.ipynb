{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d34e089f-cea2-4080-9f20-2a58321fa715",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\GZK\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d597fbd9598845b0907ffea9559ac146",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6188b444-b5af-46f9-badd-02b463f571fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f28269f-c741-415b-b379-133c9f28a478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85bc5af1-3391-4ff9-be31-28539bb2d5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at C:\\Users\\GZK\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-1ec1cd9b8a7518e3.arrow\n",
      "Loading cached processed dataset at C:\\Users\\GZK\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453\\cache-82bbdcbaee462804.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ff5760c-ecab-404c-8cac-ffc965ec238d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "data_collator = default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b201b6-d585-40b7-861f-6d32eafba7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4dfa998-2f12-4678-86f0-8f40c3aa71ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38b3ede6-17cf-4d9d-8f23-3b3dee0c7ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fbed9a5-d881-4a70-b0c3-190f6be8ce53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 66.36M\n"
     ]
    }
   ],
   "source": [
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a20c7cb-24fa-4547-a6ba-8939c6288b8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='15269' max='16425' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15269/16425 1:15:28 < 05:42, 3.37 it/s, Epoch 2.79/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.245800</td>\n",
       "      <td>1.141883</td>\n",
       "      <td>60.830400</td>\n",
       "      <td>173.762000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.916400</td>\n",
       "      <td>1.095070</td>\n",
       "      <td>60.360600</td>\n",
       "      <td>175.114000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a128ded-06d0-467f-85fe-d5453824590b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# torch.save(model, 'net.pkl')  # save entire net # save only the parameters\n",
    "model = torch.load('net.pkl')  # 加载神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97407623-2a4c-4b75-b95c-8c9e9e215359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QuestionAnsweringModelOutput(loss=None, start_logits=tensor([[-4.9537, -0.9243, -3.4084, -1.8041, -0.0718, -0.9083, -3.7310, -7.3723,\n",
      "         -5.4588, -4.8745, -2.5794, -7.2470]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>), end_logits=tensor([[-6.3243, -4.3402, -3.5235, -6.2792, -5.2571, -0.5210, -6.0681, -4.8629,\n",
      "         -6.4497, -2.6533, -2.0360, -4.7456]], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>), hidden_states=None, attentions=None)\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "# replace <PATd:/spofrte/modeH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "model.to(device)\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "encoded_input.to(device)\n",
    "output = model(**encoded_input)\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_idsprint(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d614d2c2-8032-4dc4-a708-61ba8067c2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "\n",
    "\n",
    "def predict(inputs):\n",
    "    output = model(inputs)\n",
    "    return output.start_logits, output.end_logits\n",
    "\n",
    "\n",
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "                    [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def predict_qt(question, text):\n",
    "    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "    ground_truth = '13'\n",
    "\n",
    "\n",
    "    start_scores, end_scores = predict(input_ids)\n",
    "\n",
    "\n",
    "    return (' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e72ba368-3ad0-4a87-8874-9199787349fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the boy in red si z ##ik ##un\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"who is the boy in red\"\"\"\n",
    "text = \"the boy in red si zikun\"\n",
    "answer = predict_qt(question, text)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54481ea6-0da7-4826-a5f4-a93326f850ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\GZK\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2814ee93e04441bca816640eed8d5357",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6976/1814167705.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0manswers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'validation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'answers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_qt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mf1score\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mcompute_f1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6976/200665360.py\u001b[0m in \u001b[0;36mpredict_qt\u001b[1;34m(question, text)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_qt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_input_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstruct_input_ref_pair\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mref_token_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep_token_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls_token_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6976/200665360.py\u001b[0m in \u001b[0;36mconstruct_input_ref_pair\u001b[1;34m(question, text, ref_token_id, sep_token_id, cls_token_id)\u001b[0m\n\u001b[0;32m     20\u001b[0m                     \u001b[1;33m[\u001b[0m\u001b[0mref_token_id\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext_ids\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0msep_token_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mref_input_ids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpredict_qt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "datasets = load_dataset('squad')\n",
    "f1score = 0\n",
    "for i in range(len(datasets['validation'])):\n",
    "    text = datasets['validation'][i]['context']\n",
    "    question = datasets['validation'][i]['question']\n",
    "    answers = datasets['validation'][i]['answers']['text'][0]\n",
    "\n",
    "    answer = predict_qt(question, text)\n",
    "    f1score += compute_f1(answer, answers)\n",
    "    print(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "58ccb524-6527-4f2a-b17f-aca384902b5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (C:\\Users\\GZK\\.cache\\huggingface\\datasets\\squad\\plain_text\\1.0.0\\d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448ed11e22d24e6fbda623bd19522e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question:  Which NFL team represented the AFC at Super Bowl 50?\n",
      "Predicted Answer:  denver broncos\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DistilBertForQuestionAnswering' object has no attribute 'bert'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6976/928765746.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[0manswers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'validation'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'answers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 219\u001b[1;33m     \u001b[0mexp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgenerate_explain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    220\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[0mwrite_exp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6976/928765746.py\u001b[0m in \u001b[0;36mgenerate_explain\u001b[1;34m(i, question, text, answers)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m     \u001b[0mall_tokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattributions_start_sum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred_explain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m     \u001b[0mend_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[0mstart_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6976/928765746.py\u001b[0m in \u001b[0;36mpred_explain\u001b[1;34m(question, text)\u001b[0m\n\u001b[0;32m     83\u001b[0m         question, text)\n\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m     all_tokens, attributions_start_sum = explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask,\n\u001b[0m\u001b[0;32m     86\u001b[0m                                                  start_scores, end_scores, ground_truth, all_tokens, )\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6976/928765746.py\u001b[0m in \u001b[0;36mexplain\u001b[1;34m(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens)\u001b[0m\n\u001b[0;32m     26\u001b[0m def explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores,\n\u001b[0;32m     27\u001b[0m             ground_truth, all_tokens, ):\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mlig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLayerIntegratedGradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msquad_pos_forward_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m     attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\xai\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1175\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0m\u001b[0;32m   1178\u001b[0m             type(self).__name__, name))\n\u001b[0;32m   1179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DistilBertForQuestionAnswering' object has no attribute 'bert'"
     ]
    }
   ],
   "source": [
    "import nltk as tk\n",
    "from captum.attr import LayerConductance, LayerIntegratedGradients\n",
    "\n",
    "def predict_qt(question, text):\n",
    "    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "    ground_truth = '13'\n",
    "\n",
    "\n",
    "    start_scores, end_scores = predict(input_ids)\n",
    "\n",
    "\n",
    "    print('Question: ', question)\n",
    "    print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))\n",
    "    return input_ids, ref_input_ids, None, None, None, start_scores, end_scores, ground_truth, all_tokens,\n",
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values\n",
    "def explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores,\n",
    "            ground_truth, all_tokens, ):\n",
    "    lig = LayerIntegratedGradients(squad_pos_forward_func, model.bert.embeddings)\n",
    "\n",
    "    attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "                                                    baselines=ref_input_ids,\n",
    "                                                    additional_forward_args=(\n",
    "                                                        token_type_ids, position_ids, attention_mask, 0),\n",
    "                                                    internal_batch_size=4,\n",
    "                                                    return_convergence_delta=True)\n",
    "    attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "                                                additional_forward_args=(\n",
    "                                                    token_type_ids, position_ids, attention_mask, 1),\n",
    "                                                internal_batch_size=4,\n",
    "                                                return_convergence_delta=True)\n",
    "\n",
    "    attributions_start_sum = summarize_attributions(attributions_start)\n",
    "    attributions_end_sum = summarize_attributions(attributions_end)\n",
    "    # storing couple samples in an array for visualization purposes\n",
    "    start_position_vis = viz.VisualizationDataRecord(\n",
    "        attributions_start_sum,\n",
    "        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "        torch.argmax(start_scores),\n",
    "        torch.argmax(start_scores),\n",
    "        str(ground_truth),\n",
    "        attributions_start_sum.sum(),\n",
    "        all_tokens,\n",
    "        delta_start)\n",
    "\n",
    "    end_position_vis = viz.VisualizationDataRecord(\n",
    "        attributions_end_sum,\n",
    "        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "        torch.argmax(end_scores),\n",
    "        torch.argmax(end_scores),\n",
    "        str(ground_truth),\n",
    "        attributions_end_sum.sum(),\n",
    "        all_tokens,\n",
    "        delta_end)\n",
    "    print(all_tokens)\n",
    "    print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "    viz.visualize_text([start_position_vis])\n",
    "\n",
    "    print('\\033[1m', 'Visualizations For End Position', '\\033[0m')\n",
    "\n",
    "    print(\"attributions_start_sum:   \", len(attributions_start_sum))\n",
    "    print(\"all tokens:    \", len(all_tokens))\n",
    "\n",
    "    print(torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "          torch.argmax(end_scores),\n",
    "          torch.argmax(end_scores),\n",
    "          torch.max(torch.softmax(start_scores[0], dim=0)))\n",
    "\n",
    "    return all_tokens, attributions_start_sum\n",
    "\n",
    "\n",
    "def pred_explain(question, text):\n",
    "    input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask, start_scores, end_scores, ground_truth, all_tokens, = predict_qt(\n",
    "        question, text)\n",
    "\n",
    "    all_tokens, attributions_start_sum = explain(input_ids, ref_input_ids, token_type_ids, position_ids, attention_mask,\n",
    "                                                 start_scores, end_scores, ground_truth, all_tokens, )\n",
    "\n",
    "    return all_tokens, attributions_start_sum, start_scores, end_scores\n",
    "\n",
    "\n",
    "def generate_explain(i, question, text, answers):\n",
    "    exp = {}\n",
    "    l_attributions = []\n",
    "    l_start_score = []\n",
    "    l_end_score = []\n",
    "    l_pred = []\n",
    "\n",
    "    tokens = tk.sent_tokenize(text)\n",
    "    all_tokens, attributions_start_sum, start_scores, end_scores = pred_explain(question, text)\n",
    "    end_score = float(torch.max(torch.softmax(end_scores[0], dim=0)))\n",
    "    start_score = float(torch.max(torch.softmax(start_scores[0], dim=0)))\n",
    "    l_attributions.append(torch.sum(attributions_start_sum) / len(all_tokens))\n",
    "    l_start_score.append(start_score)\n",
    "    l_end_score.append(end_score)\n",
    "    l_pred.append(' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1]))\n",
    "\n",
    "    for token in tokens:\n",
    "        all_tokens, attributions_start_sum, start_scores, end_scores = pred_explain(question, token)\n",
    "        end_score = float(torch.max(torch.softmax(end_scores[0], dim=0)))\n",
    "        start_score = float(torch.max(torch.softmax(start_scores[0], dim=0)))\n",
    "        l_attributions.append(torch.sum(attributions_start_sum) / len(all_tokens))\n",
    "        l_start_score.append(start_score)\n",
    "        l_end_score.append(end_score)\n",
    "        l_pred.append(' '.join(all_tokens[torch.argmax(start_scores): torch.argmax(end_scores) + 1]))\n",
    "\n",
    "    #转换成float\n",
    "    l_attributions = [float(i) for i in l_attributions]\n",
    "\n",
    "    exp[\"ids\"] = i\n",
    "    exp[\"preds\"] = l_pred\n",
    "    exp[\"attributions\"] = l_attributions\n",
    "    exp[\"start_score\"] = l_start_score\n",
    "    exp[\"end_score\"] = l_end_score\n",
    "    exp[\"answer\"] = answers\n",
    "    return exp\n",
    "def write_explain(exp):\n",
    "    exp_str = json.dumps(exp)\n",
    "    with open(r\"small_answers.txt\", \"a\")as f:\n",
    "        f.write(exp_str)\n",
    "        f.write(\"\\r\\n\")\n",
    "    f.close()\n",
    "\n",
    "def create_multi_bars(i, answers, labels, datas, tick_step=1, group_gap=0.2, bar_gap=0, ):\n",
    "    '''\n",
    "    labels : x轴坐标标签序列\n",
    "    datas ：数据集，二维列表，要求列表每个元素的长度必须与labels的长度一致\n",
    "    tick_step ：默认x轴刻度步长为1，通过tick_step可调整x轴刻度步长。\n",
    "    group_gap : 柱子组与组之间的间隙，最好为正值，否则组与组之间重叠\n",
    "    bar_gap ：每组柱子之间的空隙，默认为0，每组柱子紧挨，正值每组柱子之间有间隙，负值每组柱子之间重叠\n",
    "    '''\n",
    "    # ticks为x轴刻度\n",
    "    ticks = np.arange(len(labels)) * tick_step\n",
    "    # group_num为数据的组数，即每组柱子的柱子个数\n",
    "    group_num = len(datas)\n",
    "    # group_width为每组柱子的总宽度，group_gap 为柱子组与组之间的间隙。\n",
    "    group_width = tick_step - group_gap\n",
    "    # bar_span为每组柱子之间在x轴上的距离，即柱子宽度和间隙的总和\n",
    "    bar_span = group_width / group_num\n",
    "    # bar_width为每个柱子的实际宽度\n",
    "    bar_width = bar_span - bar_gap\n",
    "    # baseline_x为每组柱子第一个柱子的基准x轴位置，随后的柱子依次递增bar_span即可\n",
    "    baseline_x = ticks - (group_width - bar_span) / 2\n",
    "    for index, y in enumerate(datas):\n",
    "        if index == 0:\n",
    "            plt.bar(baseline_x + index*bar_span, y, bar_width, label='attribution score')\n",
    "        if index == 1:\n",
    "            plt.bar(baseline_x + index*bar_span, y, bar_width, label='forword attention score')\n",
    "        if index == 2:\n",
    "            plt.bar(baseline_x + index*bar_span, y, bar_width, label='backword attention score')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('LONG SHORT TEST')\n",
    "    # x轴刻度标签位置与x轴刻度一致\n",
    "    plt.xticks(ticks, labels)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig('./longshorttest/' + str(i)+ '.jpg')\n",
    "    #画布清除\n",
    "    plt.clf()\n",
    "\n",
    "\n",
    "def generate_directory(i):\n",
    "    folder = \"./longshorttest/\" + str(i)\n",
    "    # 获取此py文件路径，在此路径选创建在new_folder文件夹中的test文件夹\n",
    "    save_path = folder  # './output/Modify_algorithm'\n",
    "    try:\n",
    "        if not os.path.exists(save_path):\n",
    "            os.makedirs(save_path)\n",
    "\n",
    "    except OSError:\n",
    "        print('Error: Creating directory. ' + save_path)\n",
    "def write_image(i, exp, answers):\n",
    "    li = []\n",
    "    # exp[\"preds\"] = l_pred\n",
    "    # exp[\"attributions\"] = l_attributions\n",
    "    # exp[\"start_score\"] = l_start_score\n",
    "    # exp[\"end_score\"] = l_end_score\n",
    "    l_attributions = exp[\"attributions\"]\n",
    "    l_start_score = exp[\"start_score\"]\n",
    "    l_end_score = exp[\"end_score\"]\n",
    "\n",
    "    for attribution in l_attributions:\n",
    "        temp = attribution\n",
    "        li.append(temp)\n",
    "    l_attributions = li\n",
    "\n",
    "    l_attributions = [i*6 for i in l_attributions]\n",
    "\n",
    "    generate_directory(i)\n",
    "    label = []\n",
    "    for attribution in range(len(l_attributions)):\n",
    "        label.append(\"sen\" + str(attribution))\n",
    "\n",
    "    data = [l_attributions, l_start_score, l_end_score]\n",
    "    create_multi_bars(i, answers, label, data, bar_gap=0.1)\n",
    "\n",
    "def write_exp(i, exp,answers):\n",
    "    write_explain(exp)\n",
    "    write_image(i, exp, answers)\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset('squad')\n",
    "\n",
    "for i in range(len(datasets['validation'])):\n",
    "    text = datasets['validation'][i]['context']\n",
    "    question = datasets['validation'][i]['question']\n",
    "    answers = datasets['validation'][i]['answers']['text'][0]\n",
    "\n",
    "    exp = generate_explain(i, question, text, answers)\n",
    "\n",
    "    write_exp(i, exp, answers)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

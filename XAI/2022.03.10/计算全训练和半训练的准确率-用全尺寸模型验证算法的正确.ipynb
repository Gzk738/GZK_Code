{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1c725fa-40ce-4758-aab3-e3743bbed0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "\n",
    "\"\"\"++++++++++++++++++这几个函数是计算f1 score 数值的，代码是抄的，千万不能改！+++++++++++++++++\"\"\"\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def read_squad(path):\n",
    "    path = Path(path)\n",
    "    with open(path, 'rb') as f:\n",
    "        squad_dict = json.load(f)\n",
    "\n",
    "    contexts = []\n",
    "    questions = []\n",
    "    answers = []\n",
    "    ids = []\n",
    "    for group in squad_dict['data']:\n",
    "        for passage in group['paragraphs']:\n",
    "            context = passage['context']\n",
    "            for qa in passage['qas']:\n",
    "                question = qa['question']\n",
    "                for answer in qa['answers']:\n",
    "                    contexts.append(context)\n",
    "                    questions.append(question)\n",
    "                    answers.append(answer)\n",
    "                    ids.append(qa['id'])\n",
    "\n",
    "    return contexts, questions, answers, ids\n",
    "\n",
    "train_contexts, train_questions, train_answers, train_ids = read_squad(r'D:\\software\\github\\GZK_Code\\XAI\\2022.03.03\\squad\\train-v2.0.json')\n",
    "val_contexts, val_questions, val_answers, val_ids = read_squad(r'D:\\software\\github\\GZK_Code\\XAI\\2022.03.03\\squad\\dev-v2.0.json')\n",
    "\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "\n",
    "def get_gold_answers(example):\n",
    "    \"\"\"helper function that retrieves all possible true answers from a squad2.0 example\"\"\"\n",
    "\n",
    "    gold_answers = [answer[\"text\"] for answer in example.answers if answer[\"text\"]]\n",
    "\n",
    "    # if gold_answers doesn't exist it's because this is a negative example -\n",
    "    # the only correct answer is an empty string\n",
    "    if not gold_answers:\n",
    "        gold_answers = [\"\"]\n",
    "\n",
    "    return gold_answers\n",
    "\n",
    "\n",
    "\"\"\"+++++++++++++++++++++++++++++++++++\"\"\"\n",
    "\n",
    "def predict(inputs):\n",
    "    output = model(inputs)\n",
    "    return output.start_logits, output.end_logits\n",
    "\n",
    "\n",
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id]\n",
    "\n",
    "    # construct reference token ids\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "                    [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def predict_qt(question, text):\n",
    "    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "\n",
    "    indices = input_ids[0].detach().tolist()\n",
    "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "    ground_truth = '13'\n",
    "\n",
    "\n",
    "    start_scores, end_scores = predict(input_ids)\n",
    "\n",
    "\n",
    "    return (' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1])),float(torch.max(torch.softmax(start_scores[0], dim=0))),float(torch.max(torch.softmax(end_scores[0], dim=0)))\n",
    "def normalize_text(s):\n",
    "    \"\"\"Removing articles and punctuation, and standardizing whitespace are all typical text processing steps.\"\"\"\n",
    "    import string, re\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def compute_exact_match(prediction, truth):\n",
    "    return int(normalize_text(prediction) == normalize_text(truth))\n",
    "\n",
    "def compute_f1(prediction, truth):\n",
    "    pred_tokens = normalize_text(prediction).split()\n",
    "    truth_tokens = normalize_text(truth).split()\n",
    "\n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens)\n",
    "\n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "\n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0\n",
    "\n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5e047a-f0f2-4a0e-b0f0-bfdc79e8fd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 334.09M\n"
     ]
    }
   ],
   "source": [
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "929833f0-ec15-4928-a9d1-eb5548a35a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "0.0 失败 144\n"
     ]
    }
   ],
   "source": [
    "acc = 0\n",
    "evl_dick = {}\n",
    "failu_num = 0\n",
    "for i in range(len(val_ids)):\n",
    "    question = val_questions[i]\n",
    "    text = val_contexts[i]\n",
    "    try:\n",
    "        answer, start_score, end_score = predict_qt(question, text)\n",
    "        evl_dick[str(val_ids[i])] = answer\n",
    "\n",
    "    except:\n",
    "        failu_num+=1\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(acc/1500, \"失败\", failu_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0e96840-63b4-47e3-9db6-532ce64ee3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5882"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(evl_dick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54e96df-2d6e-4b25-a401-7e095f8df2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-15-22-41-58\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "time = str(datetime.datetime.now().strftime('%Y-%m-%d-%H-%M-%S'))\n",
    "print (time)\n",
    "json_dick = json.dumps(evl_dick)\n",
    "filname = time + \"answers.txt\"\n",
    "fo = open(filname, \"w\",encoding='utf-8')\n",
    "fo.write(json_dick)\n",
    "fo.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67178a2b-1a21-47cd-acb8-c1a680bdf0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('mrm8488/bert-small-finetuned-squadv2')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "tokenizer = BertTokenizer.from_pretrained('mrm8488/bert-small-finetuned-squadv2')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8366988-b308-479e-9b27-f6e81d7cdbda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "0.3408622777614493\n"
     ]
    }
   ],
   "source": [
    "# val_contexts, val_questions, val_answers\n",
    "acc = 0\n",
    "for i in range(1500):\n",
    "    question = train_questions[i]\n",
    "    text = train_contexts[i]\n",
    "    answer, start_score, end_score = predict_qt(question, text)\n",
    "    f1 = compute_f1(answer, train_answers[i][\"text\"])\n",
    "    acc += f1\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(acc/1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b886798b-b623-48a4-b3b9-5463101b29fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameter: 28.50M\n"
     ]
    }
   ],
   "source": [
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dad516ac-2189-4273-873f-2862d1aaf9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "0.23165809118725164\n",
      "Number of parameter: 66.36M\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "import torch\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = torch.load(\"sep_train_distilbert.pth\")\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "# val_contexts, val_questions, val_answers\n",
    "acc = 0\n",
    "for i in range(1500):\n",
    "    question = train_questions[i]\n",
    "    text = train_contexts[i]\n",
    "    answer, start_score, end_score = predict_qt(question, text)\n",
    "    f1 = compute_f1(answer, train_answers[i][\"text\"])\n",
    "    acc += f1\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(acc/1500)\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e27cb07f-c17a-4aee-927e-8276f55bf443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "0.4189422020871912\n",
      "Number of parameter: 66.36M\n",
      "分着测\n",
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "0.6277082048561468\n"
     ]
    }
   ],
   "source": [
    "model = torch.load(\"epoch10sepdata_big.pth\")\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "# val_contexts, val_questions, val_answers\n",
    "acc = 0\n",
    "for i in range(1500):\n",
    "    question = train_questions[i]\n",
    "    text = train_contexts[i]\n",
    "    answer, start_score, end_score = predict_qt(question, text)\n",
    "    f1 = compute_f1(answer, train_answers[i][\"text\"])\n",
    "    acc += f1\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "\n",
    "print(acc/1500)\n",
    "total = sum([param.nelement() for param in model.parameters()])\n",
    "\n",
    "print(\"Number of parameter: %.2fM\" % (total/1e6))\n",
    "\n",
    "print(\"分着测\")\n",
    "import nltk as tk\n",
    "f1s = []\n",
    "f1 = []\n",
    "start_scores = []\n",
    "end_scores = []\n",
    "t_start_score = []\n",
    "t_end_score = []\n",
    "for i in range(1500):\n",
    "    text = train_contexts[i]\n",
    "    tokens = tk.sent_tokenize(text)\n",
    "    for token in tokens:\n",
    "        question = train_questions[i]\n",
    "        answer, start_score, end_score = predict_qt(question, token)\n",
    "        t_start_score.append(start_score)\n",
    "        t_end_score.append(end_score)\n",
    "        f1.append(compute_f1(answer, train_answers[i][\"text\"]))\n",
    "    f1s.append(f1)\n",
    "    start_scores.append(t_start_score)\n",
    "    end_scores.append(t_end_score)\n",
    "    f1 = []\n",
    "    t_start_score = []\n",
    "    t_end_score = []\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "        \n",
    "temp = 0\n",
    "print(len(f1s))\n",
    "for i in f1s:\n",
    "    temp+=max(i)\n",
    "print(temp/1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1969700-98b2-4453-af48-23102ba50c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4686666666666667 0.4726666666666667\n"
     ]
    }
   ],
   "source": [
    "a = [1,2,3,4,5,6]\n",
    "a.index(max(a))\n",
    "temp  = 0\n",
    "start_num = []\n",
    "end_num = []\n",
    "for i in range(len(f1s)):\n",
    "    if f1s[i].index(max(f1s[i])) == start_scores[i].index(max(start_scores[i])):\n",
    "        start_num.append(i)\n",
    "    if f1s[i].index(max(f1s[i])) == end_scores[i].index(max(end_scores[i])):\n",
    "        end_num.append(i)\n",
    "print(len(start_num)/len(f1s), len(end_num)/len(f1s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4efa09-5288-42ed-88c2-75f468cb3b53",
   "metadata": {},
   "source": [
    "## 分着预测看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "64005718-f1fd-4868-ad22-d86bcf50650a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "import nltk as tk\n",
    "f1s = []\n",
    "f1 = []\n",
    "for i in range(1500):\n",
    "    text = train_contexts[i]\n",
    "    tokens = tk.sent_tokenize(text)\n",
    "    for token in tokens:\n",
    "        question = train_questions[i]\n",
    "        answer, start_score, end_score = predict_qt(question, token)\n",
    "        f1.append(compute_f1(answer, train_answers[i][\"text\"]))\n",
    "    f1s.append(f1)\n",
    "    f1 = []\n",
    "\n",
    "    if i % 500 == 0:\n",
    "        print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4414d35a-0a07-4453-8f4a-00732f8640bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500\n",
      "0.6277082048561468\n"
     ]
    }
   ],
   "source": [
    "temp = 0\n",
    "print(len(f1s))\n",
    "for i in f1s:\n",
    "    temp+=max(i)\n",
    "print(temp/1500)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting BERT Models (Part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ElectraTokenizer, ElectraForQuestionAnswering#, pipelines\n",
    "from pprint import pprint\n",
    "\n",
    "# tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "# model = ElectraForQuestionAnswering.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "\n",
    "# qa = pipelines(\"question-answering\", tokenizer=tokenizer, model=model)\n",
    "\n",
    "# pprint(qa({\n",
    "#     \"question\": \"한국의 대통령은 누구인가?\",\n",
    "#     \"context\": \"문재인 대통령은 28일 서울 코엑스에서 열린 ‘데뷰 (Deview) 2019’ 행사에 참석해 젊은 개발자들을 격려하면서 우리 정부의 인공지능 기본구상을 내놓았다.\",\n",
    "# }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we demonstrate how to interpret Bert models using  `Captum` library. In this particular case study we focus on a fine-tuned Question Answering model on SQUAD dataset using transformers library from Hugging Face: https://huggingface.co/transformers/\n",
    "\n",
    "We show how to use interpretation hooks to examine and better understand embeddings, sub-embeddings, bert, and attention layers. \n",
    "\n",
    "Note: Before running this tutorial, please install `seaborn`, `pandas` and `matplotlib`, `transformers`(from hugging face) python packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Bad key \"text.kerning_factor\" on line 4 in\n",
      "c:\\users\\jh\\.conda\\envs\\ispin\\lib\\site-packages\\matplotlib\\mpl-data\\stylelib\\_classic_test_patch.mplstyle.\n",
      "You probably need to get an updated matplotlibrc file from\n",
      "http://github.com/matplotlib/matplotlib/blob/master/matplotlibrc.template\n",
      "or from the matplotlib source distribution\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertTokenizer, BertForQuestionAnswering, BertConfig\n",
    "from transformers import ElectraModel, ElectraTokenizer\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to fine-tune BERT model on SQUAD dataset. This can be easiy accomplished by following the steps described in hugging face's official web site: https://github.com/huggingface/transformers#run_squadpy-fine-tuning-on-squad-for-question-answering \n",
    "\n",
    "Note that the fine-tuning is done on a `bert-base-uncased` pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we pretrain the model, we can load the tokenizer and pre-trained BERT model using the commands described below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\\nmodel = ElectraForQuestionAnswering.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\\nmodel.to(device)\\nmodel.eval()\\nmodel.zero_grad()'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replace <PATH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "\n",
    "model_path = 'bert-base-uncased'\n",
    "\n",
    "# load model\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "'''tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "model = ElectraForQuestionAnswering.from_pretrained(\"monologg/koelectra-small-v2-distilled-korquad-384\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to perform forward pass of the model and make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(inputs, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "    return model(inputs, token_type_ids=token_type_ids,\n",
    "                 position_ids=position_ids, attention_mask=attention_mask, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining a custom forward function that will allow us to access the start and end postitions of our prediction using `position` input argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squad_pos_forward_func(inputs, token_type_ids=None, position_ids=None, attention_mask=None, position=0):\n",
    "    pred = predict(inputs,\n",
    "                   token_type_ids=token_type_ids,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    pred = pred[position]\n",
    "    return pred.max(1).values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute attributions with respect to the `BertEmbeddings` layer.\n",
    "\n",
    "To do so, we need to define baselines / references, numericalize both the baselines and the inputs. We will define helper functions to achieve that.\n",
    "\n",
    "The cell below defines numericalized special tokens that will be later used for constructing inputs and corresponding baselines/references."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_token_id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a set of helper function for constructing references / baselines for word tokens, token types and position ids. We also provide separate helper functions that allow to construct the sub-embeddings and corresponding baselines / references for all sub-embeddings of `BertEmbeddings` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id):\n",
    "    question_ids = tokenizer.encode(question, add_special_tokens=False)\n",
    "    text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "\n",
    "    # construct input token ids\n",
    "    # 입력으로 들어갈 문장을 완성시키는 코드이다. 토큰+질문+토큰+텍스트+토큰 일케\n",
    "    input_ids = [cls_token_id] + question_ids + [sep_token_id] + text_ids + [sep_token_id] \n",
    "\n",
    "    # construct reference token ids\n",
    "    # 참조선, 베이스라인을 완성시키는 코드이다. 토큰+질문만큼 ref토큰+토큰+텍스트만큼 ref토큰+토큰\n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(question_ids) + [sep_token_id] + \\\n",
    "        [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "    #이래놓고 3개를 반환하는데 입력,참조선,질문 길이\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(question_ids)\n",
    "\n",
    "def construct_input_ref_token_type_pair(input_ids, sep_ind=0):\n",
    "    #seq_len 이다 sep_ind 랑 다름...\n",
    "    #입력의 크기를 받고\n",
    "    seq_len = input_ids.size(1)\n",
    "    #sep_ind 크기 만큼 0 array를 만든다.\n",
    "    token_type_ids = torch.tensor([[0 if i <= sep_ind else 1 for i in range(seq_len)]], device=device)\n",
    "    #기준석 토큰 배열도 0 array를 token_type_ids만큼 만든다.\n",
    "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)# * -1\n",
    "    return token_type_ids, ref_token_type_ids\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    #입력 크기 받고\n",
    "    #입력크기 만큼 \n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)\n",
    "\n",
    "def construct_bert_sub_embedding(input_ids, ref_input_ids,\n",
    "                                   token_type_ids, ref_token_type_ids,\n",
    "                                   position_ids, ref_position_ids):\n",
    "    input_embeddings = interpretable_embedding1.indices_to_embeddings(input_ids)\n",
    "    ref_input_embeddings = interpretable_embedding1.indices_to_embeddings(ref_input_ids)\n",
    "\n",
    "    input_embeddings_token_type = interpretable_embedding2.indices_to_embeddings(token_type_ids)\n",
    "    ref_input_embeddings_token_type = interpretable_embedding2.indices_to_embeddings(ref_token_type_ids)\n",
    "\n",
    "    input_embeddings_position_ids = interpretable_embedding3.indices_to_embeddings(position_ids)\n",
    "    ref_input_embeddings_position_ids = interpretable_embedding3.indices_to_embeddings(ref_position_ids)\n",
    "    \n",
    "    return (input_embeddings, ref_input_embeddings), \\\n",
    "           (input_embeddings_token_type, ref_input_embeddings_token_type), \\\n",
    "           (input_embeddings_position_ids, ref_input_embeddings_position_ids)\n",
    "    \n",
    "def construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                    token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                    position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = interpretable_embedding.indices_to_embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = interpretable_embedding.indices_to_embeddings(ref_input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    \n",
    "    return input_embeddings, ref_input_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the `question - text` pair that we'd like to use as an input for our Bert model and interpret what the model was forcusing on when predicting an answer to the question from given input text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "question, text = \"What is important to us?\", \"It is important to us to include, empower and support humans of all kinds.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's numericalize the question, the input text and generate corresponding baselines / references for all three sub-embeddings (word, token type and position embeddings) types using our helper functions defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(question, text, ref_token_id, sep_token_id, cls_token_id)\n",
    "token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids, sep_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] what is important to us? [SEP] it is important to us to include, empower and support humans of all kinds. [SEP]'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, let's define the ground truth for prediction's start and end positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7861, 11452, 1998, 2490, 4286, 1997, 2035, 7957]\n"
     ]
    }
   ],
   "source": [
    "ground_truth = \"empower and support humans of all kinds\"#\"문재인 대통령\"'to include, empower and support humans of all kinds'\n",
    "\n",
    "ground_truth_tokens = tokenizer.encode(ground_truth, add_special_tokens=False)\n",
    "print(ground_truth_tokens)\n",
    "ground_truth_end_ind = indices.index(ground_truth_tokens[-1])\n",
    "ground_truth_start_ind = ground_truth_end_ind - len(ground_truth_tokens) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=predict(input_ids,token_type_ids=token_type_ids,position_ids=position_ids,attention_mask=attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make predictions using input, token type, position id and a default attention mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  What is important to us?\n",
      "Predicted Answer:  to include , em ##power and support humans of all kinds\n"
     ]
    }
   ],
   "source": [
    "start_scores=a.start_logits\n",
    "end_scores=a.end_logits\n",
    "\n",
    "print('Question: ', question)\n",
    "print('Predicted Answer: ', ' '.join(all_tokens[torch.argmax(start_scores) : torch.argmax(end_scores)+1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertviz import head_view\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "def show_head_view(model, tokenizer, sentence_a, sentence_b=None, layer=None, heads=None):\n",
    "    inputs = tokenizer.encode_plus(sentence_a, sentence_b, return_tensors='pt', add_special_tokens=False)\n",
    "    input_ids = inputs['input_ids']\n",
    "    if sentence_b:\n",
    "        token_type_ids = inputs['token_type_ids']\n",
    "        attention = model(input_ids, token_type_ids=token_type_ids)[-1]\n",
    "        sentence_b_start = token_type_ids[0].tolist().index(1)\n",
    "    else:\n",
    "        attention = model(input_ids)[-1]\n",
    "        sentence_b_start = None\n",
    "    input_id_list = input_ids[0].tolist() # Batch index 0\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list)    \n",
    "    head_view(attention, tokens, sentence_b_start, layer=layer, heads=heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.weight', 'qa_outputs.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "1 is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-10d371e4ef0a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_version\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdo_lower_case\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdo_lower_case\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mshow_head_view\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-90923548ee35>\u001b[0m in \u001b[0;36mshow_head_view\u001b[1;34m(model, tokenizer, sentence_a, sentence_b, layer, heads)\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mtoken_type_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'token_type_ids'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0msentence_b_start\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mattention\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: 1 is not in list"
     ]
    }
   ],
   "source": [
    "model_version = 'bert-base-uncased'\n",
    "do_lower_case = True\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased', output_attentions=True)\n",
    "#model = BertModel.from_pretrained(model_version, output_attentions=True)\n",
    "tokenizer = BertTokenizer.from_pretrained(model_version, do_lower_case=do_lower_case)\n",
    "\n",
    "show_head_view(model, tokenizer, question, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two different ways of computing the attributions for `BertEmbeddings` layer. One option is to use `LayerIntegratedGradients` and compute the attributions with respect to that layer. The second option is to pre-compute the embeddings and wrap the actual embeddings with `InterpretableEmbeddingBase`. The pre-computation of embeddings for the second option is necessary because integrated gradients scales the inputs and that won't be meaningful on the level of word / token indices.\n",
    "\n",
    "Since using `LayerIntegratedGradients` is simpler, let's use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model=model.to('cuda')\n",
    "lig = LayerIntegratedGradients(squad_pos_forward_func, model.bert.embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_start, delta_start = lig.attribute(inputs=input_ids,\n",
    "                                  baselines=ref_input_ids,\n",
    "                                  additional_forward_args=(token_type_ids, position_ids, attention_mask, 0),\n",
    "                                  return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_end, delta_end = lig.attribute(inputs=input_ids, baselines=ref_input_ids,\n",
    "                                additional_forward_args=(token_type_ids, position_ids, attention_mask, 1),\n",
    "                                return_convergence_delta=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to summarize attributions for each word token in the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions_start_sum = summarize_attributions(attributions_start)\n",
    "attributions_end_sum = summarize_attributions(attributions_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Visualizations For Start Position \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>13</b></text></td><td><text style=\"padding-right:2em\"><b>13 (0.48)</b></text></td><td><text style=\"padding-right:2em\"><b>16</b></text></td><td><text style=\"padding-right:2em\"><b>1.89</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 79%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 84%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 91%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(120, 75%, 90%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(120, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(120, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(120, 75%, 70%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(120, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Visualizations For End Position \u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>23</b></text></td><td><text style=\"padding-right:2em\"><b>23 (0.81)</b></text></td><td><text style=\"padding-right:2em\"><b>23</b></text></td><td><text style=\"padding-right:2em\"><b>1.50</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table width: 100%><div style=\"border-top: 1px solid; margin-top: 5px;             padding-top: 5px; display: inline-block\"><b>Legend: </b><span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 60%)\"></span> Negative  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(0, 75%, 100%)\"></span> Neutral  <span style=\"display: inline-block; width: 10px; height: 10px;                 border: 1px solid; background-color:                 hsl(120, 75%, 50%)\"></span> Positive  </div><tr><th>True Label</th><th>Predicted Label</th><th>Attribution Label</th><th>Attribution Score</th><th>Word Importance</th><tr><td><text style=\"padding-right:2em\"><b>23</b></text></td><td><text style=\"padding-right:2em\"><b>23 (0.81)</b></text></td><td><text style=\"padding-right:2em\"><b>23</b></text></td><td><text style=\"padding-right:2em\"><b>1.50</b></text></td><td><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [CLS]                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> what                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(0, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 72%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ?                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark><mark style=\"background-color: hsl(120, 75%, 88%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> it                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> is                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> important                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> us                    </font></mark><mark style=\"background-color: hsl(120, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> to                    </font></mark><mark style=\"background-color: hsl(120, 75%, 96%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> include                    </font></mark><mark style=\"background-color: hsl(0, 75%, 92%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ,                    </font></mark><mark style=\"background-color: hsl(120, 75%, 86%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> em                    </font></mark><mark style=\"background-color: hsl(120, 75%, 89%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> ##power                    </font></mark><mark style=\"background-color: hsl(0, 75%, 97%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> and                    </font></mark><mark style=\"background-color: hsl(0, 75%, 98%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> support                    </font></mark><mark style=\"background-color: hsl(120, 75%, 99%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> humans                    </font></mark><mark style=\"background-color: hsl(120, 75%, 94%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> of                    </font></mark><mark style=\"background-color: hsl(120, 75%, 85%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> all                    </font></mark><mark style=\"background-color: hsl(0, 75%, 95%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> kinds                    </font></mark><mark style=\"background-color: hsl(0, 75%, 93%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> .                    </font></mark><mark style=\"background-color: hsl(0, 75%, 100%); opacity:1.0;                     line-height:1.75\"><font color=\"black\"> [SEP]                    </font></mark></td><tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing couple samples in an array for visualization purposes\n",
    "start_position_vis = viz.VisualizationDataRecord(\n",
    "                        attributions_start_sum,\n",
    "                        torch.max(torch.softmax(start_scores[0], dim=0)),\n",
    "                        torch.argmax(start_scores),\n",
    "                        torch.argmax(start_scores),\n",
    "                        str(ground_truth_start_ind),\n",
    "                        attributions_start_sum.sum(),       \n",
    "                        all_tokens,\n",
    "                        delta_start)\n",
    "\n",
    "end_position_vis = viz.VisualizationDataRecord(\n",
    "                        attributions_end_sum,\n",
    "                        torch.max(torch.softmax(end_scores[0], dim=0)),\n",
    "                        torch.argmax(end_scores),\n",
    "                        torch.argmax(end_scores),\n",
    "                        str(ground_truth_end_ind),\n",
    "                        attributions_end_sum.sum(),       \n",
    "                        all_tokens,\n",
    "                        delta_end)\n",
    "\n",
    "print('\\033[1m', 'Visualizations For Start Position', '\\033[0m')\n",
    "viz.visualize_text([start_position_vis])\n",
    "\n",
    "print('\\033[1m', 'Visualizations For End Position', '\\033[0m')\n",
    "viz.visualize_text([end_position_vis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attributions_start_sum: tensor([ 0.0000, -0.1840,  0.4347,  0.3344, -0.2328,  0.1407,  0.0325,  0.0000,\n",
      "         0.0592,  0.2184,  0.0547,  0.0211,  0.1465,  0.0549,  0.2454,  0.0573,\n",
      "        -0.2022, -0.0651,  0.0944,  0.0158,  0.6037,  0.0205, -0.1245,  0.0871,\n",
      "         0.0747,  0.0000], device='cuda:0', dtype=torch.float64)\n",
      "torch.max(torch.softmax(start_scores[0], dim=0)): tensor(0.4826, device='cuda:0', grad_fn=<MaxBackward1>)\n",
      "torch.argmax(start_scores): tensor(13, device='cuda:0')\n",
      "str(ground_truth_start_ind): 16\n",
      "attributions_start_sum.sum(): tensor(1.8872, device='cuda:0', dtype=torch.float64)\n",
      "all_tokens: ['[CLS]', 'what', 'is', 'important', 'to', 'us', '?', '[SEP]', 'it', 'is', 'important', 'to', 'us', 'to', 'include', ',', 'em', '##power', 'and', 'support', 'humans', 'of', 'all', 'kinds', '.', '[SEP]']\n",
      "delta_start: tensor([-0.0960], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print('attributions_start_sum:',attributions_start_sum)\n",
    "print('torch.max(torch.softmax(start_scores[0], dim=0)):',torch.max(torch.softmax(start_scores[0], dim=0)))\n",
    "print('torch.argmax(start_scores):',torch.argmax(start_scores))\n",
    "print('str(ground_truth_start_ind):',str(ground_truth_start_ind))\n",
    "print('attributions_start_sum.sum():',attributions_start_sum.sum())\n",
    "print('all_tokens:',all_tokens)\n",
    "print('delta_start:',delta_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is important us ? it is important to us to include , and support humans of kinds . '"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive=[]\n",
    "for i,j in enumerate(attributions_start_sum):\n",
    "    if j>0:\n",
    "        positive.append(i)\n",
    "s=''\n",
    "for i in positive:\n",
    "    s+=all_tokens[i]+' '\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the results above we can tell that for predicting start position our model is focusing more on the question side. More specifically on the tokens `what` and `important`. It has also slight focus on the token sequence `to us` in the text side.\n",
    "\n",
    "In contrast to that, for predicting end position, our model focuses more on the text side and has relative high attribution on the last end position token `kinds`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-Embedding attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look into the sub-embeddings of `BerEmbeddings` and try to understand the contributions and roles of each of them for both start and end predicted positions.\n",
    "\n",
    "To do so, we'd need to place interpretation hooks in each three of them.\n",
    "\n",
    "Note that we could perform attribution by using `LayerIntegratedGradients` as well but in that case we have to call attribute three times for each sub-layer since currently `LayerIntegratedGradients` takes only a layer at a time. In the future we plan to support multi-layer attribution and will be able to perform attribution by only calling attribute once. \n",
    "\n",
    "`configure_interpretable_embedding_layer` function will help us to place interpretation hooks on each sub-layer. It returns `InterpretableEmbeddingBase` layer for each sub-embedding and can be used to access the embedding vectors. \n",
    "\n",
    "Note that we need to remove InterpretableEmbeddingBase wrapper from our model using remove_interpretable_embedding_layer function after we finish interpretation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretable_embedding1 = configure_interpretable_embedding_layer(model, 'bert.embeddings.word_embeddings')\n",
    "interpretable_embedding2 = configure_interpretable_embedding_layer(model, 'bert.embeddings.token_type_embeddings')\n",
    "interpretable_embedding3 = configure_interpretable_embedding_layer(model, 'bert.embeddings.position_embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BertEmbeddings` has three sub-embeddings, namely, `word_embeddings`, `token_type_embeddings` and `position_embeddings` and this time we would like to attribute to each of them independently.\n",
    "`construct_bert_sub_embedding` helper function helps us to construct input embeddings and corresponding references in a separation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(input_embed, ref_input_embed), (token_type_ids_embed, ref_token_type_ids_embed), (position_ids_embed, ref_position_ids_embed) = construct_bert_sub_embedding(input_ids, ref_input_ids, \\\n",
    "                                         token_type_ids=token_type_ids, ref_token_type_ids=ref_token_type_ids, \\\n",
    "                                         position_ids=position_ids, ref_position_ids=ref_position_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create an instance of `IntegratedGradients` and compute the attributions with respect to all those embeddings both for the start and end positions and summarize them for each word token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig = IntegratedGradients(squad_pos_forward_func)\n",
    "\n",
    "attributions_start = ig.attribute(inputs=(input_embed, token_type_ids_embed, position_ids_embed),\n",
    "                                  baselines=(ref_input_embed, ref_token_type_ids_embed, ref_position_ids_embed),\n",
    "                                  additional_forward_args=(attention_mask, 0))\n",
    "\n",
    "attributions_end = ig.attribute(inputs=(input_embed, token_type_ids_embed, position_ids_embed),\n",
    "                                  baselines=(ref_input_embed, ref_token_type_ids_embed, ref_position_ids_embed),\n",
    "                                  additional_forward_args=(attention_mask, 1))\n",
    "\n",
    "attributions_start_word = summarize_attributions(attributions_start[0])\n",
    "attributions_end_word = summarize_attributions(attributions_end[0])\n",
    "\n",
    "attributions_start_token_type = summarize_attributions(attributions_start[1])\n",
    "attributions_end_token_type = summarize_attributions(attributions_end[1])\n",
    "\n",
    "attributions_start_position = summarize_attributions(attributions_start[2])\n",
    "attributions_end_position = summarize_attributions(attributions_end[2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An auxilary function that will help us to compute topk attributions and corresponding indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topk_attributed_tokens(attrs, k=5):\n",
    "    values, indices = torch.topk(attrs, k)\n",
    "    top_tokens = [all_tokens[idx] for idx in indices]\n",
    "    return top_tokens, values, indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing interpretation hooks from all layers after finishing attribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_interpretable_embedding_layer(model, interpretable_embedding1)\n",
    "remove_interpretable_embedding_layer(model, interpretable_embedding2)\n",
    "remove_interpretable_embedding_layer(model, interpretable_embedding3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing topk attributions for all sub-embeddings and placing them in pandas dataframes for better visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words_start, top_words_val_start, top_word_ind_start = get_topk_attributed_tokens(attributions_start_word)\n",
    "top_words_end, top_words_val_end, top_words_ind_end = get_topk_attributed_tokens(attributions_end_word)\n",
    "\n",
    "top_token_type_start, top_token_type_val_start, top_token_type_ind_start = get_topk_attributed_tokens(attributions_start_token_type)\n",
    "top_token_type_end, top_token_type_val_end, top_token_type_ind_end = get_topk_attributed_tokens(attributions_end_token_type)\n",
    "\n",
    "top_pos_start, top_pos_val_start, pos_ind_start = get_topk_attributed_tokens(attributions_start_position)\n",
    "top_pos_end, top_pos_val_end, pos_ind_end = get_topk_attributed_tokens(attributions_end_position)\n",
    "\n",
    "df_start = pd.DataFrame({'Word(Index), Attribution': [\"{} ({}), {}\".format(word, pos, round(val.item(),2)) for word, pos, val in zip(top_words_start, top_word_ind_start, top_words_val_start)],\n",
    "                   'Token Type(Index), Attribution': [\"{} ({}), {}\".format(ttype, pos, round(val.item(),2)) for ttype, pos, val in zip(top_token_type_start, top_token_type_ind_start, top_words_val_start)],\n",
    "                   'Position(Index), Attribution': [\"{} ({}), {}\".format(position, pos, round(val.item(),2)) for position, pos, val in zip(top_pos_start, pos_ind_start, top_pos_val_start)]})\n",
    "df_start.style.apply(['cell_ids: False'])\n",
    "\n",
    "df_end = pd.DataFrame({'Word(Index), Attribution': [\"{} ({}), {}\".format(word, pos, round(val.item(),2)) for word, pos, val in zip(top_words_end, top_words_ind_end, top_words_val_end)],\n",
    "                   'Token Type(Index), Attribution': [\"{} ({}), {}\".format(ttype, pos, round(val.item(),2)) for ttype, pos, val in zip(top_token_type_end, top_token_type_ind_end, top_words_val_end)],\n",
    "                   'Position(Index), Attribution': [\"{} ({}), {}\".format(position, pos, round(val.item(),2)) for position, pos, val in zip(top_pos_end, pos_ind_end, top_pos_val_end)]})\n",
    "df_end.style.apply(['cell_ids: False'])\n",
    "\n",
    "['{}({})'.format(token, str(i)) for i, token in enumerate(all_tokens)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we can see top 5 attribution results from all three embedding types in predicting start positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 attributed embeddings for start position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings help to focus more on the surrounding tokens of the predicted answer's start position to such as em, ##power and ,. It also has high attribution for the tokens in the question such as what and ?.\n",
    "\n",
    "In contrast to to word embedding, token embedding type focuses more on the tokens in the text part such as important,em and start token to.\n",
    "\n",
    "Position embedding also has high attribution score for the tokens surrounding to such as us and important. In addition to that, similar to word embedding we observe important tokens from the question.\n",
    "\n",
    "We can perform similar analysis, and visualize top 5 attributed tokens for all three embedding types, also for the end position prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 attributed embeddings for end position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to observe high concentration of highly attributed tokens such as `of`, `kinds`, `support` and `##power` for end position prediction.\n",
    "\n",
    "The token `kinds`, which is the correct predicted token appears to have high attribution score both according word and position embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Bert Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look into the layers of our network. More specifically we would like to look into the distribution of attribution scores for each token across all layers in Bert model and dive deeper into specific tokens.  \n",
    "We do that using one of layer attribution algorithms, namely, layer conductance. However, we encourage you to try out and compare the results with other algorithms as well.\n",
    "\n",
    "\n",
    "Let's configure `InterpretableEmbeddingsBase` again, in this case in order to interpret the layers of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpretable_embedding = configure_interpretable_embedding_layer(model, 'bert.embeddings')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over all layers and compute the attributions for all tokens. In addition to that let's also choose a specific token that we would like to examine in detail, specified by an id `token_to_explain` and store related information in a separate array.\n",
    "\n",
    "\n",
    "Note: Since below code is iterating over all layers it can take over 5 seconds. Please be patient!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attrs_start = []\n",
    "layer_attrs_end = []\n",
    "\n",
    "# The token that we would like to examine separately.\n",
    "token_to_explain = 23 # the index of the token that we would like to examine more thoroughly\n",
    "layer_attrs_start_dist = []\n",
    "layer_attrs_end_dist = []\n",
    "\n",
    "input_embeddings, ref_input_embeddings = construct_whole_bert_embeddings(input_ids, ref_input_ids, \\\n",
    "                                         token_type_ids=token_type_ids, ref_token_type_ids=ref_token_type_ids, \\\n",
    "                                         position_ids=position_ids, ref_position_ids=ref_position_ids)\n",
    "\n",
    "for i in range(model.config.num_hidden_layers):\n",
    "    lc = LayerConductance(squad_pos_forward_func, model.bert.encoder.layer[i])\n",
    "    layer_attributions_start = lc.attribute(inputs=input_embeddings, baselines=ref_input_embeddings, additional_forward_args=(token_type_ids, position_ids,attention_mask, 0))[0]\n",
    "    layer_attributions_end = lc.attribute(inputs=input_embeddings, baselines=ref_input_embeddings, additional_forward_args=(token_type_ids, position_ids,attention_mask, 1))[0]\n",
    " \n",
    "    layer_attrs_start.append(summarize_attributions(layer_attributions_start).cpu().detach().tolist())\n",
    "    layer_attrs_end.append(summarize_attributions(layer_attributions_end).cpu().detach().tolist())\n",
    "\n",
    "    # storing attributions of the token id that we would like to examine in more detail in token_to_explain\n",
    "    layer_attrs_start_dist.append(layer_attributions_start[0,token_to_explain,:].cpu().detach().tolist())\n",
    "    layer_attrs_end_dist.append(layer_attributions_end[0,token_to_explain,:].cpu().detach().tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below represents a heat map of attributions across all layers and tokens for the start position prediction. \n",
    "It is interesting to observe that the question word `what` gains increasingly high attribution from layer one to nine. In the last three layers that importance is slowly diminishing.  \n",
    "In contrary to `what` token, many other tokens have negative or close to zero attribution in the first 6 layers. \n",
    "\n",
    "We start seeing slightly higher attribution in tokens `important`, `us` and `to`. Interestingly token `em` is also assigned high attribution score which is remarkably high the last three layers.\n",
    "And lastly, our correctly predicted token `to` for the start position gains increasingly positive attribution has relatively high attribution especially in the last two layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "xticklabels=all_tokens\n",
    "yticklabels=list(range(1,13))\n",
    "ax = sns.heatmap(np.array(layer_attrs_start), xticklabels=xticklabels, yticklabels=yticklabels, linewidth=0.2)\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Layers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's examine the heat map of the attributions for the end position prediction. In the case of end position prediction we again observe high attribution scores for the token `what` in the last 11 layers.\n",
    "The correctly predicted end token `kinds` has positive attribution across all layers and it is especially prominent in the last two layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "xticklabels=all_tokens\n",
    "yticklabels=list(range(1,13))\n",
    "ax = sns.heatmap(np.array(layer_attrs_end), xticklabels=xticklabels, yticklabels=yticklabels, linewidth=0.2) #, annot=True\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Layers')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to note that when we compare the heat maps of start and end position, overall the colors for start position prediction on the map have darker intensities. This implies that there are less tokens that attribute positively to the start position prediction and there are more tokens which are negative indicators or signals of start position prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's dig deeper into specific tokens and look into the distribution of attributions per layer for the token `kinds` in the start and end positions. The box plot diagram below shows the presence of outliers especially in the first four layers and in layer 8. We also observe that for start position prediction interquartile range slowly decreases as we go deeper into the layers and finally it is dimishing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax = sns.boxplot(data=layer_attrs_start_dist)\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('Attribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot same distribution but for the prediction of the end position. Here attribution has larger positive values across all layers and the interquartile range doesn't change much when moving deeper into the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "ax = sns.boxplot(data=layer_attrs_end_dist)\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('Attribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's remove interpretation hooks, since we finished interpretation at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_interpretable_embedding_layer(model, interpretable_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to that we can also look into the distribution of attributions in each layer for any input token. This will help us to better understand and compare the distributional patterns of attributions across multiple layers. We can for example represent attributions as a probability density function (pdf) and compute the entropy of it in order to estimate the entropy of attributions in each layer. This can be easily computed using a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_attr(attrs, bins=100):\n",
    "    return np.histogram(attrs, bins=bins, density=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case let's compute the pdf for the attributions at end positions `kinds`. We can however do it for all tokens.\n",
    "\n",
    "We will compute and visualize the pdfs and entropies using Shannon's Entropy measure for each layer for token `kinds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_attrs_end_pdf = map(lambda layer_attrs_end_dist: pdf_attr(layer_attrs_end_dist), layer_attrs_end_dist)\n",
    "layer_attrs_end_pdf = np.array(list(layer_attrs_end_pdf))\n",
    "\n",
    "# summing attribution along embedding diemension for each layer\n",
    "# size: #layers\n",
    "attr_sum = np.array(layer_attrs_end_dist).sum(-1)\n",
    "\n",
    "# size: #layers\n",
    "layer_attrs_end_pdf_norm = np.linalg.norm(layer_attrs_end_pdf, axis=-1, ord=1)\n",
    "\n",
    "#size: #bins x #layers\n",
    "layer_attrs_end_pdf = np.transpose(layer_attrs_end_pdf)\n",
    "\n",
    "#size: #bins x #layers\n",
    "layer_attrs_end_pdf = np.divide(layer_attrs_end_pdf, layer_attrs_end_pdf_norm, where=layer_attrs_end_pdf_norm!=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot below visualizes the probability mass function (pmf) of attributions for each layer for the end position token `kinds`. From the plot we can observe that the distributions are taking bell-curved shapes with different means and variances.\n",
    "We can now use attribution pdfs to compute entropies in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "plt.plot(layer_attrs_end_pdf)\n",
    "plt.xlabel('Bins')\n",
    "plt.ylabel('Density')\n",
    "plt.legend(['Layer '+ str(i) for i in range(1,13)])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we calculate and visualize attribution entropies based on Shannon entropy measure where the x-axis corresponds to the number of layers and the y-axis corresponds to the total attribution in that layer. The size of the circles for each (layer, total_attribution) pair correspond to the normalized entropy value at that point.\n",
    "\n",
    "In this particular example, we observe that the entropy doesn't change much from layer to layer, however in a general case entropy can provide us an intuition about the distributional characteristics of attributions in each layer and can be useful especially when comparing it across multiple tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# replacing 0s with 1s. np.log(1) = 0 and np.log(0) = -inf\n",
    "layer_attrs_end_pdf[layer_attrs_end_pdf == 0] = 1\n",
    "layer_attrs_end_pdf_log = np.log2(layer_attrs_end_pdf)\n",
    "\n",
    "# size: #layers\n",
    "entropies= -(layer_attrs_end_pdf * layer_attrs_end_pdf_log).sum(0)\n",
    "\n",
    "plt.scatter(np.arange(12), attr_sum, s=entropies * 100)\n",
    "plt.xlabel('Layers')\n",
    "plt.ylabel('Total Attribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Part 2 of this tutorial we will to go deeper into attention layers, heads and compare the attributions with the attention weight matrices, study and discuss related statistics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
